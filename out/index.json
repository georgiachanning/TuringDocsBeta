[{"id":3,"pagetitle":"Home","title":"DynamicPPL.jl","ref":"/ppl/stable/#DynamicPPL.jl","content":" DynamicPPL.jl A domain-specific language and backend for probabilistic programming languages, used by  Turing.jl ."},{"id":6,"pagetitle":"API","title":"API","ref":"/ppl/stable/api/#API","content":" API Part of the API of DynamicPPL is defined in the more lightweight interface package  AbstractPPL.jl  and reexported here."},{"id":7,"pagetitle":"API","title":"Model","ref":"/ppl/stable/api/#Model","content":" Model"},{"id":8,"pagetitle":"API","title":"Macros","ref":"/ppl/stable/api/#Macros","content":" Macros A core component of DynamicPPL is the  @model  macro. It can be used to define probabilistic models in an intuitive way by specifying random variables and their distributions with  ~  statements. These statements are rewritten by  @model  as calls of  internal functions  for sampling the variables and computing their log densities."},{"id":9,"pagetitle":"API","title":"DynamicPPL.@model","ref":"/ppl/stable/api/#DynamicPPL.@model","content":" DynamicPPL.@model  —  Macro @model(expr[, warn = false]) Macro to specify a probabilistic model. If  warn  is  true , a warning is displayed if internal variable names are used in the model definition. Examples Model definition: @model function model(x, y = 42)\n    ...\nend To generate a  Model , call  model(xvalue)  or  model(xvalue, yvalue) . source One can nest models and call another model inside the model function with  @submodel ."},{"id":10,"pagetitle":"API","title":"DynamicPPL.@submodel","ref":"/ppl/stable/api/#DynamicPPL.@submodel","content":" DynamicPPL.@submodel  —  Macro @submodel model\n@submodel ... = model Run a Turing  model  nested inside of a Turing model. Examples julia> @model function demo1(x)\n           x ~ Normal()\n           return 1 + abs(x)\n       end;\n\njulia> @model function demo2(x, y)\n            @submodel a = demo1(x)\n            return y ~ Uniform(0, a)\n       end; When we sample from the model  demo2(missing, 0.4)  random variable  x  will be sampled: julia> vi = VarInfo(demo2(missing, 0.4));\n\njulia> @varname(x) in keys(vi)\ntrue Variable  a  is not tracked since it can be computed from the random variable  x  that was tracked when running  demo1 : julia> @varname(a) in keys(vi)\nfalse We can check that the log joint probability of the model accumulated in  vi  is correct: julia> x = vi[@varname(x)];\n\njulia> getlogp(vi) ≈ logpdf(Normal(), x) + logpdf(Uniform(0, 1 + abs(x)), 0.4)\ntrue source @submodel prefix=... model\n@submodel prefix=... ... = model Run a Turing  model  nested inside of a Turing model and add \" prefix .\" as a prefix to all random variables inside of the  model . Valid expressions for  prefix=...  are: prefix=false : no prefix is used. prefix=true :  attempt  to automatically determine the prefix from the left-hand side  ... = model  by first converting into a  VarName , and then calling  Symbol  on this. prefix=expression : results in the prefix  Symbol(expression) . The prefix makes it possible to run the same Turing model multiple times while keeping track of all random variables correctly. Examples Example models julia> @model function demo1(x)\n           x ~ Normal()\n           return 1 + abs(x)\n       end;\n\njulia> @model function demo2(x, y, z)\n            @submodel prefix=\"sub1\" a = demo1(x)\n            @submodel prefix=\"sub2\" b = demo1(y)\n            return z ~ Uniform(-a, b)\n       end; When we sample from the model  demo2(missing, missing, 0.4)  random variables  sub1.x  and  sub2.x  will be sampled: julia> vi = VarInfo(demo2(missing, missing, 0.4));\n\njulia> @varname(var\"sub1.x\") in keys(vi)\ntrue\n\njulia> @varname(var\"sub2.x\") in keys(vi)\ntrue Variables  a  and  b  are not tracked since they can be computed from the random variables  sub1.x  and  sub2.x  that were tracked when running  demo1 : julia> @varname(a) in keys(vi)\nfalse\n\njulia> @varname(b) in keys(vi)\nfalse We can check that the log joint probability of the model accumulated in  vi  is correct: julia> sub1_x = vi[@varname(var\"sub1.x\")];\n\njulia> sub2_x = vi[@varname(var\"sub2.x\")];\n\njulia> logprior = logpdf(Normal(), sub1_x) + logpdf(Normal(), sub2_x);\n\njulia> loglikelihood = logpdf(Uniform(-1 - abs(sub1_x), 1 + abs(sub2_x)), 0.4);\n\njulia> getlogp(vi) ≈ logprior + loglikelihood\ntrue Different ways of setting the prefix julia> @model inner() = x ~ Normal()\ninner (generic function with 2 methods)\n\njulia> # When `prefix` is unspecified, no prefix is used.\n       @model outer() = @submodel a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(x) in keys(VarInfo(outer()))\ntrue\n\njulia> # Explicitely don't use any prefix.\n       @model outer() = @submodel prefix=false a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(x) in keys(VarInfo(outer()))\ntrue\n\njulia> # Automatically determined from `a`.\n       @model outer() = @submodel prefix=true a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"a.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # Using a static string.\n       @model outer() = @submodel prefix=\"my prefix\" a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"my prefix.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # Using string interpolation.\n       @model outer() = @submodel prefix=\"$(nameof(inner()))\" a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"inner.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # Or using some arbitrary expression.\n       @model outer() = @submodel prefix=1 + 2 a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"3.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # (×) Automatic prefixing without a left-hand side expression does not work!\n       @model outer() = @submodel prefix=true inner()\nERROR: LoadError: cannot automatically prefix with no left-hand side\n[...] Notes The choice  prefix=expression  means that the prefixing will incur a runtime cost. This is also the case for  prefix=true , depending on whether the expression on the the right-hand side of  ... = model  requires runtime-information or not, e.g.  x = model  will result in the  static  prefix  x , while  x[i] = model  will be resolved at runtime. source"},{"id":11,"pagetitle":"API","title":"Type","ref":"/ppl/stable/api/#Type","content":" Type A  Model  can be created by calling the model function, as defined by  @model ."},{"id":12,"pagetitle":"API","title":"DynamicPPL.Model","ref":"/ppl/stable/api/#DynamicPPL.Model","content":" DynamicPPL.Model  —  Type struct Model{F,argnames,defaultnames,missings,Targs,Tdefaults}\n    f::F\n    args::NamedTuple{argnames,Targs}\n    defaults::NamedTuple{defaultnames,Tdefaults}\nend A  Model  struct with model evaluation function of type  F , arguments of names  argnames  types  Targs , default arguments of names  defaultnames  with types  Tdefaults , and missing arguments  missings . Here  argnames ,  defaultargnames , and  missings  are tuples of symbols, e.g.  (:a, :b) . An argument with a type of  Missing  will be in  missings  by default. However, in non-traditional use-cases  missings  can be defined differently. All variables in  missings  are treated as random variables rather than observations. The default arguments are used internally when constructing instances of the same model with different arguments. Examples julia> Model(f, (x = 1.0, y = 2.0))\nModel{typeof(f),(:x, :y),(),(),Tuple{Float64,Float64},Tuple{}}(f, (x = 1.0, y = 2.0), NamedTuple())\n\njulia> Model(f, (x = 1.0, y = 2.0), (x = 42,))\nModel{typeof(f),(:x, :y),(:x,),(),Tuple{Float64,Float64},Tuple{Int64}}(f, (x = 1.0, y = 2.0), (x = 42,))\n\njulia> Model{(:y,)}(f, (x = 1.0, y = 2.0), (x = 42,)) # with special definition of missings\nModel{typeof(f),(:x, :y),(:x,),(:y,),Tuple{Float64,Float64},Tuple{Int64}}(f, (x = 1.0, y = 2.0), (x = 42,)) source Model s are callable structs."},{"id":13,"pagetitle":"API","title":"DynamicPPL.Model","ref":"/ppl/stable/api/#DynamicPPL.Model-Tuple{}","content":" DynamicPPL.Model  —  Method (model::Model)([rng, varinfo, sampler, context]) Sample from the  model  using the  sampler  with random number generator  rng  and the  context , and store the sample and log joint probability in  varinfo . The method resets the log joint probability of  varinfo  and increases the evaluation number of  sampler . source Basic properties of a model can be accessed with  getargnames ,  getmissings , and  nameof ."},{"id":14,"pagetitle":"API","title":"Base.nameof","ref":"/ppl/stable/api/#Base.nameof-Tuple{Model}","content":" Base.nameof  —  Method nameof(model::Model) Get the name of the  model  as  Symbol . source"},{"id":15,"pagetitle":"API","title":"DynamicPPL.getargnames","ref":"/ppl/stable/api/#DynamicPPL.getargnames","content":" DynamicPPL.getargnames  —  Function getargnames(model::Model) Get a tuple of the argument names of the  model . source"},{"id":16,"pagetitle":"API","title":"DynamicPPL.getmissings","ref":"/ppl/stable/api/#DynamicPPL.getmissings","content":" DynamicPPL.getmissings  —  Function getmissings(model::Model) Get a tuple of the names of the missing arguments of the  model . source"},{"id":17,"pagetitle":"API","title":"Evaluation","ref":"/ppl/stable/api/#Evaluation","content":" Evaluation With  rand  one can draw samples from the prior distribution of a  Model ."},{"id":18,"pagetitle":"API","title":"Base.rand","ref":"/ppl/stable/api/#Base.rand","content":" Base.rand  —  Function rand([rng=Random.GLOBAL_RNG], [T=NamedTuple], model::Model) Generate a sample of type  T  from the prior distribution of the  model . source One can also evaluate the log prior, log likelihood, and log joint probability."},{"id":19,"pagetitle":"API","title":"DynamicPPL.logprior","ref":"/ppl/stable/api/#DynamicPPL.logprior","content":" DynamicPPL.logprior  —  Function logprior(model::Model, varinfo::AbstractVarInfo) Return the log prior probability of variables  varinfo  for the probabilistic  model . See also  logjoint  and  loglikelihood . source logprior(model::Model, θ) Return the log prior probability of variables  θ  for the probabilistic  model . See also  logjoint  and  loglikelihood . Examples julia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       logprior(demo([1.0]), (m = 100.0, ))\n-5000.918938533205\n\njulia> # Using a `Dict`.\n       logprior(demo([1.0]), Dict(@varname(m) => 100.0))\n-5000.918938533205\n\njulia> # Truth.\n       logpdf(Normal(), 100.0)\n-5000.918938533205 source"},{"id":20,"pagetitle":"API","title":"StatsAPI.loglikelihood","ref":"/ppl/stable/api/#StatsAPI.loglikelihood","content":" StatsAPI.loglikelihood  —  Function loglikelihood(model::Model, varinfo::AbstractVarInfo) Return the log likelihood of variables  varinfo  for the probabilistic  model . See also  logjoint  and  logprior . source loglikelihood(model::Model, θ) Return the log likelihood of variables  θ  for the probabilistic  model . See also  logjoint  and  logprior . Examples julia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       loglikelihood(demo([1.0]), (m = 100.0, ))\n-4901.418938533205\n\njulia> # Using a `Dict`.\n       loglikelihood(demo([1.0]), Dict(@varname(m) => 100.0))\n-4901.418938533205\n\njulia> # Truth.\n       logpdf(Normal(100.0, 1.0), 1.0)\n-4901.418938533205 source"},{"id":21,"pagetitle":"API","title":"DynamicPPL.logjoint","ref":"/ppl/stable/api/#DynamicPPL.logjoint","content":" DynamicPPL.logjoint  —  Function logjoint(model::Model, varinfo::AbstractVarInfo) Return the log joint probability of variables  varinfo  for the probabilistic  model . See  logjoint  and  loglikelihood . source logjoint(model::Model, θ) Return the log joint probability of variables  θ  for the probabilistic  model . See  logjoint  and  loglikelihood . Examples julia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       logjoint(demo([1.0]), (m = 100.0, ))\n-9902.33787706641\n\njulia> # Using a `Dict`.\n       logjoint(demo([1.0]), Dict(@varname(m) => 100.0))\n-9902.33787706641\n\njulia> # Truth.\n       logpdf(Normal(100.0, 1.0), 1.0) + logpdf(Normal(), 100.0)\n-9902.33787706641 source"},{"id":22,"pagetitle":"API","title":"Condition and decondition","ref":"/ppl/stable/api/#Condition-and-decondition","content":" Condition and decondition A  Model  can be conditioned on a set of observations with  AbstractPPL.condition  or its alias  | ."},{"id":23,"pagetitle":"API","title":"Base.:|","ref":"/ppl/stable/api/#Base.:|-Tuple{Model, Any}","content":" Base.:|  —  Method model | (x = 1.0, ...) Return a  Model  which now treats variables on the right-hand side as observations. See  condition  for more information and examples. source"},{"id":24,"pagetitle":"API","title":"AbstractPPL.condition","ref":"/ppl/stable/api/#AbstractPPL.condition","content":" AbstractPPL.condition  —  Function condition(model::Model; values...)\ncondition(model::Model, values::NamedTuple) Return a  Model  which now treats the variables in  values  as observations. See also:  decondition ,  conditioned Limitations This does currently  not  work with variables that are provided to the model as arguments, e.g.  @model function demo(x) ... end  means that  condition  will not affect the variable  x . Therefore if one wants to make use of  condition  and  decondition  one should not be specifying any random variables as arguments. This is done for the sake of backwards compatibility. Examples Simple univariate model julia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo();\n\njulia> m, x = model(); (m ≠ 1.0 && x ≠ 100.0)\ntrue\n\njulia> # Create a new instance which treats `x` as observed\n       # with value `100.0`, and similarly for `m=1.0`.\n       conditioned_model = condition(model, x=100.0, m=1.0);\n\njulia> m, x = conditioned_model(); (m == 1.0 && x == 100.0)\ntrue\n\njulia> # Let's only condition on `x = 100.0`.\n       conditioned_model = condition(model, x = 100.0);\n\njulia> m, x =conditioned_model(); (m ≠ 1.0 && x == 100.0)\ntrue\n\njulia> # We can also use the nicer `|` syntax.\n       conditioned_model = model | (x = 100.0, );\n\njulia> m, x = conditioned_model(); (m ≠ 1.0 && x == 100.0)\ntrue The above uses a  NamedTuple  to hold the conditioning variables, which allows us to perform some additional optimizations; in many cases, the above has zero runtime-overhead. But we can also use a  Dict , which offers more flexibility in the conditioning (see examples further below) but generally has worse performance than the  NamedTuple  approach: julia> conditioned_model_dict = condition(model, Dict(@varname(x) => 100.0));\n\njulia> m, x = conditioned_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue\n\njulia> # There's also an option using `|` by letting the right-hand side be a tuple\n       # with elements of type `Pair{<:VarName}`, i.e. `vn => value` with `vn isa VarName`.\n       conditioned_model_dict = model | (@varname(x) => 100.0, );\n\njulia> m, x = conditioned_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue Condition only a part of a multivariate variable Not only can be condition on multivariate random variables, but we can also use the standard mechanism of setting something to  missing  in the call to  condition  to only condition on a part of the variable. julia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 3 methods)\n\njulia> model = demo_mv();\n\njulia> conditioned_model = condition(model, m = [missing, 1.0]);\n\njulia> # (✓) `m[1]` sampled while `m[2]` is fixed\n       m = conditioned_model(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue Intuitively one might also expect to be able to write  model | (m[1] = 1.0, ) . Unfortunately this is not supported as it has the potential of increasing compilation times but without offering any benefit with respect to runtime: julia> # (×) `m[2]` is not set to 1.0.\n       m = condition(model, var\"m[2]\" = 1.0)(); m[2] == 1.0\nfalse But you  can  do this if you use a  Dict  as the underlying storage instead: julia> # Alternatives:\n       # - `model | (@varname(m[2]) => 1.0,)`\n       # - `condition(model, Dict(@varname(m[2] => 1.0)))`\n       # (✓) `m[2]` is set to 1.0.\n       m = condition(model, @varname(m[2]) => 1.0)(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue Nested models condition  of course also supports the use of nested models through the use of  @submodel . julia> @model demo_inner() = m ~ Normal()\ndemo_inner (generic function with 2 methods)\n\njulia> @model function demo_outer()\n           @submodel m = demo_inner()\n           return m\n       end\ndemo_outer (generic function with 2 methods)\n\njulia> model = demo_outer();\n\njulia> model() ≠ 1.0\ntrue\n\njulia> conditioned_model = model | (m = 1.0, );\n\njulia> conditioned_model()\n1.0 But one needs to be careful when prefixing variables in the nested models: julia> @model function demo_outer_prefix()\n           @submodel prefix=\"inner\" m = demo_inner()\n           return m\n       end\ndemo_outer_prefix (generic function with 2 methods)\n\njulia> # (×) This doesn't work now!\n       conditioned_model = demo_outer_prefix() | (m = 1.0, );\n\njulia> conditioned_model() == 1.0\nfalse\n\njulia> # (✓) `m` in `demo_inner` is referred to as `inner.m` internally, so we do:\n       conditioned_model = demo_outer_prefix() | (var\"inner.m\" = 1.0, );\n\njulia> conditioned_model()\n1.0\n\njulia> # Note that the above `var\"...\"` is just standard Julia syntax:\n       keys((var\"inner.m\" = 1.0, ))\n(Symbol(\"inner.m\"),) And similarly when using  Dict : julia> conditioned_model_dict = demo_outer_prefix() | (@varname(var\"inner.m\") => 1.0);\n\njulia> conditioned_model_dict()\n1.0 The difference is maybe more obvious once we look at how these different in their trace/ VarInfo : julia> keys(VarInfo(demo_outer()))\n1-element Vector{VarName{:m, Setfield.IdentityLens}}:\n m\n\njulia> keys(VarInfo(demo_outer_prefix()))\n1-element Vector{VarName{Symbol(\"inner.m\"), Setfield.IdentityLens}}:\n inner.m From this we can tell what the correct way to condition  m  within  demo_inner  is in the two different models. source condition([context::AbstractContext,] values::NamedTuple)\ncondition([context::AbstractContext]; values...) Return  ConditionContext  with  values  and  context  if  values  is non-empty, otherwise return  context  which is  DefaultContext  by default. See also:  decondition source"},{"id":25,"pagetitle":"API","title":"DynamicPPL.conditioned","ref":"/ppl/stable/api/#DynamicPPL.conditioned","content":" DynamicPPL.conditioned  —  Function conditioned(model::Model) Return  NamedTuple  of values that are conditioned on under  model . Examples julia> using Distributions\n\njulia> using DynamicPPL: conditioned, contextualize\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n       end\ndemo (generic function with 2 methods)\n\njulia> m = demo();\n\njulia> # Returns all the variables we have conditioned on + their values.\n       conditioned(condition(m, x=100.0, m=1.0))\n(x = 100.0, m = 1.0)\n\njulia> # Nested ones also work (note that `PrefixContext` does nothing to the result).\n       cm = condition(contextualize(m, PrefixContext{:a}(condition(m=1.0))), x=100.0);\n\njulia> conditioned(cm)\n(x = 100.0, m = 1.0)\n\njulia> # Since we conditioned on `m`, not `a.m` as it will appear after prefixed,\n       # `a.m` is treated as a random variable.\n       keys(VarInfo(cm))\n1-element Vector{VarName{Symbol(\"a.m\"), Setfield.IdentityLens}}:\n a.m\n\njulia> # If we instead condition on `a.m`, `m` in the model will be considered an observation.\n       cm = condition(contextualize(m, PrefixContext{:a}(condition(var\"a.m\"=1.0))), x=100.0);\n\njulia> conditioned(cm).x\n100.0\n\njulia> conditioned(cm).var\"a.m\"\n1.0\n\njulia> keys(VarInfo(cm)) # <= no variables are sampled\nAny[] source conditioned(context::AbstractContext) Return  NamedTuple  of values that are conditioned on under context`. Note that this will recursively traverse the context stack and return a merged version of the condition values. source Similarly, one can specify with  AbstractPPL.decondition  that certain, or all, random variables are not observed."},{"id":26,"pagetitle":"API","title":"AbstractPPL.decondition","ref":"/ppl/stable/api/#AbstractPPL.decondition","content":" AbstractPPL.decondition  —  Function decondition(model::Model)\ndecondition(model::Model, variables...) Return a  Model  for which  variables...  are  not  considered observations. If no  variables  are provided, then all variables currently considered observations will no longer be. This is essentially the inverse of  condition . This also means that it suffers from the same limitiations. Note that currently we only support  variables  to take on explicit values provided to `condition. Examples julia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> conditioned_model = condition(demo(), m = 1.0, x = 10.0);\n\njulia> conditioned_model()\n(m = 1.0, x = 10.0)\n\njulia> # By specifying the `VarName` to `decondition`.\n       model = decondition(conditioned_model, @varname(m));\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # When `NamedTuple` is used as the underlying, you can also provide\n       # the symbol directly (though the `@varname` approach is preferable if\n       # if the variable is known at compile-time).\n       model = decondition(conditioned_model, :m);\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # `decondition` multiple at once:\n       (m, x) = decondition(model, :m, :x)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # `decondition` without any symbols will `decondition` all variables.\n       (m, x) = decondition(model)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # Usage of `Val` to perform `decondition` at compile-time if possible\n       # is also supported.\n       model = decondition(conditioned_model, Val{:m}());\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue Similarly when using a  Dict : julia> conditioned_model_dict = condition(demo(), @varname(m) => 1.0, @varname(x) => 10.0);\n\njulia> conditioned_model_dict()\n(m = 1.0, x = 10.0)\n\njulia> deconditioned_model_dict = decondition(conditioned_model_dict, @varname(m));\n\njulia> (m, x) = deconditioned_model_dict(); m ≠ 1.0 && x == 10.0\ntrue But, as mentioned,  decondition  is only supported for variables explicitly provided to  condition  earlier; julia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 3 methods)\n\njulia> model = demo_mv();\n\njulia> conditioned_model = condition(model, @varname(m) => [1.0, 2.0]);\n\njulia> conditioned_model()\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> deconditioned_model = decondition(conditioned_model, @varname(m[1]));\n\njulia> deconditioned_model()  # (×) `m[1]` is still conditioned\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> # (✓) this works though\n       deconditioned_model_2 = deconditioned_model | (@varname(m[1]) => missing);\n\njulia> m = deconditioned_model_2(); (m[1] ≠ 1.0 && m[2] == 2.0)\ntrue source decondition(context::AbstractContext, syms...) Return  context  but with  syms  no longer conditioned on. Note that this recursively traverses contexts, deconditioning all along the way. See also:  condition source"},{"id":27,"pagetitle":"API","title":"Utilities","ref":"/ppl/stable/api/#Utilities","content":" Utilities It is possible to manually increase (or decrease) the accumulated log density from within a model function."},{"id":28,"pagetitle":"API","title":"DynamicPPL.@addlogprob!","ref":"/ppl/stable/api/#DynamicPPL.@addlogprob!","content":" DynamicPPL.@addlogprob!  —  Macro @addlogprob!(ex) Add the result of the evaluation of  ex  to the joint log probability. Examples This macro allows you to  include arbitrary terms in the likelihood julia> myloglikelihood(x, μ) = loglikelihood(Normal(μ, 1), x);\n\njulia> @model function demo(x)\n           μ ~ Normal()\n           @addlogprob! myloglikelihood(x, μ)\n       end;\n\njulia> x = [1.3, -2.1];\n\njulia> loglikelihood(demo(x), (μ=0.2,)) ≈ myloglikelihood(x, 0.2)\ntrue and to  reject samples : julia> @model function demo(x)\n           m ~ MvNormal(zero(x), I)\n           if dot(m, x) < 0\n               @addlogprob! -Inf\n               # Exit the model evaluation early\n               return\n           end\n           x ~ MvNormal(m, I)\n           return\n       end;\n\njulia> logjoint(demo([-2.1]), (m=[0.2],)) == -Inf\ntrue Note The  @addlogprob!  macro increases the accumulated log probability regardless of the evaluation context, i.e., regardless of whether you evaluate the log prior, the log likelihood or the log joint density. If you would like to avoid this behaviour you should check the evaluation context. It can be accessed with the internal variable  __context__ . For instance, in the following example the log density is not accumulated when only the log prior is computed:   julia> myloglikelihood(x, μ) = loglikelihood(Normal(μ, 1), x);\n\njulia> @model function demo(x)\n           μ ~ Normal()\n           if DynamicPPL.leafcontext(__context__) !== PriorContext()\n               @addlogprob! myloglikelihood(x, μ)\n           end\n       end;\n\njulia> x = [1.3, -2.1];\n\njulia> logprior(demo(x), (μ=0.2,)) ≈ logpdf(Normal(), 0.2)\ntrue\n\njulia> loglikelihood(demo(x), (μ=0.2,)) ≈ myloglikelihood(x, 0.2)\ntrue source Return values of the model function for a collection of samples can be obtained with  generated_quantities ."},{"id":29,"pagetitle":"API","title":"DynamicPPL.generated_quantities","ref":"/ppl/stable/api/#DynamicPPL.generated_quantities","content":" DynamicPPL.generated_quantities  —  Function generated_quantities(model::Model, chain::AbstractChains) Execute  model  for each of the samples in  chain  and return an array of the values returned by the  model  for each sample. Examples General Often you might have additional quantities computed inside the model that you want to inspect, e.g. @model function demo(x)\n    # sample and observe\n    θ ~ Prior()\n    x ~ Likelihood()\n    return interesting_quantity(θ, x)\nend\nm = demo(data)\nchain = sample(m, alg, n)\n# To inspect the `interesting_quantity(θ, x)` where `θ` is replaced by samples\n# from the posterior/`chain`:\ngenerated_quantities(m, chain) # <= results in a `Vector` of returned values\n                               #    from `interesting_quantity(θ, x)` Concrete (and simple) julia> using DynamicPPL, Turing\n\njulia> @model function demo(xs)\n           s ~ InverseGamma(2, 3)\n           m_shifted ~ Normal(10, √s)\n           m = m_shifted - 10\n\n           for i in eachindex(xs)\n               xs[i] ~ Normal(m, √s)\n           end\n\n           return (m, )\n       end\ndemo (generic function with 1 method)\n\njulia> model = demo(randn(10));\n\njulia> chain = sample(model, MH(), 10);\n\njulia> generated_quantities(model, chain)\n10×1 Array{Tuple{Float64},2}:\n (2.1964758025119338,)\n (2.1964758025119338,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.043088571494005024,)\n (-0.16489786710222099,)\n (-0.16489786710222099,) source generated_quantities(model::Model, parameters::NamedTuple)\ngenerated_quantities(model::Model, values, keys)\ngenerated_quantities(model::Model, values, keys) Execute  model  with variables  keys  set to  values  and return the values returned by the  model . If a  NamedTuple  is given,  keys=keys(parameters)  and  values=values(parameters) . Example julia> using DynamicPPL, Distributions\n\njulia> @model function demo(xs)\n           s ~ InverseGamma(2, 3)\n           m_shifted ~ Normal(10, √s)\n           m = m_shifted - 10\n           for i in eachindex(xs)\n               xs[i] ~ Normal(m, √s)\n           end\n           return (m, )\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo(randn(10));\n\njulia> parameters = (; s = 1.0, m_shifted=10);\n\njulia> generated_quantities(model, parameters)\n(0.0,)\n\njulia> generated_quantities(model, values(parameters), keys(parameters))\n(0.0,) source For a chain of samples, one can compute the pointwise log-likelihoods of each observed random variable with  pointwise_loglikelihoods ."},{"id":30,"pagetitle":"API","title":"DynamicPPL.pointwise_loglikelihoods","ref":"/ppl/stable/api/#DynamicPPL.pointwise_loglikelihoods","content":" DynamicPPL.pointwise_loglikelihoods  —  Function pointwise_loglikelihoods(model::Model, chain::Chains, keytype = String) Runs  model  on each sample in  chain  returning a  Dict{String, Matrix{Float64}}  with keys corresponding to symbols of the observations, and values being matrices of shape  (num_chains, num_samples) . keytype  specifies what the type of the keys used in the returned  Dict  are. Currently, only  String  and  VarName  are supported. Notes Say  y  is a  Vector  of  n  i.i.d.  Normal(μ, σ)  variables, with  μ  and  σ  both being  <:Real . Then the  observe  (i.e. when the left-hand side is an  observation ) statements can be implemented in three ways: using a  for  loop: for i in eachindex(y)\n    y[i] ~ Normal(μ, σ)\nend using  .~ : y .~ Normal(μ, σ) using  MvNormal : y ~ MvNormal(fill(μ, n), σ^2 * I) In (1) and (2),  y  will be treated as a collection of  n  i.i.d. 1-dimensional variables, while in (3)  y  will be treated as a  single  n-dimensional observation. This is important to keep in mind, in particular if the computation is used for downstream computations. Examples From chain julia> using DynamicPPL, Turing\n\njulia> @model function demo(xs, y)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, √s)\n           for i in eachindex(xs)\n               xs[i] ~ Normal(m, √s)\n           end\n\n           y ~ Normal(m, √s)\n       end\ndemo (generic function with 1 method)\n\njulia> model = demo(randn(3), randn());\n\njulia> chain = sample(model, MH(), 10);\n\njulia> pointwise_loglikelihoods(model, chain)\nDict{String,Array{Float64,2}} with 4 entries:\n  \"xs[3]\" => [-1.42862; -2.67573; … ; -1.66251; -1.66251]\n  \"xs[1]\" => [-1.42932; -2.68123; … ; -1.66333; -1.66333]\n  \"xs[2]\" => [-1.6724; -0.861339; … ; -1.62359; -1.62359]\n  \"y\"     => [-1.51265; -0.914129; … ; -1.5499; -1.5499]\n\njulia> pointwise_loglikelihoods(model, chain, String)\nDict{String,Array{Float64,2}} with 4 entries:\n  \"xs[3]\" => [-1.42862; -2.67573; … ; -1.66251; -1.66251]\n  \"xs[1]\" => [-1.42932; -2.68123; … ; -1.66333; -1.66333]\n  \"xs[2]\" => [-1.6724; -0.861339; … ; -1.62359; -1.62359]\n  \"y\"     => [-1.51265; -0.914129; … ; -1.5499; -1.5499]\n\njulia> pointwise_loglikelihoods(model, chain, VarName)\nDict{VarName,Array{Float64,2}} with 4 entries:\n  xs[2] => [-1.6724; -0.861339; … ; -1.62359; -1.62359]\n  y     => [-1.51265; -0.914129; … ; -1.5499; -1.5499]\n  xs[1] => [-1.42932; -2.68123; … ; -1.66333; -1.66333]\n  xs[3] => [-1.42862; -2.67573; … ; -1.66251; -1.66251] Broadcasting Note that  x .~ Dist()  will treat  x  as a collection of  independent  observations rather than as a single observation. julia> @model function demo(x)\n           x .~ Normal()\n       end;\n\njulia> m = demo([1.0, ]);\n\njulia> ℓ = pointwise_loglikelihoods(m, VarInfo(m)); first(ℓ[@varname(x[1])])\n-1.4189385332046727\n\njulia> m = demo([1.0; 1.0]);\n\njulia> ℓ = pointwise_loglikelihoods(m, VarInfo(m)); first.((ℓ[@varname(x[1])], ℓ[@varname(x[2])]))\n(-1.4189385332046727, -1.4189385332046727) source"},{"id":31,"pagetitle":"API","title":"DynamicPPL.NamedDist","ref":"/ppl/stable/api/#DynamicPPL.NamedDist","content":" DynamicPPL.NamedDist  —  Type A named distribution that carries the name of the random variable with it. source"},{"id":32,"pagetitle":"API","title":"Testing Utilities","ref":"/ppl/stable/api/#Testing-Utilities","content":" Testing Utilities DynamicPPL provides several demo models and helpers for testing samplers in the  DynamicPPL.TestUtils  submodule."},{"id":33,"pagetitle":"API","title":"DynamicPPL.TestUtils.test_sampler","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.test_sampler","content":" DynamicPPL.TestUtils.test_sampler  —  Function test_sampler(models, sampler, args...; kwargs...) Test that  sampler  produces correct marginal posterior means on each model in  models . In short, this method iterates through  models , calls  AbstractMCMC.sample  on the  model  and  sampler  to produce a  chain , and then checks  marginal_mean_of_samples(chain, vn)  for every (leaf) varname  vn  against the corresponding value returned by  posterior_mean  for each model. To change how comparison is done for a particular  chain  type, one can overload  marginal_mean_of_samples  for the corresponding type. Arguments models : A collection of instaces of  DynamicPPL.Model  to test on. sampler : The  AbstractMCMC.AbstractSampler  to test. args... : Arguments forwarded to  sample . Keyword arguments varnames_filter : A filter to apply to  varnames(model) , allowing comparison for only   a subset of the varnames. atol=1e-1 : Absolute tolerance used in  @test . rtol=1e-3 : Relative tolerance used in  @test . kwargs... : Keyword arguments forwarded to  sample . source"},{"id":34,"pagetitle":"API","title":"DynamicPPL.TestUtils.test_sampler_on_demo_models","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.test_sampler_on_demo_models","content":" DynamicPPL.TestUtils.test_sampler_on_demo_models  —  Function test_sampler_on_demo_models(meanfunction, sampler, args...; kwargs...) Test  sampler  on every model in  DEMO_MODELS . This is just a proxy for  test_sampler(meanfunction, DEMO_MODELS, sampler, args...; kwargs...) . source"},{"id":35,"pagetitle":"API","title":"DynamicPPL.TestUtils.test_sampler_continuous","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.test_sampler_continuous","content":" DynamicPPL.TestUtils.test_sampler_continuous  —  Function test_sampler_continuous(sampler, args...; kwargs...) Test that  sampler  produces the correct marginal posterior means on all models in  demo_models . As of right now, this is just an alias for  test_sampler_on_demo_models . source"},{"id":36,"pagetitle":"API","title":"DynamicPPL.TestUtils.marginal_mean_of_samples","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.marginal_mean_of_samples","content":" DynamicPPL.TestUtils.marginal_mean_of_samples  —  Function marginal_mean_of_samples(chain, varname) Return the mean of variable represented by  varname  in  chain . source"},{"id":37,"pagetitle":"API","title":"DynamicPPL.TestUtils.DEMO_MODELS","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.DEMO_MODELS","content":" DynamicPPL.TestUtils.DEMO_MODELS  —  Constant A collection of models corresponding to the posterior distribution defined by the generative process s ~ InverseGamma(2, 3)\nm ~ Normal(0, √s)\n1.5 ~ Normal(m, √s)\n2.0 ~ Normal(m, √s) or by s[1] ~ InverseGamma(2, 3)\ns[2] ~ InverseGamma(2, 3)\nm[1] ~ Normal(0, √s)\nm[2] ~ Normal(0, √s)\n1.5 ~ Normal(m[1], √s[1])\n2.0 ~ Normal(m[2], √s[2]) These are examples of a Normal-InverseGamma conjugate prior with Normal likelihood, for which the posterior is known in closed form. In particular, for the univariate model (the former one): mean(s) == 49 / 24\nmean(m) == 7 / 6 And for the multivariate one (the latter one): mean(s[1]) == 19 / 8\nmean(m[1]) == 3 / 4\nmean(s[2]) == 8 / 3\nmean(m[2]) == 1 source For every demo model, one can define the true log prior, log likelihood, and log joint probabilities."},{"id":38,"pagetitle":"API","title":"DynamicPPL.TestUtils.logprior_true","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.logprior_true","content":" DynamicPPL.TestUtils.logprior_true  —  Function logprior_true(model, args...) Return the  logprior  of  model  for  args . This should generally be implemented by hand for every specific  model . See also:  logjoint_true ,  loglikelihood_true . source"},{"id":39,"pagetitle":"API","title":"DynamicPPL.TestUtils.loglikelihood_true","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.loglikelihood_true","content":" DynamicPPL.TestUtils.loglikelihood_true  —  Function loglikelihood_true(model, args...) Return the  loglikelihood  of  model  for  args . This should generally be implemented by hand for every specific  model . See also:  logjoint_true ,  logprior_true . source"},{"id":40,"pagetitle":"API","title":"DynamicPPL.TestUtils.logjoint_true","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.logjoint_true","content":" DynamicPPL.TestUtils.logjoint_true  —  Function logjoint_true(model, args...) Return the  logjoint  of  model  for  args . Defaults to  logprior_true(model, args...) + loglikelihood_true(model, args..) . This should generally be implemented by hand for every specific  model  so that the returned value can be used as a ground-truth for testing things like: Validity of evaluation of  model  using a particular implementation of  AbstractVarInfo . Validity of a sampler when combined with DynamicPPL by running the sampler twice: once targeting ground-truth functions, e.g.  logjoint_true , and once targeting  model . And more. See also:  logprior_true ,  loglikelihood_true . source And in the case where the model includes constrained variables, it can also be useful to define"},{"id":41,"pagetitle":"API","title":"DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian","content":" DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian  —  Function logprior_true_with_logabsdet_jacobian(model::Model, args...) Return a tuple  (args_unconstrained, logprior_unconstrained)  of  model  for  args... . Unlike  logprior_true , the returned logprior computation includes the log-absdet-jacobian adjustment, thus computing logprior for the unconstrained variables. Note that  args  are assumed be in the support of  model , while  args_unconstrained  will be unconstrained. See also:  logprior_true . source"},{"id":42,"pagetitle":"API","title":"DynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian","content":" DynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian  —  Function logjoint_true_with_logabsdet_jacobian(model::Model, args...) Return a tuple  (args_unconstrained, logjoint)  of  model  for  args . Unlike  logjoint_true , the returned logjoint computation includes the log-absdet-jacobian adjustment, thus computing logjoint for the unconstrained variables. Note that  args  are assumed be in the support of  model , while  args_unconstrained  will be unconstrained. This should generally not be implemented directly, instead one should implement  logprior_true_with_logabsdet_jacobian  for a given  model . See also:  logjoint_true ,  logprior_true_with_logabsdet_jacobian . source Finally, the following methods can also be of use:"},{"id":43,"pagetitle":"API","title":"DynamicPPL.TestUtils.varnames","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.varnames","content":" DynamicPPL.TestUtils.varnames  —  Function varnames(model::Model) Return a collection of  VarName  as they are expected to appear in the model. Even though it is recommended to implement this by hand for a particular  Model , a default implementation using  SimpleVarInfo{<:Dict}  is provided. source"},{"id":44,"pagetitle":"API","title":"DynamicPPL.TestUtils.posterior_mean","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.posterior_mean","content":" DynamicPPL.TestUtils.posterior_mean  —  Function posterior_mean(model::Model) Return a  NamedTuple  compatible with  varnames(model)  where the values represent the posterior mean under  model . \"Compatible\" means that a  varname  from  varnames(model)  can be used to extract the corresponding value using  get , e.g.  get(posterior_mean(model), varname) . source"},{"id":45,"pagetitle":"API","title":"Advanced","ref":"/ppl/stable/api/#Advanced","content":" Advanced"},{"id":46,"pagetitle":"API","title":"Variable names","ref":"/ppl/stable/api/#Variable-names","content":" Variable names Names and possibly nested indices of variables are described with  AbstractPPL.VarName . They can be defined with  AbstractPPL.@varname . Please see the documentation of  AbstractPPL.jl  for further information."},{"id":47,"pagetitle":"API","title":"Data Structures of Variables","ref":"/ppl/stable/api/#Data-Structures-of-Variables","content":" Data Structures of Variables DynamicPPL provides different data structures for samples from the model and their log density. All of them are subtypes of  AbstractVarInfo ."},{"id":48,"pagetitle":"API","title":"DynamicPPL.AbstractVarInfo","ref":"/ppl/stable/api/#DynamicPPL.AbstractVarInfo","content":" DynamicPPL.AbstractVarInfo  —  Type AbstractVarInfo Abstract supertype for data structures that capture random variables when executing a probabilistic model and accumulate log densities such as the log likelihood or the log joint probability of the model. See also:  VarInfo source"},{"id":49,"pagetitle":"API","title":"Common API","ref":"/ppl/stable/api/#Common-API","content":" Common API"},{"id":50,"pagetitle":"API","title":"DynamicPPL.getlogp","ref":"/ppl/stable/api/#DynamicPPL.getlogp","content":" DynamicPPL.getlogp  —  Function getlogp(vi::VarInfo) Return the log of the joint probability of the observed data and parameters sampled in  vi . source"},{"id":51,"pagetitle":"API","title":"DynamicPPL.setlogp!!","ref":"/ppl/stable/api/#DynamicPPL.setlogp!!","content":" DynamicPPL.setlogp!!  —  Function setlogp!!(vi::VarInfo, logp) Set the log of the joint probability of the observed data and parameters sampled in  vi  to  logp , mutating if it makes sense. source"},{"id":52,"pagetitle":"API","title":"DynamicPPL.acclogp!!","ref":"/ppl/stable/api/#DynamicPPL.acclogp!!","content":" DynamicPPL.acclogp!!  —  Function acclogp!!(vi::VarInfo, logp) Add  logp  to the value of the log of the joint probability of the observed data and parameters sampled in  vi , mutating if it makes sense. source"},{"id":53,"pagetitle":"API","title":"DynamicPPL.resetlogp!!","ref":"/ppl/stable/api/#DynamicPPL.resetlogp!!","content":" DynamicPPL.resetlogp!!  —  Function resetlogp!!(vi::AbstractVarInfo) Reset the value of the log of the joint probability of the observed data and parameters sampled in  vi  to 0, mutating if it makes sense. source"},{"id":54,"pagetitle":"API","title":"Base.getindex","ref":"/ppl/stable/api/#Base.getindex","content":" Base.getindex  —  Function getindex(vi::VarInfo, vn::VarName)\ngetindex(vi::VarInfo, vns::Vector{<:VarName}) Return the current value(s) of  vn  ( vns ) in  vi  in the support of its (their) distribution(s). If the value(s) is (are) transformed to the Euclidean space, it is (they are) transformed back. source getindex(vi::VarInfo, spl::Union{SampleFromPrior, Sampler}) Return the current value(s) of the random variables sampled by  spl  in  vi . The value(s) may or may not be transformed to Euclidean space. source"},{"id":55,"pagetitle":"API","title":"BangBang.push!!","ref":"/ppl/stable/api/#BangBang.push!!","content":" BangBang.push!!  —  Function push!!(vi::VarInfo, vn::VarName, r, dist::Distribution) Push a new random variable  vn  with a sampled value  r  from a distribution  dist  to the  VarInfo vi , mutating if it makes sense. source push!!(vi::VarInfo, vn::VarName, r, dist::Distribution, spl::AbstractSampler) Push a new random variable  vn  with a sampled value  r  sampled with a sampler  spl  from a distribution  dist  to  VarInfo vi , if it makes sense. The sampler is passed here to invalidate its cache where defined. source push!!(vi::VarInfo, vn::VarName, r, dist::Distribution, gid::Selector) Push a new random variable  vn  with a sampled value  r  sampled with a sampler of selector  gid  from a distribution  dist  to  VarInfo vi . source"},{"id":56,"pagetitle":"API","title":"BangBang.empty!!","ref":"/ppl/stable/api/#BangBang.empty!!","content":" BangBang.empty!!  —  Function empty!!(vi::VarInfo) Empty the fields of  vi.metadata  and reset  vi.logp[]  and  vi.num_produce[]  to zeros. This is useful when using a sampling algorithm that assumes an empty  vi , e.g.  SMC . source"},{"id":57,"pagetitle":"API","title":"DynamicPPL.values_as","ref":"/ppl/stable/api/#DynamicPPL.values_as","content":" DynamicPPL.values_as  —  Function values_as(varinfo[, Type]) Return the values/realizations in  varinfo  as  Type , if implemented. If no  Type  is provided, return values as stored in  varinfo . Examples SimpleVarInfo  with  NamedTuple : julia> data = (x = 1.0, m = [2.0]);\n\njulia> values_as(SimpleVarInfo(data))\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), NamedTuple)\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), OrderedDict)\nOrderedDict{VarName{sym, Setfield.IdentityLens} where sym, Any} with 2 entries:\n  x => 1.0\n  m => [2.0] SimpleVarInfo  with  OrderedDict : julia> data = OrderedDict{Any,Any}(@varname(x) => 1.0, @varname(m) => [2.0]);\n\njulia> values_as(SimpleVarInfo(data))\nOrderedDict{Any, Any} with 2 entries:\n  x => 1.0\n  m => [2.0]\n\njulia> values_as(SimpleVarInfo(data), NamedTuple)\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), OrderedDict)\nOrderedDict{Any, Any} with 2 entries:\n  x => 1.0\n  m => [2.0] TypedVarInfo : julia> # Just use an example model to construct the `VarInfo` because we're lazy.\n       vi = VarInfo(DynamicPPL.TestUtils.demo_assume_dot_observe());\n\njulia> vi[@varname(s)] = 1.0; vi[@varname(m)] = 2.0;\n\njulia> # For the sake of brevity, let's just check the type.\n       md = values_as(vi); md.s isa DynamicPPL.Metadata\ntrue\n\njulia> values_as(vi, NamedTuple)\n(s = 1.0, m = 2.0)\n\njulia> values_as(vi, OrderedDict)\nOrderedDict{VarName{sym, Setfield.IdentityLens} where sym, Float64} with 2 entries:\n  s => 1.0\n  m => 2.0 UntypedVarInfo : julia> # Just use an example model to construct the `VarInfo` because we're lazy.\n       vi = VarInfo(); DynamicPPL.TestUtils.demo_assume_dot_observe()(vi);\n\njulia> vi[@varname(s)] = 1.0; vi[@varname(m)] = 2.0;\n\njulia> # For the sake of brevity, let's just check the type.\n       values_as(vi) isa DynamicPPL.Metadata\ntrue\n\njulia> values_as(vi, NamedTuple)\n(s = 1.0, m = 2.0)\n\njulia> values_as(vi, OrderedDict)\nOrderedDict{VarName{sym, Setfield.IdentityLens} where sym, Float64} with 2 entries:\n  s => 1.0\n  m => 2.0 source"},{"id":58,"pagetitle":"API","title":"SimpleVarInfo","ref":"/ppl/stable/api/#SimpleVarInfo","content":" SimpleVarInfo"},{"id":59,"pagetitle":"API","title":"DynamicPPL.SimpleVarInfo","ref":"/ppl/stable/api/#DynamicPPL.SimpleVarInfo","content":" DynamicPPL.SimpleVarInfo  —  Type struct SimpleVarInfo{NT, T, C<:DynamicPPL.AbstractTransformation} <: AbstractVarInfo A simple wrapper of the parameters with a  logp  field for accumulation of the logdensity. Currently only implemented for  NT<:NamedTuple  and  NT<:AbstractDict . Fields values underlying representation of the realization represented logp holds the accumulated log-probability transformation represents whether it assumes variables to be transformed Notes The major differences between this and  TypedVarInfo  are: SimpleVarInfo  does not require linearization. SimpleVarInfo  can use more efficient bijectors. SimpleVarInfo  is only type-stable if  NT<:NamedTuple  and either a) no indexing is used in tilde-statements, or b) the values have been specified with the correct shapes. Examples General usage julia> using StableRNGs\n\njulia> @model function demo()\n           m ~ Normal()\n           x = Vector{Float64}(undef, 2)\n           for i in eachindex(x)\n               x[i] ~ Normal()\n           end\n           return x\n       end\ndemo (generic function with 2 methods)\n\njulia> m = demo();\n\njulia> rng = StableRNG(42);\n\njulia> ### Sampling ###\n       ctx = SamplingContext(rng, SampleFromPrior(), DefaultContext());\n\njulia> # In the `NamedTuple` version we need to provide the place-holder values for\n       # the variables which are using \"containers\", e.g. `Array`.\n       # In this case, this means that we need to specify `x` but not `m`.\n       _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo((x = ones(2), )), ctx);\n\njulia> # (✓) Vroom, vroom! FAST!!!\n       vi[@varname(x[1])]\n0.4471218424633827\n\njulia> # We can also access arbitrary varnames pointing to `x`, e.g.\n       vi[@varname(x)]\n2-element Vector{Float64}:\n 0.4471218424633827\n 1.3736306979834252\n\njulia> vi[@varname(x[1:2])]\n2-element Vector{Float64}:\n 0.4471218424633827\n 1.3736306979834252\n\njulia> # (×) If we don't provide the container...\n       _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo(), ctx); vi\nERROR: type NamedTuple has no field x\n[...]\n\njulia> # If one does not know the varnames, we can use a `OrderedDict` instead.\n       _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo{Float64}(OrderedDict()), ctx);\n\njulia> # (✓) Sort of fast, but only possible at runtime.\n       vi[@varname(x[1])]\n-1.019202452456547\n\njulia> # In addtion, we can only access varnames as they appear in the model!\n       vi[@varname(x)]\nERROR: KeyError: key x not found\n[...]\n\njulia> vi[@varname(x[1:2])]\nERROR: KeyError: key x[1:2] not found\n[...] Technically , it's possible to use any implementation of  AbstractDict  in place of  OrderedDict , but  OrderedDict  ensures that certain operations, e.g. linearization/flattening of the values in the varinfo, are consistent between evaluations. Hence  OrderedDict  is the preferred implementation of  AbstractDict  to use here. You can also sample in  transformed  space: julia> @model demo_constrained() = x ~ Exponential()\ndemo_constrained (generic function with 2 methods)\n\njulia> m = demo_constrained();\n\njulia> _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo(), ctx);\n\njulia> vi[@varname(x)] # (✓) 0 ≤ x < ∞\n1.8632965762164932\n\njulia> _, vi = DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(), true), ctx);\n\njulia> vi[@varname(x)] # (✓) -∞ < x < ∞\n-0.21080155351918753\n\njulia> xs = [last(DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(), true), ctx))[@varname(x)] for i = 1:10];\n\njulia> any(xs .< 0)  # (✓) Positive probability mass on negative numbers!\ntrue\n\njulia> # And with `OrderedDict` of course!\n       _, vi = DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(OrderedDict()), true), ctx);\n\njulia> vi[@varname(x)] # (✓) -∞ < x < ∞\n0.6225185067787314\n\njulia> xs = [last(DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(), true), ctx))[@varname(x)] for i = 1:10];\n\njulia> any(xs .< 0) # (✓) Positive probability mass on negative numbers!\ntrue Evaluation in transformed space of course also works: julia> vi = DynamicPPL.settrans!!(SimpleVarInfo((x = -1.0,)), true)\nTransformed SimpleVarInfo((x = -1.0,), 0.0)\n\njulia> # (✓) Positive probability mass on negative numbers!\n       getlogp(last(DynamicPPL.evaluate!!(m, vi, DynamicPPL.DefaultContext())))\n-1.3678794411714423\n\njulia> # While if we forget to make indicate that it's transformed:\n       vi = DynamicPPL.settrans!!(SimpleVarInfo((x = -1.0,)), false)\nSimpleVarInfo((x = -1.0,), 0.0)\n\njulia> # (✓) No probability mass on negative numbers!\n       getlogp(last(DynamicPPL.evaluate!!(m, vi, DynamicPPL.DefaultContext())))\n-Inf Indexing Using  NamedTuple  as underlying storage. julia> svi_nt = SimpleVarInfo((m = (a = [1.0], ), ));\n\njulia> svi_nt[@varname(m)]\n(a = [1.0],)\n\njulia> svi_nt[@varname(m.a)]\n1-element Vector{Float64}:\n 1.0\n\njulia> svi_nt[@varname(m.a[1])]\n1.0\n\njulia> svi_nt[@varname(m.a[2])]\nERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]\n[...]\n\njulia> svi_nt[@varname(m.b)]\nERROR: type NamedTuple has no field b\n[...] Using  OrderedDict  as underlying storage. julia> svi_dict = SimpleVarInfo(OrderedDict(@varname(m) => (a = [1.0], )));\n\njulia> svi_dict[@varname(m)]\n(a = [1.0],)\n\njulia> svi_dict[@varname(m.a)]\n1-element Vector{Float64}:\n 1.0\n\njulia> svi_dict[@varname(m.a[1])]\n1.0\n\njulia> svi_dict[@varname(m.a[2])]\nERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]\n[...]\n\njulia> svi_dict[@varname(m.b)]\nERROR: type NamedTuple has no field b\n[...] source"},{"id":60,"pagetitle":"API","title":"VarInfo","ref":"/ppl/stable/api/#VarInfo","content":" VarInfo Another data structure is  VarInfo ."},{"id":61,"pagetitle":"API","title":"DynamicPPL.VarInfo","ref":"/ppl/stable/api/#DynamicPPL.VarInfo","content":" DynamicPPL.VarInfo  —  Type struct VarInfo{Tmeta, Tlogp} <: AbstractVarInfo\n    metadata::Tmeta\n    logp::Base.RefValue{Tlogp}\n    num_produce::Base.RefValue{Int}\nend A light wrapper over one or more instances of  Metadata . Let  vi  be an instance of  VarInfo . If  vi isa VarInfo{<:Metadata} , then only one  Metadata  instance is used for all the sybmols.  VarInfo{<:Metadata}  is aliased  UntypedVarInfo . If  vi isa VarInfo{<:NamedTuple} , then  vi.metadata  is a  NamedTuple  that maps each symbol used on the LHS of  ~  in the model to its  Metadata  instance. The latter allows for the type specialization of  vi  after the first sampling iteration when all the symbols have been observed.  VarInfo{<:NamedTuple}  is aliased  TypedVarInfo . Note: It is the user's responsibility to ensure that each \"symbol\" is visited at least once whenever the model is called, regardless of any stochastic branching. Each symbol refers to a Julia variable and can be a hierarchical array of many random variables, e.g.  x[1] ~ ...  and  x[2] ~ ...  both have the same symbol  x . source"},{"id":62,"pagetitle":"API","title":"DynamicPPL.TypedVarInfo","ref":"/ppl/stable/api/#DynamicPPL.TypedVarInfo","content":" DynamicPPL.TypedVarInfo  —  Type TypedVarInfo(vi::UntypedVarInfo) This function finds all the unique  sym s from the instances of  VarName{sym}  found in  vi.metadata.vns . It then extracts the metadata associated with each symbol from the global  vi.metadata  field. Finally, a new  VarInfo  is created with a new  metadata  as a  NamedTuple  mapping from symbols to type-stable  Metadata  instances, one for each symbol. source One main characteristic of  VarInfo  is that samples are stored in a linearized form."},{"id":63,"pagetitle":"API","title":"DynamicPPL.tonamedtuple","ref":"/ppl/stable/api/#DynamicPPL.tonamedtuple","content":" DynamicPPL.tonamedtuple  —  Function tonamedtuple(vi::VarInfo) Convert a  vi  into a  NamedTuple  where each variable symbol maps to the values and  indexing string of the variable. For example, a model that had a vector of vector-valued variables  x  would return (x = ([1.5, 2.0], [3.0, 1.0], [\"x[1]\", \"x[2]\"]), ) source"},{"id":64,"pagetitle":"API","title":"DynamicPPL.link!","ref":"/ppl/stable/api/#DynamicPPL.link!","content":" DynamicPPL.link!  —  Function link!(vi::VarInfo, spl::Sampler) Transform the values of the random variables sampled by  spl  in  vi  from the support of their distributions to the Euclidean space and set their corresponding  \"trans\"  flag values to  true . source"},{"id":65,"pagetitle":"API","title":"DynamicPPL.invlink!","ref":"/ppl/stable/api/#DynamicPPL.invlink!","content":" DynamicPPL.invlink!  —  Function invlink!(vi::VarInfo, spl::AbstractSampler) Transform the values of the random variables sampled by  spl  in  vi  from the Euclidean space back to the support of their distributions and sets their corresponding  \"trans\"  flag values to  false . source"},{"id":66,"pagetitle":"API","title":"DynamicPPL.istrans","ref":"/ppl/stable/api/#DynamicPPL.istrans","content":" DynamicPPL.istrans  —  Function istrans(vi::AbstractVarInfo) Return  true  if  vi  is working in unconstrained space, and  false  if  vi  is assuming realizations to be in support of the corresponding distributions. source istrans(vi::VarInfo, vn::VarName) Return true if  vn 's values in  vi  are transformed to Euclidean space, and false if they are in the support of  vn 's distribution. source"},{"id":67,"pagetitle":"API","title":"DynamicPPL.set_flag!","ref":"/ppl/stable/api/#DynamicPPL.set_flag!","content":" DynamicPPL.set_flag!  —  Function set_flag!(vi::VarInfo, vn::VarName, flag::String) Set  vn 's value for  flag  to  true  in  vi . source"},{"id":68,"pagetitle":"API","title":"DynamicPPL.unset_flag!","ref":"/ppl/stable/api/#DynamicPPL.unset_flag!","content":" DynamicPPL.unset_flag!  —  Function unset_flag!(vi::VarInfo, vn::VarName, flag::String) Set  vn 's value for  flag  to  false  in  vi . source"},{"id":69,"pagetitle":"API","title":"DynamicPPL.is_flagged","ref":"/ppl/stable/api/#DynamicPPL.is_flagged","content":" DynamicPPL.is_flagged  —  Function is_flagged(vi::VarInfo, vn::VarName, flag::String) Check whether  vn  has a true value for  flag  in  vi . source For Gibbs sampling the following functions were added."},{"id":70,"pagetitle":"API","title":"DynamicPPL.setgid!","ref":"/ppl/stable/api/#DynamicPPL.setgid!","content":" DynamicPPL.setgid!  —  Function setgid!(vi::VarInfo, gid::Selector, vn::VarName) Add  gid  to the set of sampler selectors associated with  vn  in  vi . source"},{"id":71,"pagetitle":"API","title":"DynamicPPL.updategid!","ref":"/ppl/stable/api/#DynamicPPL.updategid!","content":" DynamicPPL.updategid!  —  Function updategid!(vi::VarInfo, vn::VarName, spl::Sampler) Set  vn 's  gid  to  Set([spl.selector]) , if  vn  does not have a sampler selector linked and  vn 's symbol is in the space of  spl . source The following functions were used for sequential Monte Carlo methods."},{"id":72,"pagetitle":"API","title":"DynamicPPL.get_num_produce","ref":"/ppl/stable/api/#DynamicPPL.get_num_produce","content":" DynamicPPL.get_num_produce  —  Function get_num_produce(vi::VarInfo) Return the  num_produce  of  vi . source"},{"id":73,"pagetitle":"API","title":"DynamicPPL.set_num_produce!","ref":"/ppl/stable/api/#DynamicPPL.set_num_produce!","content":" DynamicPPL.set_num_produce!  —  Function set_num_produce!(vi::VarInfo, n::Int) Set the  num_produce  field of  vi  to  n . source"},{"id":74,"pagetitle":"API","title":"DynamicPPL.increment_num_produce!","ref":"/ppl/stable/api/#DynamicPPL.increment_num_produce!","content":" DynamicPPL.increment_num_produce!  —  Function increment_num_produce!(vi::VarInfo) Add 1 to  num_produce  in  vi . source"},{"id":75,"pagetitle":"API","title":"DynamicPPL.reset_num_produce!","ref":"/ppl/stable/api/#DynamicPPL.reset_num_produce!","content":" DynamicPPL.reset_num_produce!  —  Function reset_num_produce!(vi::AbstractVarInfo) Reset the value of  num_produce  the log of the joint probability of the observed data and parameters sampled in  vi  to 0. source"},{"id":76,"pagetitle":"API","title":"DynamicPPL.setorder!","ref":"/ppl/stable/api/#DynamicPPL.setorder!","content":" DynamicPPL.setorder!  —  Function setorder!(vi::VarInfo, vn::VarName, index::Int) Set the  order  of  vn  in  vi  to  index , where  order  is the number of  observe statements run before sampling vn`. source"},{"id":77,"pagetitle":"API","title":"DynamicPPL.set_retained_vns_del_by_spl!","ref":"/ppl/stable/api/#DynamicPPL.set_retained_vns_del_by_spl!","content":" DynamicPPL.set_retained_vns_del_by_spl!  —  Function set_retained_vns_del_by_spl!(vi::VarInfo, spl::Sampler) Set the  \"del\"  flag of variables in  vi  with  order > vi.num_produce[]  to  true . source"},{"id":78,"pagetitle":"API","title":"Base.empty!","ref":"/ppl/stable/api/#Base.empty!","content":" Base.empty!  —  Function empty!(meta::Metadata) Empty the fields of  meta . This is useful when using a sampling algorithm that assumes an empty  meta , e.g.  SMC . source"},{"id":79,"pagetitle":"API","title":"Evaluation Contexts","ref":"/ppl/stable/api/#Evaluation-Contexts","content":" Evaluation Contexts Internally, both sampling and evaluation of log densities are performed with  AbstractPPL.evaluate!! ."},{"id":80,"pagetitle":"API","title":"AbstractPPL.evaluate!!","ref":"/ppl/stable/api/#AbstractPPL.evaluate!!","content":" AbstractPPL.evaluate!!  —  Function evaluate!!(model::Model[, rng, varinfo, sampler, context]) Sample from the  model  using the  sampler  with random number generator  rng  and the  context , and store the sample and log joint probability in  varinfo . Returns both the return-value of the original model, and the resulting varinfo. The method resets the log joint probability of  varinfo  and increases the evaluation number of  sampler . source The behaviour of a model execution can be changed with evaluation contexts that are passed as additional argument to the model function. Contexts are subtypes of  AbstractPPL.AbstractContext ."},{"id":81,"pagetitle":"API","title":"DynamicPPL.SamplingContext","ref":"/ppl/stable/api/#DynamicPPL.SamplingContext","content":" DynamicPPL.SamplingContext  —  Type SamplingContext(\n        [rng::Random.AbstractRNG=Random.GLOBAL_RNG],\n        [sampler::AbstractSampler=SampleFromPrior()],\n        [context::AbstractContext=DefaultContext()],\n) Create a context that allows you to sample parameters with the  sampler  when running the model. The  context  determines how the returned log density is computed when running the model. See also:  DefaultContext ,  LikelihoodContext ,  PriorContext source"},{"id":82,"pagetitle":"API","title":"DynamicPPL.DefaultContext","ref":"/ppl/stable/api/#DynamicPPL.DefaultContext","content":" DynamicPPL.DefaultContext  —  Type struct DefaultContext <: AbstractContext end The  DefaultContext  is used by default to compute log the joint probability of the data  and parameters when running the model. source"},{"id":83,"pagetitle":"API","title":"DynamicPPL.LikelihoodContext","ref":"/ppl/stable/api/#DynamicPPL.LikelihoodContext","content":" DynamicPPL.LikelihoodContext  —  Type struct LikelihoodContext{Tvars} <: AbstractContext\n    vars::Tvars\nend The  LikelihoodContext  enables the computation of the log likelihood of the parameters when  running the model.  vars  can be used to evaluate the log likelihood for specific values  of the model's parameters. If  vars  is  nothing , the parameter values inside the  VarInfo  will be used by default. source"},{"id":84,"pagetitle":"API","title":"DynamicPPL.PriorContext","ref":"/ppl/stable/api/#DynamicPPL.PriorContext","content":" DynamicPPL.PriorContext  —  Type struct PriorContext{Tvars} <: AbstractContext\n    vars::Tvars\nend The  PriorContext  enables the computation of the log prior of the parameters  vars  when  running the model. source"},{"id":85,"pagetitle":"API","title":"DynamicPPL.MiniBatchContext","ref":"/ppl/stable/api/#DynamicPPL.MiniBatchContext","content":" DynamicPPL.MiniBatchContext  —  Type struct MiniBatchContext{Tctx, T} <: AbstractContext\n    context::Tctx\n    loglike_scalar::T\nend The  MiniBatchContext  enables the computation of   log(prior) + s * log(likelihood of a batch)  when running the model, where  s  is the   loglike_scalar  field, typically equal to  the number of data points / batch size .  This is useful in batch-based stochastic gradient descent algorithms to be optimizing   log(prior) + log(likelihood of all the data points)  in the expectation. source"},{"id":86,"pagetitle":"API","title":"DynamicPPL.PrefixContext","ref":"/ppl/stable/api/#DynamicPPL.PrefixContext","content":" DynamicPPL.PrefixContext  —  Type PrefixContext{Prefix}(context) Create a context that allows you to use the wrapped  context  when running the model and adds the  Prefix  to all parameters. This context is useful in nested models to ensure that the names of the parameters are unique. See also:  @submodel source"},{"id":87,"pagetitle":"API","title":"Samplers","ref":"/ppl/stable/api/#Samplers","content":" Samplers In DynamicPPL two samplers are defined that are used to initialize unobserved random variables:  SampleFromPrior  which samples from the prior distribution, and  SampleFromUniform  which samples from a uniform distribution."},{"id":88,"pagetitle":"API","title":"DynamicPPL.SampleFromPrior","ref":"/ppl/stable/api/#DynamicPPL.SampleFromPrior","content":" DynamicPPL.SampleFromPrior  —  Type SampleFromPrior Sampling algorithm that samples unobserved random variables from their prior distribution. source"},{"id":89,"pagetitle":"API","title":"DynamicPPL.SampleFromUniform","ref":"/ppl/stable/api/#DynamicPPL.SampleFromUniform","content":" DynamicPPL.SampleFromUniform  —  Type SampleFromUniform Sampling algorithm that samples unobserved random variables from a uniform distribution. References Stan reference manual source Additionally, a generic sampler for inference is implemented."},{"id":90,"pagetitle":"API","title":"DynamicPPL.Sampler","ref":"/ppl/stable/api/#DynamicPPL.Sampler","content":" DynamicPPL.Sampler  —  Type Sampler{T} Generic sampler type for inference algorithms of type  T  in DynamicPPL. Sampler  should implement the AbstractMCMC interface, and in particular  AbstractMCMC.step . A default implementation of the initial sampling step is provided that supports resuming sampling from a previous state and setting initial parameter values. It requires to overload  loadstate  and  initialstep  for loading previous states and actually performing the initial sampling step, respectively. Additionally, sometimes one might want to implement  initialsampler  that specifies how the initial parameter values are sampled if they are not provided. By default, values are sampled from the prior. source The default implementation of  Sampler  uses the following unexported functions."},{"id":91,"pagetitle":"API","title":"DynamicPPL.initialstep","ref":"/ppl/stable/api/#DynamicPPL.initialstep","content":" DynamicPPL.initialstep  —  Function initialstep(rng, model, sampler, varinfo; kwargs...) Perform the initial sampling step of the  sampler  for the  model . The  varinfo  contains the initial samples, which can be provided by the user or sampled randomly. source"},{"id":92,"pagetitle":"API","title":"DynamicPPL.loadstate","ref":"/ppl/stable/api/#DynamicPPL.loadstate","content":" DynamicPPL.loadstate  —  Function loadstate(data) Load sampler state from  data . source"},{"id":93,"pagetitle":"API","title":"DynamicPPL.initialsampler","ref":"/ppl/stable/api/#DynamicPPL.initialsampler","content":" DynamicPPL.initialsampler  —  Function initialsampler(sampler::Sampler) Return the sampler that is used for generating the initial parameters when sampling with  sampler . By default, it returns an instance of  SampleFromPrior . source"},{"id":94,"pagetitle":"API","title":"Model-Internal Functions","ref":"/ppl/stable/api/#model_internal","content":" Model-Internal Functions"},{"id":95,"pagetitle":"API","title":"DynamicPPL.tilde_assume","ref":"/ppl/stable/api/#DynamicPPL.tilde_assume","content":" DynamicPPL.tilde_assume  —  Function tilde_assume(context::SamplingContext, right, vn, vi) Handle assumed variables, e.g.,  x ~ Normal()  (where  x  does occur in the model inputs), accumulate the log probability, and return the sampled value with a context associated with a sampler. Falls back to tilde_assume(context.rng, context.context, context.sampler, right, vn, vi) source"},{"id":96,"pagetitle":"API","title":"DynamicPPL.dot_tilde_assume","ref":"/ppl/stable/api/#DynamicPPL.dot_tilde_assume","content":" DynamicPPL.dot_tilde_assume  —  Function dot_tilde_assume(context::SamplingContext, right, left, vn, vi) Handle broadcasted assumed variables, e.g.,  x .~ MvNormal()  (where  x  does not occur in the model inputs), accumulate the log probability, and return the sampled value for a context associated with a sampler. Falls back to dot_tilde_assume(context.rng, context.context, context.sampler, right, left, vn, vi) source"},{"id":97,"pagetitle":"API","title":"DynamicPPL.tilde_observe","ref":"/ppl/stable/api/#DynamicPPL.tilde_observe","content":" DynamicPPL.tilde_observe  —  Function tilde_observe(context::SamplingContext, right, left, vi) Handle observed constants with a  context  associated with a sampler. Falls back to  tilde_observe(context.context, context.sampler, right, left, vi) . source"},{"id":98,"pagetitle":"API","title":"DynamicPPL.dot_tilde_observe","ref":"/ppl/stable/api/#DynamicPPL.dot_tilde_observe","content":" DynamicPPL.dot_tilde_observe  —  Function dot_tilde_observe(context::SamplingContext, right, left, vi) Handle broadcasted observed constants, e.g.,  [1.0] .~ MvNormal() , accumulate the log probability, and return the observed value for a context associated with a sampler. Falls back to  dot_tilde_observe(context.context, context.sampler, right, left, vi) . source"},{"id":102,"pagetitle":"AdvancedHMC.jl","title":"AdvancedHMC.jl","ref":"/hmc/stable/#AdvancedHMC.jl","content":" AdvancedHMC.jl AdvancedHMC.jl provides a robust, modular and efficient implementation of advanced HMC algorithms. An illustrative example for AdvancedHMC's usage is given below. AdvancedHMC.jl is part of  Turing.jl , a probabilistic programming library in Julia.  If you are interested in using AdvancedHMC.jl through a probabilistic programming language, please check it out! Interfaces IMP.hmc : an experimental Python module for the Integrative Modeling Platform, which uses AdvancedHMC in its backend to sample protein structures. NEWS We presented a paper for AdvancedHMC.jl at  AABI  2019 in Vancouver, Canada. ( abs ,  pdf ,  OpenReview ) We presented a poster for AdvancedHMC.jl at  StanCon 2019  in Cambridge, UK. ( pdf ) API CHANGES [v0.2.22] Three functions are renamed. Preconditioner(metric::AbstractMetric)  ->  MassMatrixAdaptor(metric)  and  NesterovDualAveraging(δ, integrator::AbstractIntegrator)  ->  StepSizeAdaptor(δ, integrator) find_good_eps  ->  find_good_stepsize [v0.2.15]  n_adapts  is no longer needed to construct  StanHMCAdaptor ; the old constructor is deprecated. [v0.2.8] Two Hamiltonian trajectory sampling methods are renamed to avoid a name clash with Distributions. Multinomial  ->  MultinomialTS Slice  ->  SliceTS [v0.2.0] The gradient function passed to  Hamiltonian  is supposed to return a value-gradient tuple now."},{"id":103,"pagetitle":"AdvancedHMC.jl","title":"A minimal example - sampling from a multivariate Gaussian using NUTS","ref":"/hmc/stable/#A-minimal-example-sampling-from-a-multivariate-Gaussian-using-NUTS","content":" A minimal example - sampling from a multivariate Gaussian using NUTS using AdvancedHMC, Distributions, ForwardDiff\nusing LinearAlgebra\n\n# Choose parameter dimensionality and initial parameter value\nD = 10; initial_θ = rand(D)\n\n# Define the target distribution\nℓπ(θ) = logpdf(MvNormal(zeros(D), I), θ)\n\n# Set the number of samples to draw and warmup iterations\nn_samples, n_adapts = 2_000, 1_000\n\n# Define a Hamiltonian system\nmetric = DiagEuclideanMetric(D)\nhamiltonian = Hamiltonian(metric, ℓπ, ForwardDiff)\n\n# Define a leapfrog solver, with initial step size chosen heuristically\ninitial_ϵ = find_good_stepsize(hamiltonian, initial_θ)\nintegrator = Leapfrog(initial_ϵ)\n\n# Define an HMC sampler, with the following components\n#   - multinomial sampling scheme,\n#   - generalised No-U-Turn criteria, and\n#   - windowed adaption for step-size and diagonal mass matrix\nproposal = NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)\nadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))\n\n# Run the sampler to draw samples from the specified Gaussian, where\n#   - `samples` will store the samples\n#   - `stats` will store diagnostic statistics for each sample\nsamples, stats = sample(hamiltonian, proposal, initial_θ, n_samples, adaptor, n_adapts; progress=true)"},{"id":104,"pagetitle":"AdvancedHMC.jl","title":"Parallel sampling","ref":"/hmc/stable/#Parallel-sampling","content":" Parallel sampling AdvancedHMC enables parallel sampling (either distributed or multi-thread) via Julia's  parallel computing functions . It also supports vectorized sampling for static HMC and has been discussed in more detail in the documentation  here . The below example utilizes the  @threads  macro to sample 4 chains across 4 threads. # Ensure that julia was launched with appropriate number of threads\nprintln(Threads.nthreads())\n\n# Number of chains to sample\nnchains = 4\n\n# Cache to store the chains\nchains = Vector{Any}(undef, nchains)\n\n# The `samples` from each parallel chain is stored in the `chains` vector \n# Adjust the `verbose` flag as per need\nThreads.@threads for i in 1:nchains\n  samples, stats = sample(hamiltonian, proposal, initial_θ, n_samples, adaptor, n_adapts; verbose=false)\n  chains[i] = samples\nend"},{"id":105,"pagetitle":"AdvancedHMC.jl","title":"GPU Sampling with CUDA","ref":"/hmc/stable/#GPU-Sampling-with-CUDA","content":" GPU Sampling with CUDA There is experimental support for running static HMC on the GPU using CUDA.  To do so the user needs to have  CUDA.jl  installed, ensure the logdensity of the  Hamiltonian  can be executed on the GPU and that the initial points are a  CuArray .  A small working example can be found at  test/cuda.jl ."},{"id":106,"pagetitle":"AdvancedHMC.jl","title":"API and supported HMC algorithms","ref":"/hmc/stable/#API-and-supported-HMC-algorithms","content":" API and supported HMC algorithms An important design goal of AdvancedHMC.jl is modularity; we would like to support algorithmic research on HMC. This modularity means that different HMC variants can be easily constructed by composing various components, such as preconditioning metric (i.e. mass matrix), leapfrog integrators,  trajectories (static or dynamic), and adaption schemes etc.  The minimal example above can be modified to suit particular inference problems by picking components from the list below."},{"id":107,"pagetitle":"AdvancedHMC.jl","title":"Hamiltonian mass matrix ( metric )","ref":"/hmc/stable/#Hamiltonian-mass-matrix-(metric)","content":" Hamiltonian mass matrix ( metric ) Unit metric:  UnitEuclideanMetric(dim) Diagonal metric:  DiagEuclideanMetric(dim) Dense metric:  DenseEuclideanMetric(dim) where  dim  is the dimensionality of the sampling space."},{"id":108,"pagetitle":"AdvancedHMC.jl","title":"Integrator ( integrator )","ref":"/hmc/stable/#Integrator-(integrator)","content":" Integrator ( integrator ) Ordinary leapfrog integrator:  Leapfrog(ϵ) Jittered leapfrog integrator with jitter rate  n :  JitteredLeapfrog(ϵ, n) Tempered leapfrog integrator with tempering rate  a :  TemperedLeapfrog(ϵ, a) where  ϵ  is the step size of leapfrog integration."},{"id":109,"pagetitle":"AdvancedHMC.jl","title":"Proposal ( proposal )","ref":"/hmc/stable/#Proposal-(proposal)","content":" Proposal ( proposal ) Static HMC with a fixed number of steps ( n_steps ) (Neal, R. M. (2011)):  StaticTrajectory(integrator, n_steps) HMC with a fixed total trajectory length ( trajectory_length ) (Neal, R. M. (2011)):  HMCDA(integrator, trajectory_length) Original NUTS with slice sampling (Hoffman, M. D., & Gelman, A. (2014)):  NUTS{SliceTS,ClassicNoUTurn}(integrator) Generalised NUTS with slice sampling (Betancourt, M. (2017)):  NUTS{SliceTS,GeneralisedNoUTurn}(integrator) Original NUTS with multinomial sampling (Betancourt, M. (2017)):  NUTS{MultinomialTS,ClassicNoUTurn}(integrator) Generalised NUTS with multinomial sampling (Betancourt, M. (2017)):  NUTS{MultinomialTS,GeneralisedNoUTurn}(integrator)"},{"id":110,"pagetitle":"AdvancedHMC.jl","title":"Adaptor ( adaptor )","ref":"/hmc/stable/#Adaptor-(adaptor)","content":" Adaptor ( adaptor ) Adapt the mass matrix  metric  of the Hamiltonian dynamics:  mma = MassMatrixAdaptor(metric) This is lowered to  UnitMassMatrix ,  WelfordVar  or  WelfordCov  based on the type of the mass matrix  metric Adapt the step size of the leapfrog integrator  integrator :  ssa = StepSizeAdaptor(δ, integrator) It uses Nesterov's dual averaging with  δ  as the target acceptance rate. Combine the two above  naively :  NaiveHMCAdaptor(mma, ssa) Combine the first two using Stan's windowed adaptation:  StanHMCAdaptor(mma, ssa)"},{"id":111,"pagetitle":"AdvancedHMC.jl","title":"Gradients","ref":"/hmc/stable/#Gradients","content":" Gradients AdvancedHMC  supports both AD-based ( Zygote ,  Tracker  and  ForwardDiff ) and user-specified gradients. In order to use user-specified gradients, please replace  ForwardDiff  with  ℓπ_grad  in the  Hamiltonian   constructor, where the gradient function  ℓπ_grad  should return a tuple containing both the log-posterior and its gradient.  All the combinations are tested in  this file  except from using tempered leapfrog integrator together with adaptation, which we found unstable empirically."},{"id":112,"pagetitle":"AdvancedHMC.jl","title":"The  sample  function signature in detail","ref":"/hmc/stable/#The-sample-function-signature-in-detail","content":" The  sample  function signature in detail function sample(\n    rng::Union{AbstractRNG, AbstractVector{<:AbstractRNG}},\n    h::Hamiltonian,\n    κ::HMCKernel,\n    θ::AbstractVector{<:AbstractFloat},\n    n_samples::Int,\n    adaptor::AbstractAdaptor=NoAdaptation(),\n    n_adapts::Int=min(div(n_samples, 10), 1_000);\n    drop_warmup=false,\n    verbose::Bool=true,\n    progress::Bool=false,\n) Draw  n_samples  samples using the proposal  κ  under the Hamiltonian system  h The randomness is controlled by  rng . If  rng  is not provided,  GLOBAL_RNG  will be used. The initial point is given by  θ . The adaptor is set by  adaptor , for which the default is no adaptation. It will perform  n_adapts  steps of adaptation, for which the default is  1_000  or 10% of  n_samples , whichever is lower.  drop_warmup  specifies whether to drop samples. verbose  controls the verbosity. progress  controls whether to show the progress meter or not. Note that the function signature of the  sample  function exported by  AdvancedHMC.jl  differs from the  sample  function used by  Turing.jl . We refer to the documentation of  Turing.jl  for more details on the latter."},{"id":113,"pagetitle":"AdvancedHMC.jl","title":"Citing AdvancedHMC.jl","ref":"/hmc/stable/#Citing-AdvancedHMC.jl","content":" Citing AdvancedHMC.jl If you use AdvancedHMC.jl for your own research, please consider citing the following publication: Kai Xu, Hong Ge, Will Tebbutt, Mohamed Tarek, Martin Trapp, Zoubin Ghahramani: \"AdvancedHMC.jl: A robust, modular and efficient implementation of advanced HMC algorithms.\",  Symposium on Advances in Approximate Bayesian Inference , 2020. ( abs ,  pdf ) with the following BibTeX entry: @inproceedings{xu2020advancedhmc,\n  title={AdvancedHMC. jl: A robust, modular and efficient implementation of advanced HMC algorithms},\n  author={Xu, Kai and Ge, Hong and Tebbutt, Will and Tarek, Mohamed and Trapp, Martin and Ghahramani, Zoubin},\n  booktitle={Symposium on Advances in Approximate Bayesian Inference},\n  pages={1--10},\n  year={2020},\n  organization={PMLR}\n} If you using AdvancedHMC.jl directly through Turing.jl, please consider citing the following publication: Hong Ge, Kai Xu, and Zoubin Ghahramani: \"Turing: a language for flexible probabilistic inference.\",  International Conference on Artificial Intelligence and Statistics , 2018. ( abs ,  pdf ) with the following BibTeX entry: @inproceedings{ge2018turing,\n  title={Turing: A language for flexible probabilistic inference},\n  author={Ge, Hong and Xu, Kai and Ghahramani, Zoubin},\n  booktitle={International Conference on Artificial Intelligence and Statistics},\n  pages={1682--1690},\n  year={2018},\n  organization={PMLR}\n}"},{"id":114,"pagetitle":"AdvancedHMC.jl","title":"References","ref":"/hmc/stable/#References","content":" References Neal, R. M. (2011). MCMC using Hamiltonian dynamics. Handbook of Markov chain Monte Carlo, 2(11), 2. ( arXiv ) Betancourt, M. (2017). A Conceptual Introduction to Hamiltonian Monte Carlo.  arXiv preprint arXiv:1701.02434 . Girolami, M., & Calderhead, B. (2011). Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2), 123-214. ( arXiv ) Betancourt, M. J., Byrne, S., & Girolami, M. (2014). Optimizing the integrator step size for Hamiltonian Monte Carlo.  arXiv preprint arXiv:1411.6669 . Betancourt, M. (2016). Identifying the optimal integration time in Hamiltonian Monte Carlo.  arXiv preprint arXiv:1601.00225 . Hoffman, M. D., & Gelman, A. (2014). The No-U-Turn Sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1), 1593-1623. ( arXiv )"},{"id":117,"pagetitle":"AdvancedHMC.jl","title":"AdvancedHMC.jl","ref":"/hmc/stable/api/#AdvancedHMC.jl","content":" AdvancedHMC.jl Documentation for AdvancedHMC.jl AdvancedHMC.jl Structs Functions Index AdvancedHMC.jl A minimal example - sampling from a multivariate Gaussian using NUTS API and supported HMC algorithms The  sample  function signature in detail Citing AdvancedHMC.jl References"},{"id":118,"pagetitle":"AdvancedHMC.jl","title":"Structs","ref":"/hmc/stable/api/#Structs","content":" Structs"},{"id":119,"pagetitle":"AdvancedHMC.jl","title":"AdvancedHMC.ClassicNoUTurn","ref":"/hmc/stable/api/#AdvancedHMC.ClassicNoUTurn","content":" AdvancedHMC.ClassicNoUTurn  —  Type struct ClassicNoUTurn{F<:AbstractFloat} <: AdvancedHMC.DynamicTerminationCriterion Classic No-U-Turn criterion as described in Eq. (9) in [1]. Informally, this will terminate the trajectory expansion if continuing the simulation either forwards or backwards in time will decrease the distance between the left-most and right-most positions. Fields max_depth::Int64 Δ_max::AbstractFloat References Hoffman, M. D., & Gelman, A. (2014). The No-U-Turn Sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1), 1593-1623. ( arXiv ) source"},{"id":120,"pagetitle":"AdvancedHMC.jl","title":"Functions","ref":"/hmc/stable/api/#Functions","content":" Functions"},{"id":121,"pagetitle":"AdvancedHMC.jl","title":"StatsBase.sample","ref":"/hmc/stable/api/#StatsBase.sample","content":" StatsBase.sample  —  Function sample(\n    rng::AbstractRNG,\n    h::Hamiltonian,\n    κ::AbstractMCMCKernel,\n    θ::AbstractVecOrMat{T},\n    n_samples::Int,\n    adaptor::AbstractAdaptor=NoAdaptation(),\n    n_adapts::Int=min(div(n_samples, 10), 1_000);\n    drop_warmup::Bool=false,\n    verbose::Bool=true,\n    progress::Bool=false\n) Sample  n_samples  samples using the proposal  κ  under Hamiltonian  h . The randomness is controlled by  rng .  If  rng  is not provided,  GLOBAL_RNG  will be used. The initial point is given by  θ . The adaptor is set by  adaptor , for which the default is no adaptation. It will perform  n_adapts  steps of adaptation, for which the default is the minimum of  1_000  and 10% of  n_samples drop_warmup  controls to drop the samples during adaptation phase or not verbose  controls the verbosity progress  controls whether to show the progress meter or not source sample(model::DifferentiableDensityModel, kernel::AdvancedHMC.AbstractMCMCKernel, metric::AdvancedHMC.AbstractMetric, adaptor::AdvancedHMC.Adaptation.AbstractAdaptor, N::Integer; kwargs...) -> Any\n A convenient wrapper around  AbstractMCMC.sample  avoiding explicit construction of  HMCSampler . source"},{"id":122,"pagetitle":"AdvancedHMC.jl","title":"Index","ref":"/hmc/stable/api/#Index","content":" Index AdvancedHMC.ClassicNoUTurn StatsBase.sample"}]
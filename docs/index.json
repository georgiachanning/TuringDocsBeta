[{"id":22,"pagetitle":"Home","title":"DynamicPPL.jl","ref":"/ppl/stable/#DynamicPPL.jl","content":" DynamicPPL.jl A domain-specific language and backend for probabilistic programming languages, used by  Turing.jl ."},{"id":25,"pagetitle":"API","title":"API","ref":"/ppl/stable/api/#API","content":" API Part of the API of DynamicPPL is defined in the more lightweight interface package  AbstractPPL.jl  and reexported here."},{"id":26,"pagetitle":"API","title":"Model","ref":"/ppl/stable/api/#Model","content":" Model"},{"id":27,"pagetitle":"API","title":"Macros","ref":"/ppl/stable/api/#Macros","content":" Macros A core component of DynamicPPL is the  @model  macro. It can be used to define probabilistic models in an intuitive way by specifying random variables and their distributions with  ~  statements. These statements are rewritten by  @model  as calls of  internal functions  for sampling the variables and computing their log densities."},{"id":28,"pagetitle":"API","title":"DynamicPPL.@model","ref":"/ppl/stable/api/#DynamicPPL.@model","content":" DynamicPPL.@model  —  Macro @model(expr[, warn = false]) Macro to specify a probabilistic model. If  warn  is  true , a warning is displayed if internal variable names are used in the model definition. Examples Model definition: @model function model(x, y = 42)\n    ...\nend To generate a  Model , call  model(xvalue)  or  model(xvalue, yvalue) . source One can nest models and call another model inside the model function with  @submodel ."},{"id":29,"pagetitle":"API","title":"DynamicPPL.@submodel","ref":"/ppl/stable/api/#DynamicPPL.@submodel","content":" DynamicPPL.@submodel  —  Macro @submodel model\n@submodel ... = model Run a Turing  model  nested inside of a Turing model. Examples julia> @model function demo1(x)\n           x ~ Normal()\n           return 1 + abs(x)\n       end;\n\njulia> @model function demo2(x, y)\n            @submodel a = demo1(x)\n            return y ~ Uniform(0, a)\n       end; When we sample from the model  demo2(missing, 0.4)  random variable  x  will be sampled: julia> vi = VarInfo(demo2(missing, 0.4));\n\njulia> @varname(x) in keys(vi)\ntrue Variable  a  is not tracked since it can be computed from the random variable  x  that was tracked when running  demo1 : julia> @varname(a) in keys(vi)\nfalse We can check that the log joint probability of the model accumulated in  vi  is correct: julia> x = vi[@varname(x)];\n\njulia> getlogp(vi) ≈ logpdf(Normal(), x) + logpdf(Uniform(0, 1 + abs(x)), 0.4)\ntrue source @submodel prefix=... model\n@submodel prefix=... ... = model Run a Turing  model  nested inside of a Turing model and add \" prefix .\" as a prefix to all random variables inside of the  model . Valid expressions for  prefix=...  are: prefix=false : no prefix is used. prefix=true :  attempt  to automatically determine the prefix from the left-hand side  ... = model  by first converting into a  VarName , and then calling  Symbol  on this. prefix=expression : results in the prefix  Symbol(expression) . The prefix makes it possible to run the same Turing model multiple times while keeping track of all random variables correctly. Examples Example models julia> @model function demo1(x)\n           x ~ Normal()\n           return 1 + abs(x)\n       end;\n\njulia> @model function demo2(x, y, z)\n            @submodel prefix=\"sub1\" a = demo1(x)\n            @submodel prefix=\"sub2\" b = demo1(y)\n            return z ~ Uniform(-a, b)\n       end; When we sample from the model  demo2(missing, missing, 0.4)  random variables  sub1.x  and  sub2.x  will be sampled: julia> vi = VarInfo(demo2(missing, missing, 0.4));\n\njulia> @varname(var\"sub1.x\") in keys(vi)\ntrue\n\njulia> @varname(var\"sub2.x\") in keys(vi)\ntrue Variables  a  and  b  are not tracked since they can be computed from the random variables  sub1.x  and  sub2.x  that were tracked when running  demo1 : julia> @varname(a) in keys(vi)\nfalse\n\njulia> @varname(b) in keys(vi)\nfalse We can check that the log joint probability of the model accumulated in  vi  is correct: julia> sub1_x = vi[@varname(var\"sub1.x\")];\n\njulia> sub2_x = vi[@varname(var\"sub2.x\")];\n\njulia> logprior = logpdf(Normal(), sub1_x) + logpdf(Normal(), sub2_x);\n\njulia> loglikelihood = logpdf(Uniform(-1 - abs(sub1_x), 1 + abs(sub2_x)), 0.4);\n\njulia> getlogp(vi) ≈ logprior + loglikelihood\ntrue Different ways of setting the prefix julia> @model inner() = x ~ Normal()\ninner (generic function with 2 methods)\n\njulia> # When `prefix` is unspecified, no prefix is used.\n       @model outer() = @submodel a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(x) in keys(VarInfo(outer()))\ntrue\n\njulia> # Explicitely don't use any prefix.\n       @model outer() = @submodel prefix=false a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(x) in keys(VarInfo(outer()))\ntrue\n\njulia> # Automatically determined from `a`.\n       @model outer() = @submodel prefix=true a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"a.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # Using a static string.\n       @model outer() = @submodel prefix=\"my prefix\" a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"my prefix.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # Using string interpolation.\n       @model outer() = @submodel prefix=\"$(nameof(inner()))\" a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"inner.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # Or using some arbitrary expression.\n       @model outer() = @submodel prefix=1 + 2 a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"3.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # (×) Automatic prefixing without a left-hand side expression does not work!\n       @model outer() = @submodel prefix=true inner()\nERROR: LoadError: cannot automatically prefix with no left-hand side\n[...] Notes The choice  prefix=expression  means that the prefixing will incur a runtime cost. This is also the case for  prefix=true , depending on whether the expression on the the right-hand side of  ... = model  requires runtime-information or not, e.g.  x = model  will result in the  static  prefix  x , while  x[i] = model  will be resolved at runtime. source"},{"id":30,"pagetitle":"API","title":"Type","ref":"/ppl/stable/api/#Type","content":" Type A  Model  can be created by calling the model function, as defined by  @model ."},{"id":31,"pagetitle":"API","title":"DynamicPPL.Model","ref":"/ppl/stable/api/#DynamicPPL.Model","content":" DynamicPPL.Model  —  Type struct Model{F,argnames,defaultnames,missings,Targs,Tdefaults}\n    f::F\n    args::NamedTuple{argnames,Targs}\n    defaults::NamedTuple{defaultnames,Tdefaults}\nend A  Model  struct with model evaluation function of type  F , arguments of names  argnames  types  Targs , default arguments of names  defaultnames  with types  Tdefaults , and missing arguments  missings . Here  argnames ,  defaultargnames , and  missings  are tuples of symbols, e.g.  (:a, :b) . An argument with a type of  Missing  will be in  missings  by default. However, in non-traditional use-cases  missings  can be defined differently. All variables in  missings  are treated as random variables rather than observations. The default arguments are used internally when constructing instances of the same model with different arguments. Examples julia> Model(f, (x = 1.0, y = 2.0))\nModel{typeof(f),(:x, :y),(),(),Tuple{Float64,Float64},Tuple{}}(f, (x = 1.0, y = 2.0), NamedTuple())\n\njulia> Model(f, (x = 1.0, y = 2.0), (x = 42,))\nModel{typeof(f),(:x, :y),(:x,),(),Tuple{Float64,Float64},Tuple{Int64}}(f, (x = 1.0, y = 2.0), (x = 42,))\n\njulia> Model{(:y,)}(f, (x = 1.0, y = 2.0), (x = 42,)) # with special definition of missings\nModel{typeof(f),(:x, :y),(:x,),(:y,),Tuple{Float64,Float64},Tuple{Int64}}(f, (x = 1.0, y = 2.0), (x = 42,)) source Model s are callable structs."},{"id":32,"pagetitle":"API","title":"DynamicPPL.Model","ref":"/ppl/stable/api/#DynamicPPL.Model-Tuple{}","content":" DynamicPPL.Model  —  Method (model::Model)([rng, varinfo, sampler, context]) Sample from the  model  using the  sampler  with random number generator  rng  and the  context , and store the sample and log joint probability in  varinfo . The method resets the log joint probability of  varinfo  and increases the evaluation number of  sampler . source Basic properties of a model can be accessed with  getargnames ,  getmissings , and  nameof ."},{"id":33,"pagetitle":"API","title":"Base.nameof","ref":"/ppl/stable/api/#Base.nameof-Tuple{Model}","content":" Base.nameof  —  Method nameof(model::Model) Get the name of the  model  as  Symbol . source"},{"id":34,"pagetitle":"API","title":"DynamicPPL.getargnames","ref":"/ppl/stable/api/#DynamicPPL.getargnames","content":" DynamicPPL.getargnames  —  Function getargnames(model::Model) Get a tuple of the argument names of the  model . source"},{"id":35,"pagetitle":"API","title":"DynamicPPL.getmissings","ref":"/ppl/stable/api/#DynamicPPL.getmissings","content":" DynamicPPL.getmissings  —  Function getmissings(model::Model) Get a tuple of the names of the missing arguments of the  model . source"},{"id":36,"pagetitle":"API","title":"Evaluation","ref":"/ppl/stable/api/#Evaluation","content":" Evaluation With  rand  one can draw samples from the prior distribution of a  Model ."},{"id":37,"pagetitle":"API","title":"Base.rand","ref":"/ppl/stable/api/#Base.rand","content":" Base.rand  —  Function rand([rng=Random.GLOBAL_RNG], [T=NamedTuple], model::Model) Generate a sample of type  T  from the prior distribution of the  model . source One can also evaluate the log prior, log likelihood, and log joint probability."},{"id":38,"pagetitle":"API","title":"DynamicPPL.logprior","ref":"/ppl/stable/api/#DynamicPPL.logprior","content":" DynamicPPL.logprior  —  Function logprior(model::Model, varinfo::AbstractVarInfo) Return the log prior probability of variables  varinfo  for the probabilistic  model . See also  logjoint  and  loglikelihood . source logprior(model::Model, θ) Return the log prior probability of variables  θ  for the probabilistic  model . See also  logjoint  and  loglikelihood . Examples julia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       logprior(demo([1.0]), (m = 100.0, ))\n-5000.918938533205\n\njulia> # Using a `Dict`.\n       logprior(demo([1.0]), Dict(@varname(m) => 100.0))\n-5000.918938533205\n\njulia> # Truth.\n       logpdf(Normal(), 100.0)\n-5000.918938533205 source"},{"id":39,"pagetitle":"API","title":"StatsAPI.loglikelihood","ref":"/ppl/stable/api/#StatsAPI.loglikelihood","content":" StatsAPI.loglikelihood  —  Function loglikelihood(model::Model, varinfo::AbstractVarInfo) Return the log likelihood of variables  varinfo  for the probabilistic  model . See also  logjoint  and  logprior . source loglikelihood(model::Model, θ) Return the log likelihood of variables  θ  for the probabilistic  model . See also  logjoint  and  logprior . Examples julia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       loglikelihood(demo([1.0]), (m = 100.0, ))\n-4901.418938533205\n\njulia> # Using a `Dict`.\n       loglikelihood(demo([1.0]), Dict(@varname(m) => 100.0))\n-4901.418938533205\n\njulia> # Truth.\n       logpdf(Normal(100.0, 1.0), 1.0)\n-4901.418938533205 source"},{"id":40,"pagetitle":"API","title":"DynamicPPL.logjoint","ref":"/ppl/stable/api/#DynamicPPL.logjoint","content":" DynamicPPL.logjoint  —  Function logjoint(model::Model, varinfo::AbstractVarInfo) Return the log joint probability of variables  varinfo  for the probabilistic  model . See  logjoint  and  loglikelihood . source logjoint(model::Model, θ) Return the log joint probability of variables  θ  for the probabilistic  model . See  logjoint  and  loglikelihood . Examples julia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       logjoint(demo([1.0]), (m = 100.0, ))\n-9902.33787706641\n\njulia> # Using a `Dict`.\n       logjoint(demo([1.0]), Dict(@varname(m) => 100.0))\n-9902.33787706641\n\njulia> # Truth.\n       logpdf(Normal(100.0, 1.0), 1.0) + logpdf(Normal(), 100.0)\n-9902.33787706641 source"},{"id":41,"pagetitle":"API","title":"Condition and decondition","ref":"/ppl/stable/api/#Condition-and-decondition","content":" Condition and decondition A  Model  can be conditioned on a set of observations with  AbstractPPL.condition  or its alias  | ."},{"id":42,"pagetitle":"API","title":"Base.:|","ref":"/ppl/stable/api/#Base.:|-Tuple{Model, Any}","content":" Base.:|  —  Method model | (x = 1.0, ...) Return a  Model  which now treats variables on the right-hand side as observations. See  condition  for more information and examples. source"},{"id":43,"pagetitle":"API","title":"AbstractPPL.condition","ref":"/ppl/stable/api/#AbstractPPL.condition","content":" AbstractPPL.condition  —  Function condition(model::Model; values...)\ncondition(model::Model, values::NamedTuple) Return a  Model  which now treats the variables in  values  as observations. See also:  decondition ,  conditioned Limitations This does currently  not  work with variables that are provided to the model as arguments, e.g.  @model function demo(x) ... end  means that  condition  will not affect the variable  x . Therefore if one wants to make use of  condition  and  decondition  one should not be specifying any random variables as arguments. This is done for the sake of backwards compatibility. Examples Simple univariate model julia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo();\n\njulia> m, x = model(); (m ≠ 1.0 && x ≠ 100.0)\ntrue\n\njulia> # Create a new instance which treats `x` as observed\n       # with value `100.0`, and similarly for `m=1.0`.\n       conditioned_model = condition(model, x=100.0, m=1.0);\n\njulia> m, x = conditioned_model(); (m == 1.0 && x == 100.0)\ntrue\n\njulia> # Let's only condition on `x = 100.0`.\n       conditioned_model = condition(model, x = 100.0);\n\njulia> m, x =conditioned_model(); (m ≠ 1.0 && x == 100.0)\ntrue\n\njulia> # We can also use the nicer `|` syntax.\n       conditioned_model = model | (x = 100.0, );\n\njulia> m, x = conditioned_model(); (m ≠ 1.0 && x == 100.0)\ntrue The above uses a  NamedTuple  to hold the conditioning variables, which allows us to perform some additional optimizations; in many cases, the above has zero runtime-overhead. But we can also use a  Dict , which offers more flexibility in the conditioning (see examples further below) but generally has worse performance than the  NamedTuple  approach: julia> conditioned_model_dict = condition(model, Dict(@varname(x) => 100.0));\n\njulia> m, x = conditioned_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue\n\njulia> # There's also an option using `|` by letting the right-hand side be a tuple\n       # with elements of type `Pair{<:VarName}`, i.e. `vn => value` with `vn isa VarName`.\n       conditioned_model_dict = model | (@varname(x) => 100.0, );\n\njulia> m, x = conditioned_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue Condition only a part of a multivariate variable Not only can be condition on multivariate random variables, but we can also use the standard mechanism of setting something to  missing  in the call to  condition  to only condition on a part of the variable. julia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 3 methods)\n\njulia> model = demo_mv();\n\njulia> conditioned_model = condition(model, m = [missing, 1.0]);\n\njulia> # (✓) `m[1]` sampled while `m[2]` is fixed\n       m = conditioned_model(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue Intuitively one might also expect to be able to write  model | (m[1] = 1.0, ) . Unfortunately this is not supported as it has the potential of increasing compilation times but without offering any benefit with respect to runtime: julia> # (×) `m[2]` is not set to 1.0.\n       m = condition(model, var\"m[2]\" = 1.0)(); m[2] == 1.0\nfalse But you  can  do this if you use a  Dict  as the underlying storage instead: julia> # Alternatives:\n       # - `model | (@varname(m[2]) => 1.0,)`\n       # - `condition(model, Dict(@varname(m[2] => 1.0)))`\n       # (✓) `m[2]` is set to 1.0.\n       m = condition(model, @varname(m[2]) => 1.0)(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue Nested models condition  of course also supports the use of nested models through the use of  @submodel . julia> @model demo_inner() = m ~ Normal()\ndemo_inner (generic function with 2 methods)\n\njulia> @model function demo_outer()\n           @submodel m = demo_inner()\n           return m\n       end\ndemo_outer (generic function with 2 methods)\n\njulia> model = demo_outer();\n\njulia> model() ≠ 1.0\ntrue\n\njulia> conditioned_model = model | (m = 1.0, );\n\njulia> conditioned_model()\n1.0 But one needs to be careful when prefixing variables in the nested models: julia> @model function demo_outer_prefix()\n           @submodel prefix=\"inner\" m = demo_inner()\n           return m\n       end\ndemo_outer_prefix (generic function with 2 methods)\n\njulia> # (×) This doesn't work now!\n       conditioned_model = demo_outer_prefix() | (m = 1.0, );\n\njulia> conditioned_model() == 1.0\nfalse\n\njulia> # (✓) `m` in `demo_inner` is referred to as `inner.m` internally, so we do:\n       conditioned_model = demo_outer_prefix() | (var\"inner.m\" = 1.0, );\n\njulia> conditioned_model()\n1.0\n\njulia> # Note that the above `var\"...\"` is just standard Julia syntax:\n       keys((var\"inner.m\" = 1.0, ))\n(Symbol(\"inner.m\"),) And similarly when using  Dict : julia> conditioned_model_dict = demo_outer_prefix() | (@varname(var\"inner.m\") => 1.0);\n\njulia> conditioned_model_dict()\n1.0 The difference is maybe more obvious once we look at how these different in their trace/ VarInfo : julia> keys(VarInfo(demo_outer()))\n1-element Vector{VarName{:m, Setfield.IdentityLens}}:\n m\n\njulia> keys(VarInfo(demo_outer_prefix()))\n1-element Vector{VarName{Symbol(\"inner.m\"), Setfield.IdentityLens}}:\n inner.m From this we can tell what the correct way to condition  m  within  demo_inner  is in the two different models. source condition([context::AbstractContext,] values::NamedTuple)\ncondition([context::AbstractContext]; values...) Return  ConditionContext  with  values  and  context  if  values  is non-empty, otherwise return  context  which is  DefaultContext  by default. See also:  decondition source"},{"id":44,"pagetitle":"API","title":"DynamicPPL.conditioned","ref":"/ppl/stable/api/#DynamicPPL.conditioned","content":" DynamicPPL.conditioned  —  Function conditioned(model::Model) Return  NamedTuple  of values that are conditioned on under  model . Examples julia> using Distributions\n\njulia> using DynamicPPL: conditioned, contextualize\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n       end\ndemo (generic function with 2 methods)\n\njulia> m = demo();\n\njulia> # Returns all the variables we have conditioned on + their values.\n       conditioned(condition(m, x=100.0, m=1.0))\n(x = 100.0, m = 1.0)\n\njulia> # Nested ones also work (note that `PrefixContext` does nothing to the result).\n       cm = condition(contextualize(m, PrefixContext{:a}(condition(m=1.0))), x=100.0);\n\njulia> conditioned(cm)\n(x = 100.0, m = 1.0)\n\njulia> # Since we conditioned on `m`, not `a.m` as it will appear after prefixed,\n       # `a.m` is treated as a random variable.\n       keys(VarInfo(cm))\n1-element Vector{VarName{Symbol(\"a.m\"), Setfield.IdentityLens}}:\n a.m\n\njulia> # If we instead condition on `a.m`, `m` in the model will be considered an observation.\n       cm = condition(contextualize(m, PrefixContext{:a}(condition(var\"a.m\"=1.0))), x=100.0);\n\njulia> conditioned(cm).x\n100.0\n\njulia> conditioned(cm).var\"a.m\"\n1.0\n\njulia> keys(VarInfo(cm)) # <= no variables are sampled\nAny[] source conditioned(context::AbstractContext) Return  NamedTuple  of values that are conditioned on under context`. Note that this will recursively traverse the context stack and return a merged version of the condition values. source Similarly, one can specify with  AbstractPPL.decondition  that certain, or all, random variables are not observed."},{"id":45,"pagetitle":"API","title":"AbstractPPL.decondition","ref":"/ppl/stable/api/#AbstractPPL.decondition","content":" AbstractPPL.decondition  —  Function decondition(model::Model)\ndecondition(model::Model, variables...) Return a  Model  for which  variables...  are  not  considered observations. If no  variables  are provided, then all variables currently considered observations will no longer be. This is essentially the inverse of  condition . This also means that it suffers from the same limitiations. Note that currently we only support  variables  to take on explicit values provided to `condition. Examples julia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> conditioned_model = condition(demo(), m = 1.0, x = 10.0);\n\njulia> conditioned_model()\n(m = 1.0, x = 10.0)\n\njulia> # By specifying the `VarName` to `decondition`.\n       model = decondition(conditioned_model, @varname(m));\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # When `NamedTuple` is used as the underlying, you can also provide\n       # the symbol directly (though the `@varname` approach is preferable if\n       # if the variable is known at compile-time).\n       model = decondition(conditioned_model, :m);\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # `decondition` multiple at once:\n       (m, x) = decondition(model, :m, :x)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # `decondition` without any symbols will `decondition` all variables.\n       (m, x) = decondition(model)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # Usage of `Val` to perform `decondition` at compile-time if possible\n       # is also supported.\n       model = decondition(conditioned_model, Val{:m}());\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue Similarly when using a  Dict : julia> conditioned_model_dict = condition(demo(), @varname(m) => 1.0, @varname(x) => 10.0);\n\njulia> conditioned_model_dict()\n(m = 1.0, x = 10.0)\n\njulia> deconditioned_model_dict = decondition(conditioned_model_dict, @varname(m));\n\njulia> (m, x) = deconditioned_model_dict(); m ≠ 1.0 && x == 10.0\ntrue But, as mentioned,  decondition  is only supported for variables explicitly provided to  condition  earlier; julia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 3 methods)\n\njulia> model = demo_mv();\n\njulia> conditioned_model = condition(model, @varname(m) => [1.0, 2.0]);\n\njulia> conditioned_model()\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> deconditioned_model = decondition(conditioned_model, @varname(m[1]));\n\njulia> deconditioned_model()  # (×) `m[1]` is still conditioned\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> # (✓) this works though\n       deconditioned_model_2 = deconditioned_model | (@varname(m[1]) => missing);\n\njulia> m = deconditioned_model_2(); (m[1] ≠ 1.0 && m[2] == 2.0)\ntrue source decondition(context::AbstractContext, syms...) Return  context  but with  syms  no longer conditioned on. Note that this recursively traverses contexts, deconditioning all along the way. See also:  condition source"},{"id":46,"pagetitle":"API","title":"Utilities","ref":"/ppl/stable/api/#Utilities","content":" Utilities It is possible to manually increase (or decrease) the accumulated log density from within a model function."},{"id":47,"pagetitle":"API","title":"DynamicPPL.@addlogprob!","ref":"/ppl/stable/api/#DynamicPPL.@addlogprob!","content":" DynamicPPL.@addlogprob!  —  Macro @addlogprob!(ex) Add the result of the evaluation of  ex  to the joint log probability. Examples This macro allows you to  include arbitrary terms in the likelihood julia> myloglikelihood(x, μ) = loglikelihood(Normal(μ, 1), x);\n\njulia> @model function demo(x)\n           μ ~ Normal()\n           @addlogprob! myloglikelihood(x, μ)\n       end;\n\njulia> x = [1.3, -2.1];\n\njulia> loglikelihood(demo(x), (μ=0.2,)) ≈ myloglikelihood(x, 0.2)\ntrue and to  reject samples : julia> @model function demo(x)\n           m ~ MvNormal(zero(x), I)\n           if dot(m, x) < 0\n               @addlogprob! -Inf\n               # Exit the model evaluation early\n               return\n           end\n           x ~ MvNormal(m, I)\n           return\n       end;\n\njulia> logjoint(demo([-2.1]), (m=[0.2],)) == -Inf\ntrue Note The  @addlogprob!  macro increases the accumulated log probability regardless of the evaluation context, i.e., regardless of whether you evaluate the log prior, the log likelihood or the log joint density. If you would like to avoid this behaviour you should check the evaluation context. It can be accessed with the internal variable  __context__ . For instance, in the following example the log density is not accumulated when only the log prior is computed:   julia> myloglikelihood(x, μ) = loglikelihood(Normal(μ, 1), x);\n\njulia> @model function demo(x)\n           μ ~ Normal()\n           if DynamicPPL.leafcontext(__context__) !== PriorContext()\n               @addlogprob! myloglikelihood(x, μ)\n           end\n       end;\n\njulia> x = [1.3, -2.1];\n\njulia> logprior(demo(x), (μ=0.2,)) ≈ logpdf(Normal(), 0.2)\ntrue\n\njulia> loglikelihood(demo(x), (μ=0.2,)) ≈ myloglikelihood(x, 0.2)\ntrue source Return values of the model function for a collection of samples can be obtained with  generated_quantities ."},{"id":48,"pagetitle":"API","title":"DynamicPPL.generated_quantities","ref":"/ppl/stable/api/#DynamicPPL.generated_quantities","content":" DynamicPPL.generated_quantities  —  Function generated_quantities(model::Model, chain::AbstractChains) Execute  model  for each of the samples in  chain  and return an array of the values returned by the  model  for each sample. Examples General Often you might have additional quantities computed inside the model that you want to inspect, e.g. @model function demo(x)\n    # sample and observe\n    θ ~ Prior()\n    x ~ Likelihood()\n    return interesting_quantity(θ, x)\nend\nm = demo(data)\nchain = sample(m, alg, n)\n# To inspect the `interesting_quantity(θ, x)` where `θ` is replaced by samples\n# from the posterior/`chain`:\ngenerated_quantities(m, chain) # <= results in a `Vector` of returned values\n                               #    from `interesting_quantity(θ, x)` Concrete (and simple) julia> using DynamicPPL, Turing\n\njulia> @model function demo(xs)\n           s ~ InverseGamma(2, 3)\n           m_shifted ~ Normal(10, √s)\n           m = m_shifted - 10\n\n           for i in eachindex(xs)\n               xs[i] ~ Normal(m, √s)\n           end\n\n           return (m, )\n       end\ndemo (generic function with 1 method)\n\njulia> model = demo(randn(10));\n\njulia> chain = sample(model, MH(), 10);\n\njulia> generated_quantities(model, chain)\n10×1 Array{Tuple{Float64},2}:\n (2.1964758025119338,)\n (2.1964758025119338,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.043088571494005024,)\n (-0.16489786710222099,)\n (-0.16489786710222099,) source generated_quantities(model::Model, parameters::NamedTuple)\ngenerated_quantities(model::Model, values, keys)\ngenerated_quantities(model::Model, values, keys) Execute  model  with variables  keys  set to  values  and return the values returned by the  model . If a  NamedTuple  is given,  keys=keys(parameters)  and  values=values(parameters) . Example julia> using DynamicPPL, Distributions\n\njulia> @model function demo(xs)\n           s ~ InverseGamma(2, 3)\n           m_shifted ~ Normal(10, √s)\n           m = m_shifted - 10\n           for i in eachindex(xs)\n               xs[i] ~ Normal(m, √s)\n           end\n           return (m, )\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo(randn(10));\n\njulia> parameters = (; s = 1.0, m_shifted=10);\n\njulia> generated_quantities(model, parameters)\n(0.0,)\n\njulia> generated_quantities(model, values(parameters), keys(parameters))\n(0.0,) source For a chain of samples, one can compute the pointwise log-likelihoods of each observed random variable with  pointwise_loglikelihoods ."},{"id":49,"pagetitle":"API","title":"DynamicPPL.pointwise_loglikelihoods","ref":"/ppl/stable/api/#DynamicPPL.pointwise_loglikelihoods","content":" DynamicPPL.pointwise_loglikelihoods  —  Function pointwise_loglikelihoods(model::Model, chain::Chains, keytype = String) Runs  model  on each sample in  chain  returning a  Dict{String, Matrix{Float64}}  with keys corresponding to symbols of the observations, and values being matrices of shape  (num_chains, num_samples) . keytype  specifies what the type of the keys used in the returned  Dict  are. Currently, only  String  and  VarName  are supported. Notes Say  y  is a  Vector  of  n  i.i.d.  Normal(μ, σ)  variables, with  μ  and  σ  both being  <:Real . Then the  observe  (i.e. when the left-hand side is an  observation ) statements can be implemented in three ways: using a  for  loop: for i in eachindex(y)\n    y[i] ~ Normal(μ, σ)\nend using  .~ : y .~ Normal(μ, σ) using  MvNormal : y ~ MvNormal(fill(μ, n), σ^2 * I) In (1) and (2),  y  will be treated as a collection of  n  i.i.d. 1-dimensional variables, while in (3)  y  will be treated as a  single  n-dimensional observation. This is important to keep in mind, in particular if the computation is used for downstream computations. Examples From chain julia> using DynamicPPL, Turing\n\njulia> @model function demo(xs, y)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, √s)\n           for i in eachindex(xs)\n               xs[i] ~ Normal(m, √s)\n           end\n\n           y ~ Normal(m, √s)\n       end\ndemo (generic function with 1 method)\n\njulia> model = demo(randn(3), randn());\n\njulia> chain = sample(model, MH(), 10);\n\njulia> pointwise_loglikelihoods(model, chain)\nDict{String,Array{Float64,2}} with 4 entries:\n  \"xs[3]\" => [-1.42862; -2.67573; … ; -1.66251; -1.66251]\n  \"xs[1]\" => [-1.42932; -2.68123; … ; -1.66333; -1.66333]\n  \"xs[2]\" => [-1.6724; -0.861339; … ; -1.62359; -1.62359]\n  \"y\"     => [-1.51265; -0.914129; … ; -1.5499; -1.5499]\n\njulia> pointwise_loglikelihoods(model, chain, String)\nDict{String,Array{Float64,2}} with 4 entries:\n  \"xs[3]\" => [-1.42862; -2.67573; … ; -1.66251; -1.66251]\n  \"xs[1]\" => [-1.42932; -2.68123; … ; -1.66333; -1.66333]\n  \"xs[2]\" => [-1.6724; -0.861339; … ; -1.62359; -1.62359]\n  \"y\"     => [-1.51265; -0.914129; … ; -1.5499; -1.5499]\n\njulia> pointwise_loglikelihoods(model, chain, VarName)\nDict{VarName,Array{Float64,2}} with 4 entries:\n  xs[2] => [-1.6724; -0.861339; … ; -1.62359; -1.62359]\n  y     => [-1.51265; -0.914129; … ; -1.5499; -1.5499]\n  xs[1] => [-1.42932; -2.68123; … ; -1.66333; -1.66333]\n  xs[3] => [-1.42862; -2.67573; … ; -1.66251; -1.66251] Broadcasting Note that  x .~ Dist()  will treat  x  as a collection of  independent  observations rather than as a single observation. julia> @model function demo(x)\n           x .~ Normal()\n       end;\n\njulia> m = demo([1.0, ]);\n\njulia> ℓ = pointwise_loglikelihoods(m, VarInfo(m)); first(ℓ[@varname(x[1])])\n-1.4189385332046727\n\njulia> m = demo([1.0; 1.0]);\n\njulia> ℓ = pointwise_loglikelihoods(m, VarInfo(m)); first.((ℓ[@varname(x[1])], ℓ[@varname(x[2])]))\n(-1.4189385332046727, -1.4189385332046727) source"},{"id":50,"pagetitle":"API","title":"DynamicPPL.NamedDist","ref":"/ppl/stable/api/#DynamicPPL.NamedDist","content":" DynamicPPL.NamedDist  —  Type A named distribution that carries the name of the random variable with it. source"},{"id":51,"pagetitle":"API","title":"Testing Utilities","ref":"/ppl/stable/api/#Testing-Utilities","content":" Testing Utilities DynamicPPL provides several demo models and helpers for testing samplers in the  DynamicPPL.TestUtils  submodule."},{"id":52,"pagetitle":"API","title":"DynamicPPL.TestUtils.test_sampler","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.test_sampler","content":" DynamicPPL.TestUtils.test_sampler  —  Function test_sampler(models, sampler, args...; kwargs...) Test that  sampler  produces correct marginal posterior means on each model in  models . In short, this method iterates through  models , calls  AbstractMCMC.sample  on the  model  and  sampler  to produce a  chain , and then checks  marginal_mean_of_samples(chain, vn)  for every (leaf) varname  vn  against the corresponding value returned by  posterior_mean  for each model. To change how comparison is done for a particular  chain  type, one can overload  marginal_mean_of_samples  for the corresponding type. Arguments models : A collection of instaces of  DynamicPPL.Model  to test on. sampler : The  AbstractMCMC.AbstractSampler  to test. args... : Arguments forwarded to  sample . Keyword arguments varnames_filter : A filter to apply to  varnames(model) , allowing comparison for only   a subset of the varnames. atol=1e-1 : Absolute tolerance used in  @test . rtol=1e-3 : Relative tolerance used in  @test . kwargs... : Keyword arguments forwarded to  sample . source"},{"id":53,"pagetitle":"API","title":"DynamicPPL.TestUtils.test_sampler_on_demo_models","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.test_sampler_on_demo_models","content":" DynamicPPL.TestUtils.test_sampler_on_demo_models  —  Function test_sampler_on_demo_models(meanfunction, sampler, args...; kwargs...) Test  sampler  on every model in  DEMO_MODELS . This is just a proxy for  test_sampler(meanfunction, DEMO_MODELS, sampler, args...; kwargs...) . source"},{"id":54,"pagetitle":"API","title":"DynamicPPL.TestUtils.test_sampler_continuous","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.test_sampler_continuous","content":" DynamicPPL.TestUtils.test_sampler_continuous  —  Function test_sampler_continuous(sampler, args...; kwargs...) Test that  sampler  produces the correct marginal posterior means on all models in  demo_models . As of right now, this is just an alias for  test_sampler_on_demo_models . source"},{"id":55,"pagetitle":"API","title":"DynamicPPL.TestUtils.marginal_mean_of_samples","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.marginal_mean_of_samples","content":" DynamicPPL.TestUtils.marginal_mean_of_samples  —  Function marginal_mean_of_samples(chain, varname) Return the mean of variable represented by  varname  in  chain . source"},{"id":56,"pagetitle":"API","title":"DynamicPPL.TestUtils.DEMO_MODELS","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.DEMO_MODELS","content":" DynamicPPL.TestUtils.DEMO_MODELS  —  Constant A collection of models corresponding to the posterior distribution defined by the generative process s ~ InverseGamma(2, 3)\nm ~ Normal(0, √s)\n1.5 ~ Normal(m, √s)\n2.0 ~ Normal(m, √s) or by s[1] ~ InverseGamma(2, 3)\ns[2] ~ InverseGamma(2, 3)\nm[1] ~ Normal(0, √s)\nm[2] ~ Normal(0, √s)\n1.5 ~ Normal(m[1], √s[1])\n2.0 ~ Normal(m[2], √s[2]) These are examples of a Normal-InverseGamma conjugate prior with Normal likelihood, for which the posterior is known in closed form. In particular, for the univariate model (the former one): mean(s) == 49 / 24\nmean(m) == 7 / 6 And for the multivariate one (the latter one): mean(s[1]) == 19 / 8\nmean(m[1]) == 3 / 4\nmean(s[2]) == 8 / 3\nmean(m[2]) == 1 source For every demo model, one can define the true log prior, log likelihood, and log joint probabilities."},{"id":57,"pagetitle":"API","title":"DynamicPPL.TestUtils.logprior_true","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.logprior_true","content":" DynamicPPL.TestUtils.logprior_true  —  Function logprior_true(model, args...) Return the  logprior  of  model  for  args . This should generally be implemented by hand for every specific  model . See also:  logjoint_true ,  loglikelihood_true . source"},{"id":58,"pagetitle":"API","title":"DynamicPPL.TestUtils.loglikelihood_true","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.loglikelihood_true","content":" DynamicPPL.TestUtils.loglikelihood_true  —  Function loglikelihood_true(model, args...) Return the  loglikelihood  of  model  for  args . This should generally be implemented by hand for every specific  model . See also:  logjoint_true ,  logprior_true . source"},{"id":59,"pagetitle":"API","title":"DynamicPPL.TestUtils.logjoint_true","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.logjoint_true","content":" DynamicPPL.TestUtils.logjoint_true  —  Function logjoint_true(model, args...) Return the  logjoint  of  model  for  args . Defaults to  logprior_true(model, args...) + loglikelihood_true(model, args..) . This should generally be implemented by hand for every specific  model  so that the returned value can be used as a ground-truth for testing things like: Validity of evaluation of  model  using a particular implementation of  AbstractVarInfo . Validity of a sampler when combined with DynamicPPL by running the sampler twice: once targeting ground-truth functions, e.g.  logjoint_true , and once targeting  model . And more. See also:  logprior_true ,  loglikelihood_true . source And in the case where the model includes constrained variables, it can also be useful to define"},{"id":60,"pagetitle":"API","title":"DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian","content":" DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian  —  Function logprior_true_with_logabsdet_jacobian(model::Model, args...) Return a tuple  (args_unconstrained, logprior_unconstrained)  of  model  for  args... . Unlike  logprior_true , the returned logprior computation includes the log-absdet-jacobian adjustment, thus computing logprior for the unconstrained variables. Note that  args  are assumed be in the support of  model , while  args_unconstrained  will be unconstrained. See also:  logprior_true . source"},{"id":61,"pagetitle":"API","title":"DynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian","content":" DynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian  —  Function logjoint_true_with_logabsdet_jacobian(model::Model, args...) Return a tuple  (args_unconstrained, logjoint)  of  model  for  args . Unlike  logjoint_true , the returned logjoint computation includes the log-absdet-jacobian adjustment, thus computing logjoint for the unconstrained variables. Note that  args  are assumed be in the support of  model , while  args_unconstrained  will be unconstrained. This should generally not be implemented directly, instead one should implement  logprior_true_with_logabsdet_jacobian  for a given  model . See also:  logjoint_true ,  logprior_true_with_logabsdet_jacobian . source Finally, the following methods can also be of use:"},{"id":62,"pagetitle":"API","title":"DynamicPPL.TestUtils.varnames","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.varnames","content":" DynamicPPL.TestUtils.varnames  —  Function varnames(model::Model) Return a collection of  VarName  as they are expected to appear in the model. Even though it is recommended to implement this by hand for a particular  Model , a default implementation using  SimpleVarInfo{<:Dict}  is provided. source"},{"id":63,"pagetitle":"API","title":"DynamicPPL.TestUtils.posterior_mean","ref":"/ppl/stable/api/#DynamicPPL.TestUtils.posterior_mean","content":" DynamicPPL.TestUtils.posterior_mean  —  Function posterior_mean(model::Model) Return a  NamedTuple  compatible with  varnames(model)  where the values represent the posterior mean under  model . \"Compatible\" means that a  varname  from  varnames(model)  can be used to extract the corresponding value using  get , e.g.  get(posterior_mean(model), varname) . source"},{"id":64,"pagetitle":"API","title":"Advanced","ref":"/ppl/stable/api/#Advanced","content":" Advanced"},{"id":65,"pagetitle":"API","title":"Variable names","ref":"/ppl/stable/api/#Variable-names","content":" Variable names Names and possibly nested indices of variables are described with  AbstractPPL.VarName . They can be defined with  AbstractPPL.@varname . Please see the documentation of  AbstractPPL.jl  for further information."},{"id":66,"pagetitle":"API","title":"Data Structures of Variables","ref":"/ppl/stable/api/#Data-Structures-of-Variables","content":" Data Structures of Variables DynamicPPL provides different data structures for samples from the model and their log density. All of them are subtypes of  AbstractVarInfo ."},{"id":67,"pagetitle":"API","title":"DynamicPPL.AbstractVarInfo","ref":"/ppl/stable/api/#DynamicPPL.AbstractVarInfo","content":" DynamicPPL.AbstractVarInfo  —  Type AbstractVarInfo Abstract supertype for data structures that capture random variables when executing a probabilistic model and accumulate log densities such as the log likelihood or the log joint probability of the model. See also:  VarInfo source"},{"id":68,"pagetitle":"API","title":"Common API","ref":"/ppl/stable/api/#Common-API","content":" Common API"},{"id":69,"pagetitle":"API","title":"DynamicPPL.getlogp","ref":"/ppl/stable/api/#DynamicPPL.getlogp","content":" DynamicPPL.getlogp  —  Function getlogp(vi::VarInfo) Return the log of the joint probability of the observed data and parameters sampled in  vi . source"},{"id":70,"pagetitle":"API","title":"DynamicPPL.setlogp!!","ref":"/ppl/stable/api/#DynamicPPL.setlogp!!","content":" DynamicPPL.setlogp!!  —  Function setlogp!!(vi::VarInfo, logp) Set the log of the joint probability of the observed data and parameters sampled in  vi  to  logp , mutating if it makes sense. source"},{"id":71,"pagetitle":"API","title":"DynamicPPL.acclogp!!","ref":"/ppl/stable/api/#DynamicPPL.acclogp!!","content":" DynamicPPL.acclogp!!  —  Function acclogp!!(vi::VarInfo, logp) Add  logp  to the value of the log of the joint probability of the observed data and parameters sampled in  vi , mutating if it makes sense. source"},{"id":72,"pagetitle":"API","title":"DynamicPPL.resetlogp!!","ref":"/ppl/stable/api/#DynamicPPL.resetlogp!!","content":" DynamicPPL.resetlogp!!  —  Function resetlogp!!(vi::AbstractVarInfo) Reset the value of the log of the joint probability of the observed data and parameters sampled in  vi  to 0, mutating if it makes sense. source"},{"id":73,"pagetitle":"API","title":"Base.getindex","ref":"/ppl/stable/api/#Base.getindex","content":" Base.getindex  —  Function getindex(vi::VarInfo, vn::VarName)\ngetindex(vi::VarInfo, vns::Vector{<:VarName}) Return the current value(s) of  vn  ( vns ) in  vi  in the support of its (their) distribution(s). If the value(s) is (are) transformed to the Euclidean space, it is (they are) transformed back. source getindex(vi::VarInfo, spl::Union{SampleFromPrior, Sampler}) Return the current value(s) of the random variables sampled by  spl  in  vi . The value(s) may or may not be transformed to Euclidean space. source"},{"id":74,"pagetitle":"API","title":"BangBang.push!!","ref":"/ppl/stable/api/#BangBang.push!!","content":" BangBang.push!!  —  Function push!!(vi::VarInfo, vn::VarName, r, dist::Distribution) Push a new random variable  vn  with a sampled value  r  from a distribution  dist  to the  VarInfo vi , mutating if it makes sense. source push!!(vi::VarInfo, vn::VarName, r, dist::Distribution, spl::AbstractSampler) Push a new random variable  vn  with a sampled value  r  sampled with a sampler  spl  from a distribution  dist  to  VarInfo vi , if it makes sense. The sampler is passed here to invalidate its cache where defined. source push!!(vi::VarInfo, vn::VarName, r, dist::Distribution, gid::Selector) Push a new random variable  vn  with a sampled value  r  sampled with a sampler of selector  gid  from a distribution  dist  to  VarInfo vi . source"},{"id":75,"pagetitle":"API","title":"BangBang.empty!!","ref":"/ppl/stable/api/#BangBang.empty!!","content":" BangBang.empty!!  —  Function empty!!(vi::VarInfo) Empty the fields of  vi.metadata  and reset  vi.logp[]  and  vi.num_produce[]  to zeros. This is useful when using a sampling algorithm that assumes an empty  vi , e.g.  SMC . source"},{"id":76,"pagetitle":"API","title":"DynamicPPL.values_as","ref":"/ppl/stable/api/#DynamicPPL.values_as","content":" DynamicPPL.values_as  —  Function values_as(varinfo[, Type]) Return the values/realizations in  varinfo  as  Type , if implemented. If no  Type  is provided, return values as stored in  varinfo . Examples SimpleVarInfo  with  NamedTuple : julia> data = (x = 1.0, m = [2.0]);\n\njulia> values_as(SimpleVarInfo(data))\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), NamedTuple)\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), OrderedDict)\nOrderedDict{VarName{sym, Setfield.IdentityLens} where sym, Any} with 2 entries:\n  x => 1.0\n  m => [2.0] SimpleVarInfo  with  OrderedDict : julia> data = OrderedDict{Any,Any}(@varname(x) => 1.0, @varname(m) => [2.0]);\n\njulia> values_as(SimpleVarInfo(data))\nOrderedDict{Any, Any} with 2 entries:\n  x => 1.0\n  m => [2.0]\n\njulia> values_as(SimpleVarInfo(data), NamedTuple)\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), OrderedDict)\nOrderedDict{Any, Any} with 2 entries:\n  x => 1.0\n  m => [2.0] TypedVarInfo : julia> # Just use an example model to construct the `VarInfo` because we're lazy.\n       vi = VarInfo(DynamicPPL.TestUtils.demo_assume_dot_observe());\n\njulia> vi[@varname(s)] = 1.0; vi[@varname(m)] = 2.0;\n\njulia> # For the sake of brevity, let's just check the type.\n       md = values_as(vi); md.s isa DynamicPPL.Metadata\ntrue\n\njulia> values_as(vi, NamedTuple)\n(s = 1.0, m = 2.0)\n\njulia> values_as(vi, OrderedDict)\nOrderedDict{VarName{sym, Setfield.IdentityLens} where sym, Float64} with 2 entries:\n  s => 1.0\n  m => 2.0 UntypedVarInfo : julia> # Just use an example model to construct the `VarInfo` because we're lazy.\n       vi = VarInfo(); DynamicPPL.TestUtils.demo_assume_dot_observe()(vi);\n\njulia> vi[@varname(s)] = 1.0; vi[@varname(m)] = 2.0;\n\njulia> # For the sake of brevity, let's just check the type.\n       values_as(vi) isa DynamicPPL.Metadata\ntrue\n\njulia> values_as(vi, NamedTuple)\n(s = 1.0, m = 2.0)\n\njulia> values_as(vi, OrderedDict)\nOrderedDict{VarName{sym, Setfield.IdentityLens} where sym, Float64} with 2 entries:\n  s => 1.0\n  m => 2.0 source"},{"id":77,"pagetitle":"API","title":"SimpleVarInfo","ref":"/ppl/stable/api/#SimpleVarInfo","content":" SimpleVarInfo"},{"id":78,"pagetitle":"API","title":"DynamicPPL.SimpleVarInfo","ref":"/ppl/stable/api/#DynamicPPL.SimpleVarInfo","content":" DynamicPPL.SimpleVarInfo  —  Type struct SimpleVarInfo{NT, T, C<:DynamicPPL.AbstractTransformation} <: AbstractVarInfo A simple wrapper of the parameters with a  logp  field for accumulation of the logdensity. Currently only implemented for  NT<:NamedTuple  and  NT<:AbstractDict . Fields values underlying representation of the realization represented logp holds the accumulated log-probability transformation represents whether it assumes variables to be transformed Notes The major differences between this and  TypedVarInfo  are: SimpleVarInfo  does not require linearization. SimpleVarInfo  can use more efficient bijectors. SimpleVarInfo  is only type-stable if  NT<:NamedTuple  and either a) no indexing is used in tilde-statements, or b) the values have been specified with the correct shapes. Examples General usage julia> using StableRNGs\n\njulia> @model function demo()\n           m ~ Normal()\n           x = Vector{Float64}(undef, 2)\n           for i in eachindex(x)\n               x[i] ~ Normal()\n           end\n           return x\n       end\ndemo (generic function with 2 methods)\n\njulia> m = demo();\n\njulia> rng = StableRNG(42);\n\njulia> ### Sampling ###\n       ctx = SamplingContext(rng, SampleFromPrior(), DefaultContext());\n\njulia> # In the `NamedTuple` version we need to provide the place-holder values for\n       # the variables which are using \"containers\", e.g. `Array`.\n       # In this case, this means that we need to specify `x` but not `m`.\n       _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo((x = ones(2), )), ctx);\n\njulia> # (✓) Vroom, vroom! FAST!!!\n       vi[@varname(x[1])]\n0.4471218424633827\n\njulia> # We can also access arbitrary varnames pointing to `x`, e.g.\n       vi[@varname(x)]\n2-element Vector{Float64}:\n 0.4471218424633827\n 1.3736306979834252\n\njulia> vi[@varname(x[1:2])]\n2-element Vector{Float64}:\n 0.4471218424633827\n 1.3736306979834252\n\njulia> # (×) If we don't provide the container...\n       _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo(), ctx); vi\nERROR: type NamedTuple has no field x\n[...]\n\njulia> # If one does not know the varnames, we can use a `OrderedDict` instead.\n       _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo{Float64}(OrderedDict()), ctx);\n\njulia> # (✓) Sort of fast, but only possible at runtime.\n       vi[@varname(x[1])]\n-1.019202452456547\n\njulia> # In addtion, we can only access varnames as they appear in the model!\n       vi[@varname(x)]\nERROR: KeyError: key x not found\n[...]\n\njulia> vi[@varname(x[1:2])]\nERROR: KeyError: key x[1:2] not found\n[...] Technically , it's possible to use any implementation of  AbstractDict  in place of  OrderedDict , but  OrderedDict  ensures that certain operations, e.g. linearization/flattening of the values in the varinfo, are consistent between evaluations. Hence  OrderedDict  is the preferred implementation of  AbstractDict  to use here. You can also sample in  transformed  space: julia> @model demo_constrained() = x ~ Exponential()\ndemo_constrained (generic function with 2 methods)\n\njulia> m = demo_constrained();\n\njulia> _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo(), ctx);\n\njulia> vi[@varname(x)] # (✓) 0 ≤ x < ∞\n1.8632965762164932\n\njulia> _, vi = DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(), true), ctx);\n\njulia> vi[@varname(x)] # (✓) -∞ < x < ∞\n-0.21080155351918753\n\njulia> xs = [last(DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(), true), ctx))[@varname(x)] for i = 1:10];\n\njulia> any(xs .< 0)  # (✓) Positive probability mass on negative numbers!\ntrue\n\njulia> # And with `OrderedDict` of course!\n       _, vi = DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(OrderedDict()), true), ctx);\n\njulia> vi[@varname(x)] # (✓) -∞ < x < ∞\n0.6225185067787314\n\njulia> xs = [last(DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(), true), ctx))[@varname(x)] for i = 1:10];\n\njulia> any(xs .< 0) # (✓) Positive probability mass on negative numbers!\ntrue Evaluation in transformed space of course also works: julia> vi = DynamicPPL.settrans!!(SimpleVarInfo((x = -1.0,)), true)\nTransformed SimpleVarInfo((x = -1.0,), 0.0)\n\njulia> # (✓) Positive probability mass on negative numbers!\n       getlogp(last(DynamicPPL.evaluate!!(m, vi, DynamicPPL.DefaultContext())))\n-1.3678794411714423\n\njulia> # While if we forget to make indicate that it's transformed:\n       vi = DynamicPPL.settrans!!(SimpleVarInfo((x = -1.0,)), false)\nSimpleVarInfo((x = -1.0,), 0.0)\n\njulia> # (✓) No probability mass on negative numbers!\n       getlogp(last(DynamicPPL.evaluate!!(m, vi, DynamicPPL.DefaultContext())))\n-Inf Indexing Using  NamedTuple  as underlying storage. julia> svi_nt = SimpleVarInfo((m = (a = [1.0], ), ));\n\njulia> svi_nt[@varname(m)]\n(a = [1.0],)\n\njulia> svi_nt[@varname(m.a)]\n1-element Vector{Float64}:\n 1.0\n\njulia> svi_nt[@varname(m.a[1])]\n1.0\n\njulia> svi_nt[@varname(m.a[2])]\nERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]\n[...]\n\njulia> svi_nt[@varname(m.b)]\nERROR: type NamedTuple has no field b\n[...] Using  OrderedDict  as underlying storage. julia> svi_dict = SimpleVarInfo(OrderedDict(@varname(m) => (a = [1.0], )));\n\njulia> svi_dict[@varname(m)]\n(a = [1.0],)\n\njulia> svi_dict[@varname(m.a)]\n1-element Vector{Float64}:\n 1.0\n\njulia> svi_dict[@varname(m.a[1])]\n1.0\n\njulia> svi_dict[@varname(m.a[2])]\nERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]\n[...]\n\njulia> svi_dict[@varname(m.b)]\nERROR: type NamedTuple has no field b\n[...] source"},{"id":79,"pagetitle":"API","title":"VarInfo","ref":"/ppl/stable/api/#VarInfo","content":" VarInfo Another data structure is  VarInfo ."},{"id":80,"pagetitle":"API","title":"DynamicPPL.VarInfo","ref":"/ppl/stable/api/#DynamicPPL.VarInfo","content":" DynamicPPL.VarInfo  —  Type struct VarInfo{Tmeta, Tlogp} <: AbstractVarInfo\n    metadata::Tmeta\n    logp::Base.RefValue{Tlogp}\n    num_produce::Base.RefValue{Int}\nend A light wrapper over one or more instances of  Metadata . Let  vi  be an instance of  VarInfo . If  vi isa VarInfo{<:Metadata} , then only one  Metadata  instance is used for all the sybmols.  VarInfo{<:Metadata}  is aliased  UntypedVarInfo . If  vi isa VarInfo{<:NamedTuple} , then  vi.metadata  is a  NamedTuple  that maps each symbol used on the LHS of  ~  in the model to its  Metadata  instance. The latter allows for the type specialization of  vi  after the first sampling iteration when all the symbols have been observed.  VarInfo{<:NamedTuple}  is aliased  TypedVarInfo . Note: It is the user's responsibility to ensure that each \"symbol\" is visited at least once whenever the model is called, regardless of any stochastic branching. Each symbol refers to a Julia variable and can be a hierarchical array of many random variables, e.g.  x[1] ~ ...  and  x[2] ~ ...  both have the same symbol  x . source"},{"id":81,"pagetitle":"API","title":"DynamicPPL.TypedVarInfo","ref":"/ppl/stable/api/#DynamicPPL.TypedVarInfo","content":" DynamicPPL.TypedVarInfo  —  Type TypedVarInfo(vi::UntypedVarInfo) This function finds all the unique  sym s from the instances of  VarName{sym}  found in  vi.metadata.vns . It then extracts the metadata associated with each symbol from the global  vi.metadata  field. Finally, a new  VarInfo  is created with a new  metadata  as a  NamedTuple  mapping from symbols to type-stable  Metadata  instances, one for each symbol. source One main characteristic of  VarInfo  is that samples are stored in a linearized form."},{"id":82,"pagetitle":"API","title":"DynamicPPL.tonamedtuple","ref":"/ppl/stable/api/#DynamicPPL.tonamedtuple","content":" DynamicPPL.tonamedtuple  —  Function tonamedtuple(vi::VarInfo) Convert a  vi  into a  NamedTuple  where each variable symbol maps to the values and  indexing string of the variable. For example, a model that had a vector of vector-valued variables  x  would return (x = ([1.5, 2.0], [3.0, 1.0], [\"x[1]\", \"x[2]\"]), ) source"},{"id":83,"pagetitle":"API","title":"DynamicPPL.link!","ref":"/ppl/stable/api/#DynamicPPL.link!","content":" DynamicPPL.link!  —  Function link!(vi::VarInfo, spl::Sampler) Transform the values of the random variables sampled by  spl  in  vi  from the support of their distributions to the Euclidean space and set their corresponding  \"trans\"  flag values to  true . source"},{"id":84,"pagetitle":"API","title":"DynamicPPL.invlink!","ref":"/ppl/stable/api/#DynamicPPL.invlink!","content":" DynamicPPL.invlink!  —  Function invlink!(vi::VarInfo, spl::AbstractSampler) Transform the values of the random variables sampled by  spl  in  vi  from the Euclidean space back to the support of their distributions and sets their corresponding  \"trans\"  flag values to  false . source"},{"id":85,"pagetitle":"API","title":"DynamicPPL.istrans","ref":"/ppl/stable/api/#DynamicPPL.istrans","content":" DynamicPPL.istrans  —  Function istrans(vi::AbstractVarInfo) Return  true  if  vi  is working in unconstrained space, and  false  if  vi  is assuming realizations to be in support of the corresponding distributions. source istrans(vi::VarInfo, vn::VarName) Return true if  vn 's values in  vi  are transformed to Euclidean space, and false if they are in the support of  vn 's distribution. source"},{"id":86,"pagetitle":"API","title":"DynamicPPL.set_flag!","ref":"/ppl/stable/api/#DynamicPPL.set_flag!","content":" DynamicPPL.set_flag!  —  Function set_flag!(vi::VarInfo, vn::VarName, flag::String) Set  vn 's value for  flag  to  true  in  vi . source"},{"id":87,"pagetitle":"API","title":"DynamicPPL.unset_flag!","ref":"/ppl/stable/api/#DynamicPPL.unset_flag!","content":" DynamicPPL.unset_flag!  —  Function unset_flag!(vi::VarInfo, vn::VarName, flag::String) Set  vn 's value for  flag  to  false  in  vi . source"},{"id":88,"pagetitle":"API","title":"DynamicPPL.is_flagged","ref":"/ppl/stable/api/#DynamicPPL.is_flagged","content":" DynamicPPL.is_flagged  —  Function is_flagged(vi::VarInfo, vn::VarName, flag::String) Check whether  vn  has a true value for  flag  in  vi . source For Gibbs sampling the following functions were added."},{"id":89,"pagetitle":"API","title":"DynamicPPL.setgid!","ref":"/ppl/stable/api/#DynamicPPL.setgid!","content":" DynamicPPL.setgid!  —  Function setgid!(vi::VarInfo, gid::Selector, vn::VarName) Add  gid  to the set of sampler selectors associated with  vn  in  vi . source"},{"id":90,"pagetitle":"API","title":"DynamicPPL.updategid!","ref":"/ppl/stable/api/#DynamicPPL.updategid!","content":" DynamicPPL.updategid!  —  Function updategid!(vi::VarInfo, vn::VarName, spl::Sampler) Set  vn 's  gid  to  Set([spl.selector]) , if  vn  does not have a sampler selector linked and  vn 's symbol is in the space of  spl . source The following functions were used for sequential Monte Carlo methods."},{"id":91,"pagetitle":"API","title":"DynamicPPL.get_num_produce","ref":"/ppl/stable/api/#DynamicPPL.get_num_produce","content":" DynamicPPL.get_num_produce  —  Function get_num_produce(vi::VarInfo) Return the  num_produce  of  vi . source"},{"id":92,"pagetitle":"API","title":"DynamicPPL.set_num_produce!","ref":"/ppl/stable/api/#DynamicPPL.set_num_produce!","content":" DynamicPPL.set_num_produce!  —  Function set_num_produce!(vi::VarInfo, n::Int) Set the  num_produce  field of  vi  to  n . source"},{"id":93,"pagetitle":"API","title":"DynamicPPL.increment_num_produce!","ref":"/ppl/stable/api/#DynamicPPL.increment_num_produce!","content":" DynamicPPL.increment_num_produce!  —  Function increment_num_produce!(vi::VarInfo) Add 1 to  num_produce  in  vi . source"},{"id":94,"pagetitle":"API","title":"DynamicPPL.reset_num_produce!","ref":"/ppl/stable/api/#DynamicPPL.reset_num_produce!","content":" DynamicPPL.reset_num_produce!  —  Function reset_num_produce!(vi::AbstractVarInfo) Reset the value of  num_produce  the log of the joint probability of the observed data and parameters sampled in  vi  to 0. source"},{"id":95,"pagetitle":"API","title":"DynamicPPL.setorder!","ref":"/ppl/stable/api/#DynamicPPL.setorder!","content":" DynamicPPL.setorder!  —  Function setorder!(vi::VarInfo, vn::VarName, index::Int) Set the  order  of  vn  in  vi  to  index , where  order  is the number of  observe statements run before sampling vn`. source"},{"id":96,"pagetitle":"API","title":"DynamicPPL.set_retained_vns_del_by_spl!","ref":"/ppl/stable/api/#DynamicPPL.set_retained_vns_del_by_spl!","content":" DynamicPPL.set_retained_vns_del_by_spl!  —  Function set_retained_vns_del_by_spl!(vi::VarInfo, spl::Sampler) Set the  \"del\"  flag of variables in  vi  with  order > vi.num_produce[]  to  true . source"},{"id":97,"pagetitle":"API","title":"Base.empty!","ref":"/ppl/stable/api/#Base.empty!","content":" Base.empty!  —  Function empty!(meta::Metadata) Empty the fields of  meta . This is useful when using a sampling algorithm that assumes an empty  meta , e.g.  SMC . source"},{"id":98,"pagetitle":"API","title":"Evaluation Contexts","ref":"/ppl/stable/api/#Evaluation-Contexts","content":" Evaluation Contexts Internally, both sampling and evaluation of log densities are performed with  AbstractPPL.evaluate!! ."},{"id":99,"pagetitle":"API","title":"AbstractPPL.evaluate!!","ref":"/ppl/stable/api/#AbstractPPL.evaluate!!","content":" AbstractPPL.evaluate!!  —  Function evaluate!!(model::Model[, rng, varinfo, sampler, context]) Sample from the  model  using the  sampler  with random number generator  rng  and the  context , and store the sample and log joint probability in  varinfo . Returns both the return-value of the original model, and the resulting varinfo. The method resets the log joint probability of  varinfo  and increases the evaluation number of  sampler . source The behaviour of a model execution can be changed with evaluation contexts that are passed as additional argument to the model function. Contexts are subtypes of  AbstractPPL.AbstractContext ."},{"id":100,"pagetitle":"API","title":"DynamicPPL.SamplingContext","ref":"/ppl/stable/api/#DynamicPPL.SamplingContext","content":" DynamicPPL.SamplingContext  —  Type SamplingContext(\n        [rng::Random.AbstractRNG=Random.GLOBAL_RNG],\n        [sampler::AbstractSampler=SampleFromPrior()],\n        [context::AbstractContext=DefaultContext()],\n) Create a context that allows you to sample parameters with the  sampler  when running the model. The  context  determines how the returned log density is computed when running the model. See also:  DefaultContext ,  LikelihoodContext ,  PriorContext source"},{"id":101,"pagetitle":"API","title":"DynamicPPL.DefaultContext","ref":"/ppl/stable/api/#DynamicPPL.DefaultContext","content":" DynamicPPL.DefaultContext  —  Type struct DefaultContext <: AbstractContext end The  DefaultContext  is used by default to compute log the joint probability of the data  and parameters when running the model. source"},{"id":102,"pagetitle":"API","title":"DynamicPPL.LikelihoodContext","ref":"/ppl/stable/api/#DynamicPPL.LikelihoodContext","content":" DynamicPPL.LikelihoodContext  —  Type struct LikelihoodContext{Tvars} <: AbstractContext\n    vars::Tvars\nend The  LikelihoodContext  enables the computation of the log likelihood of the parameters when  running the model.  vars  can be used to evaluate the log likelihood for specific values  of the model's parameters. If  vars  is  nothing , the parameter values inside the  VarInfo  will be used by default. source"},{"id":103,"pagetitle":"API","title":"DynamicPPL.PriorContext","ref":"/ppl/stable/api/#DynamicPPL.PriorContext","content":" DynamicPPL.PriorContext  —  Type struct PriorContext{Tvars} <: AbstractContext\n    vars::Tvars\nend The  PriorContext  enables the computation of the log prior of the parameters  vars  when  running the model. source"},{"id":104,"pagetitle":"API","title":"DynamicPPL.MiniBatchContext","ref":"/ppl/stable/api/#DynamicPPL.MiniBatchContext","content":" DynamicPPL.MiniBatchContext  —  Type struct MiniBatchContext{Tctx, T} <: AbstractContext\n    context::Tctx\n    loglike_scalar::T\nend The  MiniBatchContext  enables the computation of   log(prior) + s * log(likelihood of a batch)  when running the model, where  s  is the   loglike_scalar  field, typically equal to  the number of data points / batch size .  This is useful in batch-based stochastic gradient descent algorithms to be optimizing   log(prior) + log(likelihood of all the data points)  in the expectation. source"},{"id":105,"pagetitle":"API","title":"DynamicPPL.PrefixContext","ref":"/ppl/stable/api/#DynamicPPL.PrefixContext","content":" DynamicPPL.PrefixContext  —  Type PrefixContext{Prefix}(context) Create a context that allows you to use the wrapped  context  when running the model and adds the  Prefix  to all parameters. This context is useful in nested models to ensure that the names of the parameters are unique. See also:  @submodel source"},{"id":106,"pagetitle":"API","title":"Samplers","ref":"/ppl/stable/api/#Samplers","content":" Samplers In DynamicPPL two samplers are defined that are used to initialize unobserved random variables:  SampleFromPrior  which samples from the prior distribution, and  SampleFromUniform  which samples from a uniform distribution."},{"id":107,"pagetitle":"API","title":"DynamicPPL.SampleFromPrior","ref":"/ppl/stable/api/#DynamicPPL.SampleFromPrior","content":" DynamicPPL.SampleFromPrior  —  Type SampleFromPrior Sampling algorithm that samples unobserved random variables from their prior distribution. source"},{"id":108,"pagetitle":"API","title":"DynamicPPL.SampleFromUniform","ref":"/ppl/stable/api/#DynamicPPL.SampleFromUniform","content":" DynamicPPL.SampleFromUniform  —  Type SampleFromUniform Sampling algorithm that samples unobserved random variables from a uniform distribution. References Stan reference manual source Additionally, a generic sampler for inference is implemented."},{"id":109,"pagetitle":"API","title":"DynamicPPL.Sampler","ref":"/ppl/stable/api/#DynamicPPL.Sampler","content":" DynamicPPL.Sampler  —  Type Sampler{T} Generic sampler type for inference algorithms of type  T  in DynamicPPL. Sampler  should implement the AbstractMCMC interface, and in particular  AbstractMCMC.step . A default implementation of the initial sampling step is provided that supports resuming sampling from a previous state and setting initial parameter values. It requires to overload  loadstate  and  initialstep  for loading previous states and actually performing the initial sampling step, respectively. Additionally, sometimes one might want to implement  initialsampler  that specifies how the initial parameter values are sampled if they are not provided. By default, values are sampled from the prior. source The default implementation of  Sampler  uses the following unexported functions."},{"id":110,"pagetitle":"API","title":"DynamicPPL.initialstep","ref":"/ppl/stable/api/#DynamicPPL.initialstep","content":" DynamicPPL.initialstep  —  Function initialstep(rng, model, sampler, varinfo; kwargs...) Perform the initial sampling step of the  sampler  for the  model . The  varinfo  contains the initial samples, which can be provided by the user or sampled randomly. source"},{"id":111,"pagetitle":"API","title":"DynamicPPL.loadstate","ref":"/ppl/stable/api/#DynamicPPL.loadstate","content":" DynamicPPL.loadstate  —  Function loadstate(data) Load sampler state from  data . source"},{"id":112,"pagetitle":"API","title":"DynamicPPL.initialsampler","ref":"/ppl/stable/api/#DynamicPPL.initialsampler","content":" DynamicPPL.initialsampler  —  Function initialsampler(sampler::Sampler) Return the sampler that is used for generating the initial parameters when sampling with  sampler . By default, it returns an instance of  SampleFromPrior . source"},{"id":113,"pagetitle":"API","title":"Model-Internal Functions","ref":"/ppl/stable/api/#model_internal","content":" Model-Internal Functions"},{"id":114,"pagetitle":"API","title":"DynamicPPL.tilde_assume","ref":"/ppl/stable/api/#DynamicPPL.tilde_assume","content":" DynamicPPL.tilde_assume  —  Function tilde_assume(context::SamplingContext, right, vn, vi) Handle assumed variables, e.g.,  x ~ Normal()  (where  x  does occur in the model inputs), accumulate the log probability, and return the sampled value with a context associated with a sampler. Falls back to tilde_assume(context.rng, context.context, context.sampler, right, vn, vi) source"},{"id":115,"pagetitle":"API","title":"DynamicPPL.dot_tilde_assume","ref":"/ppl/stable/api/#DynamicPPL.dot_tilde_assume","content":" DynamicPPL.dot_tilde_assume  —  Function dot_tilde_assume(context::SamplingContext, right, left, vn, vi) Handle broadcasted assumed variables, e.g.,  x .~ MvNormal()  (where  x  does not occur in the model inputs), accumulate the log probability, and return the sampled value for a context associated with a sampler. Falls back to dot_tilde_assume(context.rng, context.context, context.sampler, right, left, vn, vi) source"},{"id":116,"pagetitle":"API","title":"DynamicPPL.tilde_observe","ref":"/ppl/stable/api/#DynamicPPL.tilde_observe","content":" DynamicPPL.tilde_observe  —  Function tilde_observe(context::SamplingContext, right, left, vi) Handle observed constants with a  context  associated with a sampler. Falls back to  tilde_observe(context.context, context.sampler, right, left, vi) . source"},{"id":117,"pagetitle":"API","title":"DynamicPPL.dot_tilde_observe","ref":"/ppl/stable/api/#DynamicPPL.dot_tilde_observe","content":" DynamicPPL.dot_tilde_observe  —  Function dot_tilde_observe(context::SamplingContext, right, left, vi) Handle broadcasted observed constants, e.g.,  [1.0] .~ MvNormal() , accumulate the log probability, and return the observed value for a context associated with a sampler. Falls back to  dot_tilde_observe(context.context, context.sampler, right, left, vi) . source"},{"id":121,"pagetitle":"AdvancedHMC.jl","title":"AdvancedHMC.jl","ref":"/hmc/stable/#AdvancedHMC.jl","content":" AdvancedHMC.jl AdvancedHMC.jl provides a robust, modular and efficient implementation of advanced HMC algorithms. An illustrative example for AdvancedHMC's usage is given below. AdvancedHMC.jl is part of  Turing.jl , a probabilistic programming library in Julia.  If you are interested in using AdvancedHMC.jl through a probabilistic programming language, please check it out! Interfaces IMP.hmc : an experimental Python module for the Integrative Modeling Platform, which uses AdvancedHMC in its backend to sample protein structures. NEWS We presented a paper for AdvancedHMC.jl at  AABI  2019 in Vancouver, Canada. ( abs ,  pdf ,  OpenReview ) We presented a poster for AdvancedHMC.jl at  StanCon 2019  in Cambridge, UK. ( pdf ) API CHANGES [v0.2.22] Three functions are renamed. Preconditioner(metric::AbstractMetric)  ->  MassMatrixAdaptor(metric)  and  NesterovDualAveraging(δ, integrator::AbstractIntegrator)  ->  StepSizeAdaptor(δ, integrator) find_good_eps  ->  find_good_stepsize [v0.2.15]  n_adapts  is no longer needed to construct  StanHMCAdaptor ; the old constructor is deprecated. [v0.2.8] Two Hamiltonian trajectory sampling methods are renamed to avoid a name clash with Distributions. Multinomial  ->  MultinomialTS Slice  ->  SliceTS [v0.2.0] The gradient function passed to  Hamiltonian  is supposed to return a value-gradient tuple now."},{"id":122,"pagetitle":"AdvancedHMC.jl","title":"A minimal example - sampling from a multivariate Gaussian using NUTS","ref":"/hmc/stable/#A-minimal-example-sampling-from-a-multivariate-Gaussian-using-NUTS","content":" A minimal example - sampling from a multivariate Gaussian using NUTS using AdvancedHMC, Distributions, ForwardDiff\nusing LinearAlgebra\n\n# Choose parameter dimensionality and initial parameter value\nD = 10; initial_θ = rand(D)\n\n# Define the target distribution\nℓπ(θ) = logpdf(MvNormal(zeros(D), I), θ)\n\n# Set the number of samples to draw and warmup iterations\nn_samples, n_adapts = 2_000, 1_000\n\n# Define a Hamiltonian system\nmetric = DiagEuclideanMetric(D)\nhamiltonian = Hamiltonian(metric, ℓπ, ForwardDiff)\n\n# Define a leapfrog solver, with initial step size chosen heuristically\ninitial_ϵ = find_good_stepsize(hamiltonian, initial_θ)\nintegrator = Leapfrog(initial_ϵ)\n\n# Define an HMC sampler, with the following components\n#   - multinomial sampling scheme,\n#   - generalised No-U-Turn criteria, and\n#   - windowed adaption for step-size and diagonal mass matrix\nproposal = NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)\nadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))\n\n# Run the sampler to draw samples from the specified Gaussian, where\n#   - `samples` will store the samples\n#   - `stats` will store diagnostic statistics for each sample\nsamples, stats = sample(hamiltonian, proposal, initial_θ, n_samples, adaptor, n_adapts; progress=true)"},{"id":123,"pagetitle":"AdvancedHMC.jl","title":"Parallel sampling","ref":"/hmc/stable/#Parallel-sampling","content":" Parallel sampling AdvancedHMC enables parallel sampling (either distributed or multi-thread) via Julia's  parallel computing functions . It also supports vectorized sampling for static HMC and has been discussed in more detail in the documentation  here . The below example utilizes the  @threads  macro to sample 4 chains across 4 threads. # Ensure that julia was launched with appropriate number of threads\nprintln(Threads.nthreads())\n\n# Number of chains to sample\nnchains = 4\n\n# Cache to store the chains\nchains = Vector{Any}(undef, nchains)\n\n# The `samples` from each parallel chain is stored in the `chains` vector \n# Adjust the `verbose` flag as per need\nThreads.@threads for i in 1:nchains\n  samples, stats = sample(hamiltonian, proposal, initial_θ, n_samples, adaptor, n_adapts; verbose=false)\n  chains[i] = samples\nend"},{"id":124,"pagetitle":"AdvancedHMC.jl","title":"GPU Sampling with CUDA","ref":"/hmc/stable/#GPU-Sampling-with-CUDA","content":" GPU Sampling with CUDA There is experimental support for running static HMC on the GPU using CUDA.  To do so the user needs to have  CUDA.jl  installed, ensure the logdensity of the  Hamiltonian  can be executed on the GPU and that the initial points are a  CuArray .  A small working example can be found at  test/cuda.jl ."},{"id":125,"pagetitle":"AdvancedHMC.jl","title":"API and supported HMC algorithms","ref":"/hmc/stable/#API-and-supported-HMC-algorithms","content":" API and supported HMC algorithms An important design goal of AdvancedHMC.jl is modularity; we would like to support algorithmic research on HMC. This modularity means that different HMC variants can be easily constructed by composing various components, such as preconditioning metric (i.e. mass matrix), leapfrog integrators,  trajectories (static or dynamic), and adaption schemes etc.  The minimal example above can be modified to suit particular inference problems by picking components from the list below."},{"id":126,"pagetitle":"AdvancedHMC.jl","title":"Hamiltonian mass matrix ( metric )","ref":"/hmc/stable/#Hamiltonian-mass-matrix-(metric)","content":" Hamiltonian mass matrix ( metric ) Unit metric:  UnitEuclideanMetric(dim) Diagonal metric:  DiagEuclideanMetric(dim) Dense metric:  DenseEuclideanMetric(dim) where  dim  is the dimensionality of the sampling space."},{"id":127,"pagetitle":"AdvancedHMC.jl","title":"Integrator ( integrator )","ref":"/hmc/stable/#Integrator-(integrator)","content":" Integrator ( integrator ) Ordinary leapfrog integrator:  Leapfrog(ϵ) Jittered leapfrog integrator with jitter rate  n :  JitteredLeapfrog(ϵ, n) Tempered leapfrog integrator with tempering rate  a :  TemperedLeapfrog(ϵ, a) where  ϵ  is the step size of leapfrog integration."},{"id":128,"pagetitle":"AdvancedHMC.jl","title":"Proposal ( proposal )","ref":"/hmc/stable/#Proposal-(proposal)","content":" Proposal ( proposal ) Static HMC with a fixed number of steps ( n_steps ) (Neal, R. M. (2011)):  StaticTrajectory(integrator, n_steps) HMC with a fixed total trajectory length ( trajectory_length ) (Neal, R. M. (2011)):  HMCDA(integrator, trajectory_length) Original NUTS with slice sampling (Hoffman, M. D., & Gelman, A. (2014)):  NUTS{SliceTS,ClassicNoUTurn}(integrator) Generalised NUTS with slice sampling (Betancourt, M. (2017)):  NUTS{SliceTS,GeneralisedNoUTurn}(integrator) Original NUTS with multinomial sampling (Betancourt, M. (2017)):  NUTS{MultinomialTS,ClassicNoUTurn}(integrator) Generalised NUTS with multinomial sampling (Betancourt, M. (2017)):  NUTS{MultinomialTS,GeneralisedNoUTurn}(integrator)"},{"id":129,"pagetitle":"AdvancedHMC.jl","title":"Adaptor ( adaptor )","ref":"/hmc/stable/#Adaptor-(adaptor)","content":" Adaptor ( adaptor ) Adapt the mass matrix  metric  of the Hamiltonian dynamics:  mma = MassMatrixAdaptor(metric) This is lowered to  UnitMassMatrix ,  WelfordVar  or  WelfordCov  based on the type of the mass matrix  metric Adapt the step size of the leapfrog integrator  integrator :  ssa = StepSizeAdaptor(δ, integrator) It uses Nesterov's dual averaging with  δ  as the target acceptance rate. Combine the two above  naively :  NaiveHMCAdaptor(mma, ssa) Combine the first two using Stan's windowed adaptation:  StanHMCAdaptor(mma, ssa)"},{"id":130,"pagetitle":"AdvancedHMC.jl","title":"Gradients","ref":"/hmc/stable/#Gradients","content":" Gradients AdvancedHMC  supports both AD-based ( Zygote ,  Tracker  and  ForwardDiff ) and user-specified gradients. In order to use user-specified gradients, please replace  ForwardDiff  with  ℓπ_grad  in the  Hamiltonian   constructor, where the gradient function  ℓπ_grad  should return a tuple containing both the log-posterior and its gradient.  All the combinations are tested in  this file  except from using tempered leapfrog integrator together with adaptation, which we found unstable empirically."},{"id":131,"pagetitle":"AdvancedHMC.jl","title":"The  sample  function signature in detail","ref":"/hmc/stable/#The-sample-function-signature-in-detail","content":" The  sample  function signature in detail function sample(\n    rng::Union{AbstractRNG, AbstractVector{<:AbstractRNG}},\n    h::Hamiltonian,\n    κ::HMCKernel,\n    θ::AbstractVector{<:AbstractFloat},\n    n_samples::Int,\n    adaptor::AbstractAdaptor=NoAdaptation(),\n    n_adapts::Int=min(div(n_samples, 10), 1_000);\n    drop_warmup=false,\n    verbose::Bool=true,\n    progress::Bool=false,\n) Draw  n_samples  samples using the proposal  κ  under the Hamiltonian system  h The randomness is controlled by  rng . If  rng  is not provided,  GLOBAL_RNG  will be used. The initial point is given by  θ . The adaptor is set by  adaptor , for which the default is no adaptation. It will perform  n_adapts  steps of adaptation, for which the default is  1_000  or 10% of  n_samples , whichever is lower.  drop_warmup  specifies whether to drop samples. verbose  controls the verbosity. progress  controls whether to show the progress meter or not. Note that the function signature of the  sample  function exported by  AdvancedHMC.jl  differs from the  sample  function used by  Turing.jl . We refer to the documentation of  Turing.jl  for more details on the latter."},{"id":132,"pagetitle":"AdvancedHMC.jl","title":"Citing AdvancedHMC.jl","ref":"/hmc/stable/#Citing-AdvancedHMC.jl","content":" Citing AdvancedHMC.jl If you use AdvancedHMC.jl for your own research, please consider citing the following publication: Kai Xu, Hong Ge, Will Tebbutt, Mohamed Tarek, Martin Trapp, Zoubin Ghahramani: \"AdvancedHMC.jl: A robust, modular and efficient implementation of advanced HMC algorithms.\",  Symposium on Advances in Approximate Bayesian Inference , 2020. ( abs ,  pdf ) with the following BibTeX entry: @inproceedings{xu2020advancedhmc,\n  title={AdvancedHMC. jl: A robust, modular and efficient implementation of advanced HMC algorithms},\n  author={Xu, Kai and Ge, Hong and Tebbutt, Will and Tarek, Mohamed and Trapp, Martin and Ghahramani, Zoubin},\n  booktitle={Symposium on Advances in Approximate Bayesian Inference},\n  pages={1--10},\n  year={2020},\n  organization={PMLR}\n} If you using AdvancedHMC.jl directly through Turing.jl, please consider citing the following publication: Hong Ge, Kai Xu, and Zoubin Ghahramani: \"Turing: a language for flexible probabilistic inference.\",  International Conference on Artificial Intelligence and Statistics , 2018. ( abs ,  pdf ) with the following BibTeX entry: @inproceedings{ge2018turing,\n  title={Turing: A language for flexible probabilistic inference},\n  author={Ge, Hong and Xu, Kai and Ghahramani, Zoubin},\n  booktitle={International Conference on Artificial Intelligence and Statistics},\n  pages={1682--1690},\n  year={2018},\n  organization={PMLR}\n}"},{"id":133,"pagetitle":"AdvancedHMC.jl","title":"References","ref":"/hmc/stable/#References","content":" References Neal, R. M. (2011). MCMC using Hamiltonian dynamics. Handbook of Markov chain Monte Carlo, 2(11), 2. ( arXiv ) Betancourt, M. (2017). A Conceptual Introduction to Hamiltonian Monte Carlo.  arXiv preprint arXiv:1701.02434 . Girolami, M., & Calderhead, B. (2011). Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(2), 123-214. ( arXiv ) Betancourt, M. J., Byrne, S., & Girolami, M. (2014). Optimizing the integrator step size for Hamiltonian Monte Carlo.  arXiv preprint arXiv:1411.6669 . Betancourt, M. (2016). Identifying the optimal integration time in Hamiltonian Monte Carlo.  arXiv preprint arXiv:1601.00225 . Hoffman, M. D., & Gelman, A. (2014). The No-U-Turn Sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1), 1593-1623. ( arXiv )"},{"id":136,"pagetitle":"AdvancedHMC.jl","title":"AdvancedHMC.jl","ref":"/hmc/stable/api/#AdvancedHMC.jl","content":" AdvancedHMC.jl Documentation for AdvancedHMC.jl AdvancedHMC.jl Structs Functions Index AdvancedHMC.jl A minimal example - sampling from a multivariate Gaussian using NUTS API and supported HMC algorithms The  sample  function signature in detail Citing AdvancedHMC.jl References"},{"id":137,"pagetitle":"AdvancedHMC.jl","title":"Structs","ref":"/hmc/stable/api/#Structs","content":" Structs"},{"id":138,"pagetitle":"AdvancedHMC.jl","title":"AdvancedHMC.ClassicNoUTurn","ref":"/hmc/stable/api/#AdvancedHMC.ClassicNoUTurn","content":" AdvancedHMC.ClassicNoUTurn  —  Type struct ClassicNoUTurn{F<:AbstractFloat} <: AdvancedHMC.DynamicTerminationCriterion Classic No-U-Turn criterion as described in Eq. (9) in [1]. Informally, this will terminate the trajectory expansion if continuing the simulation either forwards or backwards in time will decrease the distance between the left-most and right-most positions. Fields max_depth::Int64 Δ_max::AbstractFloat References Hoffman, M. D., & Gelman, A. (2014). The No-U-Turn Sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1), 1593-1623. ( arXiv ) source"},{"id":139,"pagetitle":"AdvancedHMC.jl","title":"Functions","ref":"/hmc/stable/api/#Functions","content":" Functions"},{"id":140,"pagetitle":"AdvancedHMC.jl","title":"StatsBase.sample","ref":"/hmc/stable/api/#StatsBase.sample","content":" StatsBase.sample  —  Function sample(\n    rng::AbstractRNG,\n    h::Hamiltonian,\n    κ::AbstractMCMCKernel,\n    θ::AbstractVecOrMat{T},\n    n_samples::Int,\n    adaptor::AbstractAdaptor=NoAdaptation(),\n    n_adapts::Int=min(div(n_samples, 10), 1_000);\n    drop_warmup::Bool=false,\n    verbose::Bool=true,\n    progress::Bool=false\n) Sample  n_samples  samples using the proposal  κ  under Hamiltonian  h . The randomness is controlled by  rng .  If  rng  is not provided,  GLOBAL_RNG  will be used. The initial point is given by  θ . The adaptor is set by  adaptor , for which the default is no adaptation. It will perform  n_adapts  steps of adaptation, for which the default is the minimum of  1_000  and 10% of  n_samples drop_warmup  controls to drop the samples during adaptation phase or not verbose  controls the verbosity progress  controls whether to show the progress meter or not source sample(model::DifferentiableDensityModel, kernel::AdvancedHMC.AbstractMCMCKernel, metric::AdvancedHMC.AbstractMetric, adaptor::AdvancedHMC.Adaptation.AbstractAdaptor, N::Integer; kwargs...) -> Any\n A convenient wrapper around  AbstractMCMC.sample  avoiding explicit construction of  HMCSampler . source"},{"id":141,"pagetitle":"AdvancedHMC.jl","title":"Index","ref":"/hmc/stable/api/#Index","content":" Index AdvancedHMC.ClassicNoUTurn StatsBase.sample"},{"id":145,"pagetitle":"Home","title":"NestedSamplers.jl","ref":"/ns/stable/#NestedSamplers.jl","content":" NestedSamplers.jl A Julian implementation of single- and multi-ellipsoidal nested sampling algorithms using the  AbstractMCMC  interface. This package was heavily influenced by  nestle ,  dynesty , and  NestedSampling.jl ."},{"id":146,"pagetitle":"Home","title":"Installation","ref":"/ns/stable/#Installation","content":" Installation To use the nested samplers first install this library julia> ]add NestedSamplers"},{"id":147,"pagetitle":"Home","title":"Usage","ref":"/ns/stable/#Usage","content":" Usage The samplers are built using the  AbstractMCMC  interface. To use it, we need to create a  NestedModel . using Random\nusing AbstractMCMC\nAbstractMCMC.setprogress!(false)\nRandom.seed!(8452); [ Info: progress logging is disabled globally using Distributions\nusing LinearAlgebra\nusing NestedSamplers\nusing StatsFuns: logaddexp\n\n# Gaussian mixture model\nσ = 0.1\nμ1 = ones(2)\nμ2 = -ones(2)\ninv_σ = diagm(0 => fill(1 / σ^2, 2))\n\nfunction logl(x)\n    dx1 = x .- μ1\n    dx2 = x .- μ2\n    f1 = -dx1' * (inv_σ * dx1) / 2\n    f2 = -dx2' * (inv_σ * dx2) / 2\n    return logaddexp(f1, f2)\nend\npriors = [\n    Uniform(-5, 5),\n    Uniform(-5, 5)\n]\n# or equivalently\nprior_transform(X) = 10 .* X .- 5\n# create the model\n# or model = NestedModel(logl, prior_transform)\nmodel = NestedModel(logl, priors); now, we set up our sampling using  StatsBase . Important:  the state of the sampler is returned in addition to the chain by  sample . using StatsBase: sample, Weights\n\n# create our sampler\n# 2 parameters, 1000 active points, multi-ellipsoid. See docstring\nspl = Nested(2, 1000)\n# by default, uses dlogz for convergence. Set the keyword args here\n# currently Chains and Array are support chain_types\nchain, state = sample(model, spl; dlogz=0.2, param_names=[\"x\", \"y\"])\n# optionally resample the chain using the weights\nchain_res = sample(chain, Weights(vec(chain[\"weights\"])), length(chain)); Chains MCMC chain (9360×3×1 Array{Float64, 3}):\n\nIterations        = 1:9360\nNumber of chains  = 1\nSamples per chain = 9360\nparameters        = y, x\ninternals         = weights\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat\n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64\n\n           x    0.0946    1.0043     0.0104    0.0088   8657.6848    1.0002\n           y    0.0935    1.0010     0.0103    0.0089   8680.0644    1.0002\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n           x   -1.1679   -0.9890    0.8681    1.0145    1.1781\n           y   -1.1534   -0.9872    0.8573    1.0149    1.1724\n let's take a look at the resampled posteriors using StatsPlots\ndensity(chain_res)\n# analytical posterior maxima\nvline!([-1, 1], c=:black, ls=:dash, subplot=1)\nvline!([-1, 1], c=:black, ls=:dash, subplot=2) and compare our estimate of the Bayesian (log-)evidence to the analytical value analytic_logz = log(4π * σ^2 / 100)\n# within 2-sigma\n@assert isapprox(analytic_logz, state.logz, atol=2state.logzerr)"},{"id":148,"pagetitle":"Home","title":"Contributing","ref":"/ns/stable/#Contributing","content":" Contributing Primary Author:  Miles Lucas ( @mileslucas ) Contributions are always welcome! Take a look at the  issues  for ideas of open problems! To discuss ideas or plan contributions, open a  discussion ."},{"id":151,"pagetitle":"API/Reference","title":"API/Reference","ref":"/ns/stable/api/#API/Reference","content":" API/Reference NestedSamplers.Bounds NestedSamplers.Models NestedSamplers.Proposals NestedSamplers.Bounds.Ellipsoid NestedSamplers.Bounds.MultiEllipsoid NestedSamplers.Bounds.NoBounds NestedSamplers.Nested NestedSamplers.NestedModel NestedSamplers.Proposals.RSlice NestedSamplers.Proposals.RStagger NestedSamplers.Proposals.RWalk NestedSamplers.Proposals.Slice NestedSamplers.Proposals.Uniform NestedSamplers.Models.CorrelatedGaussian NestedSamplers.Models.GaussianShells"},{"id":152,"pagetitle":"API/Reference","title":"Samplers","ref":"/ns/stable/api/#Samplers","content":" Samplers"},{"id":153,"pagetitle":"API/Reference","title":"NestedSamplers.NestedModel","ref":"/ns/stable/api/#NestedSamplers.NestedModel","content":" NestedSamplers.NestedModel  —  Type NestedModel(loglike, prior_transform)\nNestedModel(loglike, priors::AbstractVector{<:Distribution}) loglike  must be callable with a signature  loglike(::AbstractVector)  where the length of the vector must match the number of parameters in your model. prior_transform  must be a callable with a signature  prior_transform(::AbstractVector)  that returns the transformation from the unit-cube to parameter space. This is effectively the quantile or ppf of a statistical distribution. For convenience, if a vector of  Distribution  is provided (as a set of priors), a transformation function will automatically be constructed using  Distributions.quantile . Note: loglike  is the only function used for likelihood calculations. This means if you want your priors to be used for the likelihood calculations they must be manually included in the  loglike  function. source"},{"id":154,"pagetitle":"API/Reference","title":"NestedSamplers.Nested","ref":"/ns/stable/api/#NestedSamplers.Nested","content":" NestedSamplers.Nested  —  Type Nested(ndims, nactive;\n    bounds=Bounds.MultiEllipsoid,\n    proposal=:auto,\n    enlarge=1.25,\n    update_interval=default_update_interval(proposal, ndims),\n    min_ncall=2nactive,\n    min_eff=0.10) Static nested sampler with  nactive  active points and  ndims  parameters. ndims  is equivalent to the number of parameters to fit, which defines the dimensionality of the prior volume used in evidence sampling.  nactive  is the number of live or active points in the prior volume. This is a static sampler, so the number of live points will be constant for all of the sampling. Bounds and Proposals bounds  declares the Type of  Bounds.AbstractBoundingSpace  to use in the prior volume. The available bounds are described by  Bounds .  proposal  declares the algorithm used for proposing new points. The available proposals are described in  Proposals . If  proposal  is  :auto , will choose the proposal based on  ndims ndims < 10  -  Proposals.Uniform 10 ≤ ndims ≤ 20  -  Proposals.RWalk ndims > 20  -  Proposals.Slice The original nested sampling algorithm is roughly equivalent to using  Bounds.Ellipsoid  with  Proposals.Uniform . The MultiNest algorithm is roughly equivalent to  Bounds.MultiEllipsoid  with  Proposals.Uniform . The PolyChord algorithm is roughly equivalent to using  Proposals.RSlice . Other Parameters enlarge  - When fitting the bounds to live points, they will be enlarged (in terms of volume) by this linear factor. update_interval  - How often to refit the live points with the bounds as a fraction of  nactive . By default this will be determined using  default_update_interval  for the given proposal Proposals.Uniform  -  1.5 Proposals.RWalk  and  Proposals.RStagger  -  0.15 * walks Proposals.Slice  -  0.9 * ndims * slices Proposals.RSlice  -  2 * slices min_ncall  - The minimum number of iterations before trying to fit the first bound min_eff  - The maximum efficiency before trying to fit the first bound source"},{"id":155,"pagetitle":"API/Reference","title":"Convergence","ref":"/ns/stable/api/#Convergence","content":" Convergence There are a few convergence criteria available, by default the  dlogz  criterion will be used. dlogz=0.5  sample until the  fraction of the remaining evidence  is below the given value ( more info ). maxiter=Inf  stop after the given number of iterations maxcall=Inf  stop after the given number of  log-likelihood function calls maxlogl=Inf  stop after reaching the target log-likelihood"},{"id":156,"pagetitle":"API/Reference","title":"Bounds","ref":"/ns/stable/api/#Bounds","content":" Bounds"},{"id":157,"pagetitle":"API/Reference","title":"NestedSamplers.Bounds","ref":"/ns/stable/api/#NestedSamplers.Bounds","content":" NestedSamplers.Bounds  —  Module NestedSamplers.Bounds This module contains the different algorithms for bounding the prior volume. The available implementations are Bounds.NoBounds  - no bounds on the prior volume (equivalent to a unit cube) Bounds.Ellipsoid  - bound using a single ellipsoid Bounds.MultiEllipsoid  - bound using multiple ellipsoids in an optimal cluster source"},{"id":158,"pagetitle":"API/Reference","title":"NestedSamplers.Bounds.NoBounds","ref":"/ns/stable/api/#NestedSamplers.Bounds.NoBounds","content":" NestedSamplers.Bounds.NoBounds  —  Type Bounds.NoBounds([T=Float64], N) Unbounded prior volume; equivalent to the unit cube in  N  dimensions. source"},{"id":159,"pagetitle":"API/Reference","title":"NestedSamplers.Bounds.Ellipsoid","ref":"/ns/stable/api/#NestedSamplers.Bounds.Ellipsoid","content":" NestedSamplers.Bounds.Ellipsoid  —  Type Bounds.Ellipsoid([T=Float64], N)\nBounds.Ellipsoid(center::AbstractVector, A::AbstractMatrix) An  N -dimensional ellipsoid defined by \\[(x - center)^T A (x - center) = 1\\] where  size(center) == (N,)  and  size(A) == (N,N) . source"},{"id":160,"pagetitle":"API/Reference","title":"NestedSamplers.Bounds.MultiEllipsoid","ref":"/ns/stable/api/#NestedSamplers.Bounds.MultiEllipsoid","content":" NestedSamplers.Bounds.MultiEllipsoid  —  Type Bounds.MultiEllipsoid([T=Float64], ndims)\nBounds.MultiEllipsoid(::AbstractVector{Ellipsoid}) Use multiple  Ellipsoid s in an optimal clustering to bound prior space. For more details about the bounding algorithm, see the extended help ( ??Bounds.MultiEllipsoid ) source"},{"id":161,"pagetitle":"API/Reference","title":"Proposals","ref":"/ns/stable/api/#Proposals","content":" Proposals"},{"id":162,"pagetitle":"API/Reference","title":"NestedSamplers.Proposals","ref":"/ns/stable/api/#NestedSamplers.Proposals","content":" NestedSamplers.Proposals  —  Module NestedSamplers.Proposals This module contains the different algorithms for proposing new points within a bounding volume in unit space. The available implementations are Proposals.Uniform  - samples uniformly within the bounding volume Proposals.RWalk  - random walks to a new point given an existing one Proposals.RStagger  - random staggering away to a new point given an existing one Proposals.Slice  - slicing away to a new point given an existing one Proposals.RSlice  - random slicing away to a new point given an existing one source"},{"id":163,"pagetitle":"API/Reference","title":"NestedSamplers.Proposals.Uniform","ref":"/ns/stable/api/#NestedSamplers.Proposals.Uniform","content":" NestedSamplers.Proposals.Uniform  —  Type Proposals.Uniform() Propose a new live point by uniformly sampling within the bounding volume. source"},{"id":164,"pagetitle":"API/Reference","title":"NestedSamplers.Proposals.RWalk","ref":"/ns/stable/api/#NestedSamplers.Proposals.RWalk","content":" NestedSamplers.Proposals.RWalk  —  Type Proposals.RWalk(;ratio=0.5, walks=25, scale=1) Propose a new live point by random walking away from an existing live point. Parameters ratio  is the target acceptance ratio walks  is the minimum number of steps to take scale  is the proposal distribution scale, which will update  between  proposals. source"},{"id":165,"pagetitle":"API/Reference","title":"NestedSamplers.Proposals.RStagger","ref":"/ns/stable/api/#NestedSamplers.Proposals.RStagger","content":" NestedSamplers.Proposals.RStagger  —  Type Proposals.RStagger(;ratio=0.5, walks=25, scale=1) Propose a new live point by random staggering away from an existing live point.  This differs from the random walk proposal in that the step size here is exponentially adjusted to reach a target acceptance rate  during  each proposal, in addition to  between  proposals. Parameters ratio  is the target acceptance ratio walks  is the minimum number of steps to take scale  is the proposal distribution scale, which will update  between  proposals. source"},{"id":166,"pagetitle":"API/Reference","title":"NestedSamplers.Proposals.Slice","ref":"/ns/stable/api/#NestedSamplers.Proposals.Slice","content":" NestedSamplers.Proposals.Slice  —  Type Proposals.Slice(;slices=5, scale=1) Propose a new live point by a series of random slices away from an existing live point. This is a standard  Gibbs-like  implementation where a single multivariate slice is a combination of  slices  univariate slices through each axis. Parameters slices  is the minimum number of slices scale  is the proposal distribution scale, which will update  between  proposals. source"},{"id":167,"pagetitle":"API/Reference","title":"NestedSamplers.Proposals.RSlice","ref":"/ns/stable/api/#NestedSamplers.Proposals.RSlice","content":" NestedSamplers.Proposals.RSlice  —  Type Proposals.RSlice(;slices=5, scale=1) Propose a new live point by a series of random slices away from an existing live point. This is a standard  random  implementation where each slice is along a random direction based on the provided axes. Parameters slices  is the minimum number of slices scale  is the proposal distribution scale, which will update  between  proposals. source"},{"id":168,"pagetitle":"API/Reference","title":"Models","ref":"/ns/stable/api/#Models","content":" Models"},{"id":169,"pagetitle":"API/Reference","title":"NestedSamplers.Models","ref":"/ns/stable/api/#NestedSamplers.Models","content":" NestedSamplers.Models  —  Module This module contains various statistical models in the form of  NestedModel s. These models can be used for examples and for testing. Models.GaussianShells Models.CorrelatedGaussian source"},{"id":170,"pagetitle":"API/Reference","title":"NestedSamplers.Models.GaussianShells","ref":"/ns/stable/api/#NestedSamplers.Models.GaussianShells","content":" NestedSamplers.Models.GaussianShells  —  Function Models.GaussianShells() 2-D Gaussian shells centered at  [-3.5, 0]  and  [3.5, 0]  with a radius of 2 and a shell width of 0.1 Examples julia> model, lnZ = Models.GaussianShells();\n\njulia> lnZ\n-1.75 source"},{"id":171,"pagetitle":"API/Reference","title":"NestedSamplers.Models.CorrelatedGaussian","ref":"/ns/stable/api/#NestedSamplers.Models.CorrelatedGaussian","content":" NestedSamplers.Models.CorrelatedGaussian  —  Function Models.CorrelatedGaussian(ndims) Creates a highly-correlated Gaussian with the given dimensionality. \\[\\mathbf\\theta \\sim \\mathcal{N}\\left(2\\mathbf{1}, \\mathbf{I}\\right)\\] \\[\\Sigma_{ij} = \\begin{cases} 1 &\\quad i=j \\\\ 0.95 &\\quad i\\neq j \\end{cases}\\] \\[\\mathcal{L}(\\mathbf\\theta) = \\mathcal{N}\\left(\\mathbf\\theta | \\mathbf{0}, \\mathbf\\Sigma \\right)\\] the analytical evidence of the model is \\[Z = \\mathcal{N}\\left(2\\mathbf{1} | \\mathbf{0}, \\mathbf\\Sigma + \\mathbf{I} \\right)\\] Examples julia> model, lnZ = Models.CorrelatedGaussian(10);\n\njulia> lnZ\n-12.482738597926607 source"},{"id":174,"pagetitle":"Correlated Gaussian","title":"Correlated Gaussian","ref":"/ns/stable/examples/correlated/#Correlated-Gaussian","content":" Correlated Gaussian This example will explore a highly-correlated Gaussian using  Models.CorrelatedGaussian . This model uses a conjuage Gaussian prior, see the docstring for the mathematical definition."},{"id":175,"pagetitle":"Correlated Gaussian","title":"Setup","ref":"/ns/stable/examples/correlated/#Setup","content":" Setup For this example, you'll need to add the following packages julia>]add Distributions MCMCChains Measurements NestedSamplers StatsBase StatsPlots"},{"id":176,"pagetitle":"Correlated Gaussian","title":"Define model","ref":"/ns/stable/examples/correlated/#Define-model","content":" Define model using NestedSamplers\n\n# set up a 4-dimensional Gaussian\nD = 4\nmodel, logz = Models.CorrelatedGaussian(D) let's take a look at a couple of parameters to see what the likelihood surface looks like using StatsPlots\n\nθ1 = range(-1, 1, length=1000)\nθ2 = range(-1, 1, length=1000)\nlogf = [model.loglike([t1, t2, 0, 0]) for t2 in θ2, t1 in θ1]\nheatmap(\n    θ1, θ2, exp.(logf),\n    aspect_ratio=1,\n    xlims=extrema(θ1),\n    ylims=extrema(θ2),\n    xlabel=\"θ1\",\n    ylabel=\"θ2\"\n)"},{"id":177,"pagetitle":"Correlated Gaussian","title":"Sample","ref":"/ns/stable/examples/correlated/#Sample","content":" Sample using MCMCChains\nusing StatsBase\n# using single Ellipsoid for bounds\n# using Gibbs-style slicing for proposing new points\nsampler = Nested(D, 50D;\n    bounds=Bounds.Ellipsoid,\n    proposal=Proposals.Slice()\n)\nnames = [\"θ_$i\" for i in 1:D]\nchain, state = sample(model, sampler; dlogz=0.01, param_names=names)\n# resample chain using statistical weights\nchain_resampled = sample(chain, Weights(vec(chain[:weights])), length(chain));"},{"id":178,"pagetitle":"Correlated Gaussian","title":"Results","ref":"/ns/stable/examples/correlated/#Results","content":" Results chain_resampled Chains MCMC chain (2350×5×1 Array{Float64, 3}):\n\nIterations        = 1:2350\nNumber of chains  = 1\nSamples per chain = 2350\nparameters        = θ_4, θ_1, θ_2, θ_3\ninternals         = weights\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat\n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64\n\n         θ_1    1.5516    0.5344     0.0110    0.0134   2200.7829    1.0010\n         θ_2    1.5807    0.5107     0.0105    0.0103   2273.9140    1.0012\n         θ_3    1.5702    0.5353     0.0110    0.0128   2151.7091    1.0025\n         θ_4    1.5758    0.5044     0.0104    0.0128   2163.6311    1.0027\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n         θ_1    0.5364    1.1896    1.5638    1.9298    2.5273\n         θ_2    0.5894    1.2055    1.5788    1.9714    2.4899\n         θ_3    0.4824    1.2044    1.5810    1.9622    2.5383\n         θ_4    0.5462    1.2371    1.5836    1.9230    2.5612\n corner(chain_resampled) using Measurements\nlogz_est = state.logz ± state.logzerr\ndiff = logz_est - logz\nprint(\"logz: \", logz, \"\\nestimate: \", logz_est, \"\\ndiff: \", diff)"},{"id":181,"pagetitle":"Gaussian Shells","title":"Gaussian Shells","ref":"/ns/stable/examples/shells/#Gaussian-Shells","content":" Gaussian Shells This example will explore the classic Gaussian shells model using  Models.GaussianShells ."},{"id":182,"pagetitle":"Gaussian Shells","title":"Setup","ref":"/ns/stable/examples/shells/#Setup","content":" Setup For this example, you'll need to add the following packages julia>]add Distributions MCMCChains Measurements NestedSamplers StatsBase StatsPlots"},{"id":183,"pagetitle":"Gaussian Shells","title":"Define model","ref":"/ns/stable/examples/shells/#Define-model","content":" Define model using NestedSamplers\n\nmodel, logz = Models.GaussianShells() let's take a look at a couple of parameters to see what the likelihood surface looks like using StatsPlots\n\nx = range(-6, 6, length=1000)\ny = range(-6, 6, length=1000)\nlogf = [model.loglike([xi, yi]) for yi in y, xi in x]\nheatmap(\n    x, y, exp.(logf),\n    aspect_ratio=1,\n    xlims=extrema(x),\n    ylims=extrema(y),\n    xlabel=\"x\",\n    ylabel=\"y\",\n    size=(400, 400)\n)"},{"id":184,"pagetitle":"Gaussian Shells","title":"Sample","ref":"/ns/stable/examples/shells/#Sample","content":" Sample using MCMCChains\nusing StatsBase\n# using multi-ellipsoid for bounds\n# using default rejection sampler for proposals\nsampler = Nested(2, 1000)\nchain, state = sample(model, sampler; dlogz=0.05, param_names=[\"x\", \"y\"])\n# resample chain using statistical weights\nchain_resampled = sample(chain, Weights(vec(chain[:weights])), length(chain));"},{"id":185,"pagetitle":"Gaussian Shells","title":"Results","ref":"/ns/stable/examples/shells/#Results","content":" Results chain_resampled Chains MCMC chain (7072×3×1 Array{Float64, 3}):\n\nIterations        = 1:7072\nNumber of chains  = 1\nSamples per chain = 7072\nparameters        = y, x\ninternals         = weights\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat\n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64\n\n           x    0.0980    3.8160     0.0454    0.0487   6822.1849    1.0001\n           y   -0.0012    1.4043     0.0167    0.0153   7340.2303    1.0002\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n           x   -5.4828   -3.3625    1.4146    3.7535    5.4787\n           y   -2.0464   -1.4215    0.0821    1.3428    2.0243\n marginalkde(chain[:x], chain[:y]) density(chain_resampled)\nvline!([-5.5, -1.5, 1.5, 5.5], c=:black, ls=:dash, sp=1)\nvline!([-2, 2], c=:black, ls=:dash, sp=2) using Measurements\nlogz_est = state.logz ± state.logzerr\ndiff = logz_est - logz\nprint(\"logz: \", logz, \"\\nestimate: \", logz_est, \"\\ndiff: \", diff)"},{"id":189,"pagetitle":"MCMCChains","title":"MCMCChains","ref":"/mcmcc/stable/#MCMCChains","content":" MCMCChains Implementation of Julia types for summarizing MCMC simulations and utility functions for  diagnostics  and  visualizations ."},{"id":192,"pagetitle":"Chains","title":"Chains","ref":"/mcmcc/stable/chains/#Chains","content":" Chains The methods listed below are defined in  src/chains.jl ."},{"id":193,"pagetitle":"Chains","title":"MCMCChains.Chains","ref":"/mcmcc/stable/chains/#MCMCChains.Chains-Tuple{Chains, Union{String, Symbol}}","content":" MCMCChains.Chains  —  Method Chains(c::Chains, section::Union{Symbol,String})\nChains(c::Chains, sections) Return a new chain with only a specific  section  or multiple  sections  pulled out. Examples julia> chn = Chains(rand(100, 2, 1), [:a, :b], Dict(:internals => [:a]));\n\njulia> names(chn)\n2-element Vector{Symbol}:\n :a\n :b\n\njulia> chn2 = Chains(chn, :internals);\n\njulia> names(chn2)\n1-element Vector{Symbol}:\n :a source"},{"id":194,"pagetitle":"Chains","title":"Base.get","ref":"/mcmcc/stable/chains/#Base.get-Tuple{Chains, Vector{Symbol}}","content":" Base.get  —  Method Base.get(c::Chains, v::Symbol; flatten=false)\nBase.get(c::Chains, vs::Vector{Symbol}; flatten=false) Return a  NamedTuple  with  v  as the key, and matching parameter names as the values. Passing  flatten=true  will return a  NamedTuple  with keys ungrouped. Example julia> chn = Chains([1:2 3:4]);\n\njulia> get(chn, :param_1)\n(param_1 = [1; 2;;],)\n\njulia> get(chn, [:param_2])\n(param_2 = [3; 4;;],)\n\njulia> get(chn, :param_1; flatten=true)\n(param_1 = 1,) source"},{"id":195,"pagetitle":"Chains","title":"Base.get","ref":"/mcmcc/stable/chains/#Base.get-Tuple{Chains}","content":" Base.get  —  Method get(c::Chains; section::Union{Symbol,AbstractVector{Symbol}}; flatten=false) Return all parameters in a given section(s) as a  NamedTuple . Passing  flatten=true  will return a  NamedTuple  with keys ungrouped. Example julia> chn = Chains([1:2 3:4], [:a, :b], Dict(:internals => [:a]));\n\njulia> get(chn; section=:parameters)\n(b = [3; 4;;],)\n\njulia> get(chn; section=[:internals])\n(a = [1; 2;;],) source"},{"id":196,"pagetitle":"Chains","title":"Base.names","ref":"/mcmcc/stable/chains/#Base.names-Tuple{Chains, Any}","content":" Base.names  —  Method names(chains::Chains, sections) Return the parameter names of the  sections  in the  chains . source"},{"id":197,"pagetitle":"Chains","title":"Base.names","ref":"/mcmcc/stable/chains/#Base.names-Tuple{Chains, Symbol}","content":" Base.names  —  Method names(chains::Chains, section::Symbol) Return the parameter names of a  section  in the  chains . source"},{"id":198,"pagetitle":"Chains","title":"Base.names","ref":"/mcmcc/stable/chains/#Base.names-Tuple{Chains}","content":" Base.names  —  Method names(chains::Chains) Return the parameter names in the  chains . source"},{"id":199,"pagetitle":"Chains","title":"Base.range","ref":"/mcmcc/stable/chains/#Base.range-Tuple{Chains}","content":" Base.range  —  Method range(chains::Chains) Return the range of iteration indices of the  chains . source"},{"id":200,"pagetitle":"Chains","title":"Base.sort","ref":"/mcmcc/stable/chains/#Base.sort-Tuple{Chains}","content":" Base.sort  —  Method sort(c::Chains[; lt=NaturalSort.natural]) Return a new column-sorted version of  c . By default the columns are sorted in natural sort order. source"},{"id":201,"pagetitle":"Chains","title":"MCMCChains.chains","ref":"/mcmcc/stable/chains/#MCMCChains.chains-Tuple{Chains}","content":" MCMCChains.chains  —  Method chains(c::Chains) Return the names or symbols of each chain in a  Chains  object. source"},{"id":202,"pagetitle":"Chains","title":"MCMCChains.compute_duration","ref":"/mcmcc/stable/chains/#MCMCChains.compute_duration-Tuple{Chains}","content":" MCMCChains.compute_duration  —  Method compute_duration(c::Chains; start=start_times(c), stop=stop_times(c)) Calculate the compute time for all chains in seconds. The duration is calculated as the sum of  start - stop  in seconds.  compute_duration  is more useful in cases of parallel sampling, where  wall_duration  may understate how much computation time was utilitzed. source"},{"id":203,"pagetitle":"Chains","title":"MCMCChains.get_params","ref":"/mcmcc/stable/chains/#MCMCChains.get_params-Tuple{Chains}","content":" MCMCChains.get_params  —  Method get_params(c::Chains; flatten=false) Return all parameters packaged as a  NamedTuple . Variables with a bracket in their name (as in \"P[1]\") will be grouped into the returned value as P. Passing  flatten=true  will return a  NamedTuple  with keys ungrouped. Example julia> chn = Chains(1:5);\n\njulia> x = get_params(chn);\n\njulia> x.param_1\n2-dimensional AxisArray{Int64,2,...} with axes:\n    :iter, 1:1:5\n    :chain, 1:1\nAnd data, a 5×1 Matrix{Int64}:\n 1\n 2\n 3\n 4\n 5 source"},{"id":204,"pagetitle":"Chains","title":"MCMCChains.get_sections","ref":"/mcmcc/stable/chains/#MCMCChains.get_sections","content":" MCMCChains.get_sections  —  Function get_sections(chains[, sections]) Return multiple  Chains  objects, each containing only a single section. source"},{"id":205,"pagetitle":"Chains","title":"MCMCChains.group","ref":"/mcmcc/stable/chains/#MCMCChains.group-Tuple{Chains, Union{String, Symbol}}","content":" MCMCChains.group  —  Method group(chains::Chains, name::Union{String,Symbol}; index_type::Symbol=:bracket) Return a subset of the chain containing parameters with the same  name , but a different index. Bracket indexing format in the form of  :name[index]  is assumed by default. Use  index_type=:dot  for parameters with dot  indexing, i.e.  :sym.index . source"},{"id":206,"pagetitle":"Chains","title":"MCMCChains.header","ref":"/mcmcc/stable/chains/#MCMCChains.header-Tuple{Chains}","content":" MCMCChains.header  —  Method header(c::Chains; section=missing) Return a string containing summary information for a  Chains  object. If the  section  keyword is used, this function prints only the relevant section header. Example # Printing the whole header.\nheader(chn)\n\n# Print only one section's header.\nheader(chn, section = :parameter) source"},{"id":207,"pagetitle":"Chains","title":"MCMCChains.max_stop","ref":"/mcmcc/stable/chains/#MCMCChains.max_stop-Tuple{Chains}","content":" MCMCChains.max_stop  —  Method max_stop(c::Chains) Retrieve the maximum of the stop times (as  DateTime ) from  chain.info . It is assumed that the start times are stored in  chain.info.stop_time  as  DateTime  or unix timestamps of type  Float64 . source"},{"id":208,"pagetitle":"Chains","title":"MCMCChains.min_start","ref":"/mcmcc/stable/chains/#MCMCChains.min_start-Tuple{Chains}","content":" MCMCChains.min_start  —  Method min_start(c::Chains) Retrieve the minimum of the start times (as  DateTime ) from  chain.info . It is assumed that the start times are stored in  chain.info.start_time  as  DateTime  or unix timestamps of type  Float64 . source"},{"id":209,"pagetitle":"Chains","title":"MCMCChains.namesingroup","ref":"/mcmcc/stable/chains/#MCMCChains.namesingroup-Tuple{Chains, String}","content":" MCMCChains.namesingroup  —  Method namesingroup(chains::Chains, sym::Symbol; index_type::Symbol=:bracket) Return the parameters with the same name  sym , but have a different index. Bracket indexing format in the form of  :sym[index]  is assumed by default. Use  index_type=:dot  for parameters with dot  indexing, i.e.  :sym.index . If the chain contains a parameter of name  :sym  it will be returned as well. Example julia> chn = Chains(rand(100, 2, 2), [\"A[1]\", \"A[2]\"]);\n\njulia> namesingroup(chn, :A)\n2-element Vector{Symbol}:\n Symbol(\"A[1]\")\n Symbol(\"A[2]\") julia> chn = Chains(rand(100, 3, 2), [\"A.1\", \"A.2\", \"B\"]);\n\njulia> namesingroup(chn, :A; index_type=:dot)\n2-element Vector{Symbol}:\n Symbol(\"A.1\")\n Symbol(\"A.2\") source"},{"id":210,"pagetitle":"Chains","title":"MCMCChains.replacenames","ref":"/mcmcc/stable/chains/#MCMCChains.replacenames-Tuple{Chains, AbstractDict}","content":" MCMCChains.replacenames  —  Method replacenames(chains::Chains, dict::AbstractDict) Replace parameter names by creating a new  Chains  object that shares the same underlying data. Examples julia> chn = Chains(rand(100, 2, 2), [\"one\", \"two\"]);\n\njulia> chn2 = replacenames(chn, \"one\" => \"A\");\n\njulia> names(chn2)\n2-element Vector{Symbol}:\n :A\n :two\n\njulia> chn3 = replacenames(chn2, Dict(\"A\" => \"one\", \"two\" => \"B\"));\n\njulia> names(chn3) \n2-element Vector{Symbol}:\n :one\n :B source"},{"id":211,"pagetitle":"Chains","title":"MCMCChains.resetrange","ref":"/mcmcc/stable/chains/#MCMCChains.resetrange-Tuple{Chains}","content":" MCMCChains.resetrange  —  Method resetrange(chains::Chains) Generate a new chain from  chains  with iterations indexed by  1:n , where  n  is the number of samples per chain. The new chain and  chains  share the same data in memory. source"},{"id":212,"pagetitle":"Chains","title":"MCMCChains.sections","ref":"/mcmcc/stable/chains/#MCMCChains.sections-Tuple{Chains}","content":" MCMCChains.sections  —  Method sections(c::Chains) Retrieve a list of the sections in a chain. source"},{"id":213,"pagetitle":"Chains","title":"MCMCChains.set_section","ref":"/mcmcc/stable/chains/#MCMCChains.set_section-Tuple{Chains, Any}","content":" MCMCChains.set_section  —  Method set_section(chains::Chains, namemap) Create a new  Chains  object from  chains  with the provided  namemap  mapping of parameter names. Both chains share the same underlying data. Any parameters in the chain that are unassigned will be placed into the  :parameters  section. source"},{"id":214,"pagetitle":"Chains","title":"MCMCChains.setinfo","ref":"/mcmcc/stable/chains/#MCMCChains.setinfo-Tuple{Chains, NamedTuple}","content":" MCMCChains.setinfo  —  Method setinfo(c::Chains, n::NamedTuple) Return a new  Chains  object with a  NamedTuple  type  n  placed in the  info  field. Example new_chn = setinfo(chn, NamedTuple{(:a, :b)}((1, 2))) source"},{"id":215,"pagetitle":"Chains","title":"MCMCChains.setrange","ref":"/mcmcc/stable/chains/#MCMCChains.setrange-Tuple{Chains, AbstractVector{Int64}}","content":" MCMCChains.setrange  —  Method setrange(chains::Chains, range::AbstractVector{Int}) Generate a new chain from  chains  with iterations indexed by  range . The new chain and  chains  share the same data in memory. source"},{"id":216,"pagetitle":"Chains","title":"MCMCChains.start_times","ref":"/mcmcc/stable/chains/#MCMCChains.start_times-Tuple{Chains}","content":" MCMCChains.start_times  —  Method start_times(c::Chains) Retrieve the contents of  c.info.start_time , or  missing  if no   start_time  is set. source"},{"id":217,"pagetitle":"Chains","title":"MCMCChains.stop_times","ref":"/mcmcc/stable/chains/#MCMCChains.stop_times-Tuple{Chains}","content":" MCMCChains.stop_times  —  Method stop_times(c::Chains) Retrieve the contents of  c.info.stop_time , or  missing  if no   stop_time  is set. source"},{"id":218,"pagetitle":"Chains","title":"MCMCChains.wall_duration","ref":"/mcmcc/stable/chains/#MCMCChains.wall_duration-Tuple{Chains}","content":" MCMCChains.wall_duration  —  Method wall_duration(c::Chains; start=min_start(c), stop=max_stop(c)) Calculate the wall clock time for all chains in seconds. The duration is calculated as  stop - start , where as default  stop  is the latest stopping time and  start  is the earliest starting time. source"},{"id":221,"pagetitle":"Diagnostics","title":"Diagnostics","ref":"/mcmcc/stable/diagnostics/#Diagnostics","content":" Diagnostics"},{"id":222,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.discretediag","ref":"/mcmcc/stable/diagnostics/#MCMCDiagnosticTools.discretediag-Tuple{Chains{var\"#s12\", A} where {var\"#s12\"<:Real, A<:(AxisArrays.AxisArray{var\"#s12\", 3})}}","content":" MCMCDiagnosticTools.discretediag  —  Method discretediag(chains::Chains{<:Real}; sections, kwargs...) Discrete diagnostic where  method  can be  [:weiss, :hangartner, :DARBOOT, MCBOOT, :billinsgley, :billingsleyBOOT] . source"},{"id":223,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.rstar","ref":"/mcmcc/stable/diagnostics/#MCMCDiagnosticTools.rstar-Tuple{MLJModelInterface.Supervised, Chains}","content":" MCMCDiagnosticTools.rstar  —  Method rstar(rng=Random.GLOBAL_RNG, classifier, chains::Chains; kwargs...) Compute the  $R^*$  convergence diagnostic of the MCMC  chains  with the  classifier . The keyword arguments supported here are the same as those in  rstar  for arrays of samples and chain indices. Examples julia> using MLJBase, MLJDecisionTreeInterface, Statistics\n\njulia> chains = Chains(fill(4.0, 100, 2, 3)); One can compute the distribution of the  $R^*$  statistic with the probabilistic classifier. julia> distribution = rstar(DecisionTreeClassifier(), chains);\n\njulia> isapprox(mean(distribution), 1; atol=0.1)\ntrue For deterministic classifiers, a single  $R^*$  statistic is returned. julia> decisiontree_deterministic = Pipeline(\n           DecisionTreeClassifier();\n           operation=predict_mode,\n       );\n\njulia> value = rstar(decisiontree_deterministic, chains);\n\njulia> isapprox(value, 1; atol=0.2)\ntrue source"},{"id":224,"pagetitle":"Diagnostics","title":"MCMCDiagnosticTools.ess_rhat","ref":"/mcmcc/stable/diagnostics/#MCMCDiagnosticTools.ess_rhat-Tuple{Chains}","content":" MCMCDiagnosticTools.ess_rhat  —  Method ess_rhat(chains::Chains; duration=compute_duration, kwargs...) Estimate the effective sample size and the potential scale reduction. ESS per second options include  duration=MCMCChains.compute_duration  (the default) and  duration=MCMCChains.wall_duration . source"},{"id":227,"pagetitle":"Gadfly.jl","title":"Gadfly.jl plots","ref":"/mcmcc/stable/gadfly/#Gadfly.jl-plots","content":" Gadfly.jl plots To plot the Chains via  Gadfly.jl , use the DataFrames constructor: using DataFrames\nusing CategoricalArrays\nusing Gadfly\nusing MCMCChains\n\nchn = Chains(randn(100, 2, 3), [:A, :B])\ndf = DataFrame(chn)\ndf[!, :chain] = categorical(df.chain)\n\nplot(df, x=:A, color=:chain, Geom.density, Guide.ylabel(\"Density\")) A -20 -15 -10 -5 0 5 10 15 20 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -20 -10 0 10 20 -15.0 -14.5 -14.0 -13.5 -13.0 -12.5 -12.0 -11.5 -11.0 -10.5 -10.0 -9.5 -9.0 -8.5 -8.0 -7.5 -7.0 -6.5 -6.0 -5.5 -5.0 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0 1 2 3 chain h,j,k,l,arrows,drag to pan i,o,+,-,scroll,shift-drag to zoom r,dbl-click to reset c for coordinates ? for help ? -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 -0.5 0.0 0.5 1.0 -0.50 -0.48 -0.46 -0.44 -0.42 -0.40 -0.38 -0.36 -0.34 -0.32 -0.30 -0.28 -0.26 -0.24 -0.22 -0.20 -0.18 -0.16 -0.14 -0.12 -0.10 -0.08 -0.06 -0.04 -0.02 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00 Density"},{"id":228,"pagetitle":"Gadfly.jl","title":"Multiple parameters","ref":"/mcmcc/stable/gadfly/#Multiple-parameters","content":" Multiple parameters Or, to show multiple parameters in one plot, use  DataFrames.stack sdf = stack(df, names(chn), variable_name=:parameter)\nfirst(sdf, 5) 5 rows × 4 columns iteration chain parameter value Int64 Cat… String Float64 1 1 1 A 0.0868661 2 2 1 A -0.147563 3 3 1 A 2.44663 4 4 1 A 0.257283 5 5 1 A -0.24736 and  Gadfly.Geom.subplot_grid plot(sdf, ygroup=:parameter, x=:value, color=:chain,\n    Geom.subplot_grid(Geom.density), Guide.ylabel(\"Density\")) value 1 2 3 chain -20 -15 -10 -5 0 5 10 15 20 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -20 -10 0 10 20 -15.0 -14.5 -14.0 -13.5 -13.0 -12.5 -12.0 -11.5 -11.0 -10.5 -10.0 -9.5 -9.0 -8.5 -8.0 -7.5 -7.0 -6.5 -6.0 -5.5 -5.0 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 -0.5 0.0 0.5 1.0 -0.50 -0.48 -0.46 -0.44 -0.42 -0.40 -0.38 -0.36 -0.34 -0.32 -0.30 -0.28 -0.26 -0.24 -0.22 -0.20 -0.18 -0.16 -0.14 -0.12 -0.10 -0.08 -0.06 -0.04 -0.02 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00 B -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 -0.5 0.0 0.5 1.0 -0.50 -0.48 -0.46 -0.44 -0.42 -0.40 -0.38 -0.36 -0.34 -0.32 -0.30 -0.28 -0.26 -0.24 -0.22 -0.20 -0.18 -0.16 -0.14 -0.12 -0.10 -0.08 -0.06 -0.04 -0.02 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00 A Density This is very flexible. For example, we can look at the first two chains only by using  DataFrames.filter first_chain = filter([:chain] => c -> c == 1 || c == 2, sdf)\n\nplot(first_chain, xgroup=:parameter, ygroup=:chain, x=:value,\n    Geom.subplot_grid(Geom.density, Guide.xlabel(orientation=:horizontal)),\n    Guide.xlabel(\"Parameter\"), Guide.ylabel(\"Chain\")) Parameter B A -20 -15 -10 -5 0 5 10 15 20 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -20 -10 0 10 20 -15.0 -14.5 -14.0 -13.5 -13.0 -12.5 -12.0 -11.5 -11.0 -10.5 -10.0 -9.5 -9.0 -8.5 -8.0 -7.5 -7.0 -6.5 -6.0 -5.5 -5.0 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0 -20 -15 -10 -5 0 5 10 15 20 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -20 -10 0 10 20 -15.0 -14.5 -14.0 -13.5 -13.0 -12.5 -12.0 -11.5 -11.0 -10.5 -10.0 -9.5 -9.0 -8.5 -8.0 -7.5 -7.0 -6.5 -6.0 -5.5 -5.0 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 10.5 11.0 11.5 12.0 12.5 13.0 13.5 14.0 14.5 15.0 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 -0.5 0.0 0.5 1.0 -0.50 -0.48 -0.46 -0.44 -0.42 -0.40 -0.38 -0.36 -0.34 -0.32 -0.30 -0.28 -0.26 -0.24 -0.22 -0.20 -0.18 -0.16 -0.14 -0.12 -0.10 -0.08 -0.06 -0.04 -0.02 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00 2 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 -0.5 0.0 0.5 1.0 -0.50 -0.48 -0.46 -0.44 -0.42 -0.40 -0.38 -0.36 -0.34 -0.32 -0.30 -0.28 -0.26 -0.24 -0.22 -0.20 -0.18 -0.16 -0.14 -0.12 -0.10 -0.08 -0.06 -0.04 -0.02 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70 0.72 0.74 0.76 0.78 0.80 0.82 0.84 0.86 0.88 0.90 0.92 0.94 0.96 0.98 1.00 1 Chain"},{"id":229,"pagetitle":"Gadfly.jl","title":"Trace","ref":"/mcmcc/stable/gadfly/#Trace","content":" Trace plot(first_chain, ygroup=:parameter, x=:iteration, y=:value, color=:chain,\n    Geom.subplot_grid(Geom.point), Guide.ylabel(\"Sample value\")) iteration 1 2 chain -150 -100 -50 0 50 100 150 200 250 -100 -90 -80 -70 -60 -50 -40 -30 -20 -10 0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 -100 0 100 200 -100 -95 -90 -85 -80 -75 -70 -65 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120 125 130 135 140 145 150 155 160 165 170 175 180 185 190 195 200 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 -9.0 -8.5 -8.0 -7.5 -7.0 -6.5 -6.0 -5.5 -5.0 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 -10 -5 0 5 10 -9.0 -8.8 -8.6 -8.4 -8.2 -8.0 -7.8 -7.6 -7.4 -7.2 -7.0 -6.8 -6.6 -6.4 -6.2 -6.0 -5.8 -5.6 -5.4 -5.2 -5.0 -4.8 -4.6 -4.4 -4.2 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2 -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6 8.8 9.0 B -10 -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 -9.0 -8.5 -8.0 -7.5 -7.0 -6.5 -6.0 -5.5 -5.0 -4.5 -4.0 -3.5 -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 9.0 -10 -5 0 5 10 -9.0 -8.8 -8.6 -8.4 -8.2 -8.0 -7.8 -7.6 -7.4 -7.2 -7.0 -6.8 -6.6 -6.4 -6.2 -6.0 -5.8 -5.6 -5.4 -5.2 -5.0 -4.8 -4.6 -4.4 -4.2 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2 -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 5.8 6.0 6.2 6.4 6.6 6.8 7.0 7.2 7.4 7.6 7.8 8.0 8.2 8.4 8.6 8.8 9.0 A Sample value"},{"id":232,"pagetitle":"Getting started","title":"Getting started","ref":"/mcmcc/stable/getting-started/#Getting-started","content":" Getting started"},{"id":233,"pagetitle":"Getting started","title":"Chains type","ref":"/mcmcc/stable/getting-started/#Chains-type","content":" Chains type"},{"id":234,"pagetitle":"Getting started","title":"MCMCChains.Chains","ref":"/mcmcc/stable/getting-started/#MCMCChains.Chains","content":" MCMCChains.Chains  —  Type Chains Parameters: value : An  AxisArray  object with axes  iter  ×  var  ×  chains logevidence  : A field containing the logevidence. name_map  : A  NamedTuple  mapping each variable to a section. info  : A  NamedTuple  containing miscellaneous information relevant to the chain. The  info  field can be set using  setinfo(c::Chains, n::NamedTuple) . source"},{"id":235,"pagetitle":"Getting started","title":"Indexing and parameter Names","ref":"/mcmcc/stable/getting-started/#Indexing-and-parameter-Names","content":" Indexing and parameter Names Chains can be constructed with parameter names. For example, to create a chains object with 500 samples, 2 parameters (named  a  and  b ) 3 chains use val = rand(500, 2, 3)\nchn = Chains(val, [:a, :b]) Chains MCMC chain (500×2×3 Array{Float64, 3}):\n\nIterations        = 1:1:500\nNumber of chains  = 3\nSamples per chain = 500\nparameters        = a, b\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat\n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64\n\n           a    0.4959    0.2925     0.0076    0.0088   1242.2144    0.9994\n           b    0.5049    0.2956     0.0076    0.0079   1480.9180    1.0011\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n           a    0.0245    0.2336    0.5015    0.7479    0.9695\n           b    0.0245    0.2399    0.5077    0.7758    0.9768\n By default, parameters will be given the name  param_i , where  i  is the parameter number: chn = Chains(rand(100, 2, 2)) Chains MCMC chain (100×2×2 Array{Float64, 3}):\n\nIterations        = 1:1:100\nNumber of chains  = 2\nSamples per chain = 100\nparameters        = param_1, param_2\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse        ess      rhat\n      Symbol   Float64   Float64    Float64   Float64    Float64   Float64\n\n     param_1    0.4926    0.2994     0.0212    0.0226   194.1312    1.0009\n     param_2    0.5139    0.2784     0.0197    0.0221   218.2370    0.9981\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n     param_1    0.0191    0.2334    0.5001    0.7764    0.9529\n     param_2    0.0352    0.2718    0.5279    0.7469    0.9725\n We can set and get indexes for parameter 2: chn_param2 = chn[1:5,2,:]; 2-dimensional AxisArray{Float64,2,...} with axes:\n    :iter, 1:1:5\n    :chain, 1:2\nAnd data, a 5×2 Matrix{Float64}:\n 0.162927  0.388083\n 0.716976  0.334046\n 0.417752  0.286943\n 0.920146  0.552454\n 0.647637  0.735593 chn[:,2,:] = fill(4, 100, 1, 2)\nchn Chains MCMC chain (100×2×2 Array{Float64, 3}):\n\nIterations        = 1:1:100\nNumber of chains  = 2\nSamples per chain = 100\nparameters        = param_1, param_2\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse        ess      rhat\n      Symbol   Float64   Float64    Float64   Float64    Float64   Float64\n\n     param_1    0.4926    0.2994     0.0212    0.0226   194.1312    1.0009\n     param_2    4.0000    0.0000     0.0000    0.0000        NaN       NaN\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n     param_1    0.0191    0.2334    0.5001    0.7764    0.9529\n     param_2    4.0000    4.0000    4.0000    4.0000    4.0000\n"},{"id":236,"pagetitle":"Getting started","title":"Rename Parameters","ref":"/mcmcc/stable/getting-started/#Rename-Parameters","content":" Rename Parameters Parameter names can be changed with the function  replacenames :"},{"id":237,"pagetitle":"Getting started","title":"MCMCChains.replacenames","ref":"/mcmcc/stable/getting-started/#MCMCChains.replacenames","content":" MCMCChains.replacenames  —  Function replacenames(chains::Chains, dict::AbstractDict) Replace parameter names by creating a new  Chains  object that shares the same underlying data. Examples julia> chn = Chains(rand(100, 2, 2), [\"one\", \"two\"]);\n\njulia> chn2 = replacenames(chn, \"one\" => \"A\");\n\njulia> names(chn2)\n2-element Vector{Symbol}:\n :A\n :two\n\njulia> chn3 = replacenames(chn2, Dict(\"A\" => \"one\", \"two\" => \"B\"));\n\njulia> names(chn3) \n2-element Vector{Symbol}:\n :one\n :B source"},{"id":238,"pagetitle":"Getting started","title":"Sections","ref":"/mcmcc/stable/getting-started/#Sections","content":" Sections Chains parameters are sorted into sections that represent groups of parameters, see   MCMCChains.group . By default, every chain contains a  parameters  section, to which all unassigned parameters are assigned to. Chains can be assigned a named map during construction: chn = Chains(rand(100, 4, 2), [:a, :b, :c, :d]) Chains MCMC chain (100×4×2 Array{Float64, 3}):\n\nIterations        = 1:1:100\nNumber of chains  = 2\nSamples per chain = 100\nparameters        = a, b, c, d\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse        ess      rhat\n      Symbol   Float64   Float64    Float64   Float64    Float64   Float64\n\n           a    0.4758    0.2866     0.0203    0.0205   176.2749    0.9920\n           b    0.4675    0.2940     0.0208    0.0122   303.8196    0.9905\n           c    0.4679    0.2858     0.0202    0.0147   221.0484    0.9930\n           d    0.5162    0.3041     0.0215    0.0257   135.8895    1.0325\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n           a    0.0217    0.2177    0.4505    0.6937    0.9651\n           b    0.0175    0.2070    0.4482    0.7156    0.9819\n           c    0.0243    0.2259    0.4518    0.6724    0.9740\n           d    0.0138    0.2743    0.4781    0.7673    0.9823\n The  MCMCChains.set_section  function returns a new  Chains  object: chn2 = set_section(chn, Dict(:internals => [:c, :d])) Chains MCMC chain (100×4×2 Array{Float64, 3}):\n\nIterations        = 1:1:100\nNumber of chains  = 2\nSamples per chain = 100\nparameters        = a, b\ninternals         = c, d\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse        ess      rhat\n      Symbol   Float64   Float64    Float64   Float64    Float64   Float64\n\n           a    0.4758    0.2866     0.0203    0.0205   176.2749    0.9920\n           b    0.4675    0.2940     0.0208    0.0122   303.8196    0.9905\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n           a    0.0217    0.2177    0.4505    0.6937    0.9651\n           b    0.0175    0.2070    0.4482    0.7156    0.9819\n Note that only  a  and  b  are being shown. You can explicity retrieve an array of the summary statistics and the quantiles of the  :internals  section by calling  describe(chn; sections = :internals) , or of all variables with  describe(chn; sections = nothing) . Many functions such as  MCMCChains.summarize  support the  sections  keyword argument."},{"id":239,"pagetitle":"Getting started","title":"Groups of parameters","ref":"/mcmcc/stable/getting-started/#Groups-of-parameters","content":" Groups of parameters You can access the names of all parameters in a  chain  that belong to the group  name  by using"},{"id":240,"pagetitle":"Getting started","title":"MCMCChains.namesingroup","ref":"/mcmcc/stable/getting-started/#MCMCChains.namesingroup","content":" MCMCChains.namesingroup  —  Function namesingroup(chains::Chains, sym::Symbol; index_type::Symbol=:bracket) Return the parameters with the same name  sym , but have a different index. Bracket indexing format in the form of  :sym[index]  is assumed by default. Use  index_type=:dot  for parameters with dot  indexing, i.e.  :sym.index . If the chain contains a parameter of name  :sym  it will be returned as well. Example julia> chn = Chains(rand(100, 2, 2), [\"A[1]\", \"A[2]\"]);\n\njulia> namesingroup(chn, :A)\n2-element Vector{Symbol}:\n Symbol(\"A[1]\")\n Symbol(\"A[2]\") julia> chn = Chains(rand(100, 3, 2), [\"A.1\", \"A.2\", \"B\"]);\n\njulia> namesingroup(chn, :A; index_type=:dot)\n2-element Vector{Symbol}:\n Symbol(\"A.1\")\n Symbol(\"A.2\") source"},{"id":241,"pagetitle":"Getting started","title":"The  get  Function","ref":"/mcmcc/stable/getting-started/#The-get-Function","content":" The  get  Function MCMCChains also provides a  get  function designed to make it easier to access parameters: val = rand(6, 3, 1)\nchn = Chains(val, [:a, :b, :c]);\n\nx = get(chn, :a) (a = [0.4406973184362195; 0.06413541593814787; … ; 0.6564549998197146; 0.4260695808953028;;],) You can also access the variables via  getproperty : x.a 2-dimensional AxisArray{Float64,2,...} with axes:\n    :iter, 1:1:6\n    :chain, 1:1\nAnd data, a 6×1 Matrix{Float64}:\n 0.4406973184362195\n 0.06413541593814787\n 0.45084693164962364\n 0.7032412293454546\n 0.6564549998197146\n 0.4260695808953028 get  also accepts vectors of things to retrieve, so you can call  x = get(chn, [:a, :b]) (a = [0.4406973184362195; 0.06413541593814787; … ; 0.6564549998197146; 0.4260695808953028;;],\n b = [0.94708496660485; 0.20744026514507408; … ; 0.8847254154251538; 0.33010442589688427;;],)"},{"id":242,"pagetitle":"Getting started","title":"Saving and Loading Chains","ref":"/mcmcc/stable/getting-started/#Saving-and-Loading-Chains","content":" Saving and Loading Chains Like any Julia object, a  Chains  object can be saved using  Serialization.serialize  and loaded back by  Serialization.deserialize  as identical as possible. Note, however, that in general  this process will not work if the reading and writing are done by different versions of Julia, or an instance of Julia with a different system image . You might want to consider  JLSO  for saving metadata such as the Julia version and the versions of all packages installed as well. using Serialization\n\nserialize(\"chain-file.jls\", chn)\nchn2 = deserialize(\"chain-file.jls\") The  MCMCChainsStorage.jl  package also provides the ability to serialize/deserialize a chain to an HDF5 file across different versions of Julia and/or different system images."},{"id":243,"pagetitle":"Getting started","title":"Exporting Chains","ref":"/mcmcc/stable/getting-started/#Exporting-Chains","content":" Exporting Chains A few utility export functions have been provided to convert  Chains  objects to either an Array or a DataFrame: chn = Chains(rand(3, 2, 2), [:a, :b])\n\nArray(chn) 6×2 Matrix{Float64}:\n 0.591424  0.0238958\n 0.63722   0.117082\n 0.384731  0.152856\n 0.534329  0.0840422\n 0.020047  0.0846894\n 0.196054  0.576018 Array(chn, [:parameters]) 6×2 Matrix{Float64}:\n 0.591424  0.0238958\n 0.63722   0.117082\n 0.384731  0.152856\n 0.534329  0.0840422\n 0.020047  0.0846894\n 0.196054  0.576018 By default chains are appended. This can be disabled by using the  append_chains  keyword  argument: A = Array(chn, append_chains=false) 2-element Vector{Matrix{Float64}}:\n [0.5914241067613842 0.023895751073906513; 0.6372198453275196 0.11708206509997343; 0.3847313444636996 0.15285574309518535]\n [0.5343285749143705 0.08404220391148931; 0.020047016741082002 0.08468937966306267; 0.19605406287287575 0.5760176829441074] which will return a matrix for each chain. For example, for the second chain: A[2] 3×2 Matrix{Float64}:\n 0.534329  0.0840422\n 0.020047  0.0846894\n 0.196054  0.576018 Similarly, for DataFrames: using DataFrames\n\nDataFrame(chn) 6 rows × 4 columns iteration chain a b Int64 Int64 Float64 Float64 1 1 1 0.591424 0.0238958 2 2 1 0.63722 0.117082 3 3 1 0.384731 0.152856 4 1 2 0.534329 0.0840422 5 2 2 0.020047 0.0846894 6 3 2 0.196054 0.576018 See also  ?DataFrame  and  ?Array  for more help."},{"id":244,"pagetitle":"Getting started","title":"Sampling Chains","ref":"/mcmcc/stable/getting-started/#Sampling-Chains","content":" Sampling Chains MCMCChains overloads several  sample  methods as defined in StatsBase:"},{"id":245,"pagetitle":"Getting started","title":"StatsBase.sample","ref":"/mcmcc/stable/getting-started/#StatsBase.sample-Tuple{Chains, Integer}","content":" StatsBase.sample  —  Method sample([rng,] chn::Chains, [wv::AbstractWeights,] n; replace=true, ordered=false) Sample  n  samples from the pooled (!) chain  chn . The keyword arguments  replace  and  ordered  determine whether sampling is performed with replacement and whether the sample is ordered, respectively. If specified, sampling probabilities are proportional to weights  wv . Note If  chn  contains multiple chains, they are pooled (i.e., appended) before sampling. This ensures that even in this case exactly  n  samples are returned: julia> chn = Chains(randn(11, 4, 3));\n\njulia> size(sample(chn, 7)) == (7, 4, 1)\ntrue source See  ?sample  for additional help on sampling. Alternatively, you can construct and sample from a kernel density estimator using  KernelDensity.jl , see  test/sampling_tests.jl ."},{"id":248,"pagetitle":"Makie.jl","title":"Makie.jl plots","ref":"/mcmcc/stable/makie/#Makie.jl-plots","content":" Makie.jl plots This page shows an example of plotting MCMCChains.jl with Makie.jl. The example is meant to provide an useful basis to build upon. Let's define some random chain and load the required packages: using MCMCChains\n\nchns = Chains(randn(300, 5, 3), [:A, :B, :C, :D, :E]) Chains MCMC chain (300×5×3 Array{Float64, 3}):\n\nIterations        = 1:1:300\nNumber of chains  = 3\nSamples per chain = 300\nparameters        = A, B, C, D, E\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat\n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64\n\n           A    0.0324    1.0226     0.0341    0.0345    915.3818    1.0016\n           B    0.0080    0.9717     0.0324    0.0315    831.5448    1.0016\n           C    0.0038    1.0124     0.0337    0.0356    868.9161    0.9997\n           D   -0.0481    0.9906     0.0330    0.0319    917.3192    1.0012\n           E    0.0060    0.9734     0.0324    0.0339   1039.1511    1.0003\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n           A   -2.0496   -0.6946    0.0758    0.7028    2.1171\n           B   -1.8296   -0.6719    0.0039    0.6825    1.9137\n           C   -1.9518   -0.6502   -0.0133    0.6851    1.9428\n           D   -2.0363   -0.6882   -0.0158    0.6090    1.8324\n           E   -1.8552   -0.6468    0.0328    0.6693    1.8702\n A basic way to visualize the chains is to show the drawn samples at each iteration. Colors depict different chains. using CairoMakie\nCairoMakie.activate!(; type=\"svg\")\n\nparams = names(chns, :parameters)\n\nn_chains = length(chains(chns))\nn_samples = length(chns)\n\nfig = Figure(; resolution=(1_000, 800))\n\nfor (i, param) in enumerate(params)\n    ax = Axis(fig[i, 1]; ylabel=string(param))\n    for chain in 1:n_chains\n        values = chns[:, param, chain]\n        lines!(ax, 1:n_samples, values; label=string(chain))\n    end\n\n    hideydecorations!(ax; label=false)\n    if i < length(params)\n        hidexdecorations!(ax; grid=false)\n    else\n        ax.xlabel = \"Iteration\"\n    end\nend\n\nfig Next, we can add a second row of plots next to it which show the density estimate for these samples: for (i, param) in enumerate(params)\n    ax = Axis(fig[i, 2]; ylabel=string(param))\n    for chain in 1:n_chains\n        values = chns[:, param, chain]\n        density!(ax, values; label=string(chain))\n    end\n\n    hideydecorations!(ax)\n    if i < length(params)\n        hidexdecorations!(ax; grid=false)\n    else\n        ax.xlabel = \"Parameter estimate\"\n    end\nend\n\naxes = [only(contents(fig[i, 2])) for i in 1:length(params)]\nlinkxaxes!(axes...)\n\nfig Finally, let's add a simple legend. Thanks to setting  label  above, this legend will have the right labels: axislegend(first(axes))\n\nfig"},{"id":251,"pagetitle":"Model selection","title":"Model selection","ref":"/mcmcc/stable/modelstats/#Model-selection","content":" Model selection The methods listed below are defined in  src/modelstats.jl ."},{"id":252,"pagetitle":"Model selection","title":"MCMCChains.dic","ref":"/mcmcc/stable/modelstats/#MCMCChains.dic-Tuple{Chains, Function}","content":" MCMCChains.dic  —  Method dic(chain::Chains, logpdf::Function) -> (DIC, pD) Compute the deviance information criterion (DIC). (Smaller is better) Note: DIC assumes that the posterior distribution is approx. multivariate Gaussian and tends to select overfitted models. Returns: DIC : The calculated deviance information criterion pD : The effective number of parameters Usage: chn ... # sampling results\nlpfun = function f(chain::Chains) # function to compute the logpdf values\n    niter, nparams, nchains = size(chain)\n    lp = zeros(niter + nchains) # resulting logpdf values\n    for i = 1:nparams\n        lp += map(p -> logpdf( ... , x), Array(chain[:,i,:]))\n    end\n    return lp\nend\n\nDIC, pD = dic(chn, lpfun) source"},{"id":256,"pagetitle":"Posterior statistics","title":"Posterior statistics","ref":"/mcmcc/stable/stats/#Posterior-statistics","content":" Posterior statistics The methods listed below are defined in  src/stats.jl ."},{"id":257,"pagetitle":"Posterior statistics","title":"StatsBase.autocor","ref":"/mcmcc/stable/stats/#StatsBase.autocor","content":" StatsBase.autocor  —  Function autocor(\n    chains;\n    append_chains = true,\n    demean = true,\n    [lags,]\n    kwargs...,\n) Compute the autocorrelation of each parameter for the chain. The default  lags  are  [1, 5, 10, 50] , upper-bounded by  n - 1  where  n  is the number of samples used in the estimation. Setting  append_chains=false  will return a vector of dataframes containing the autocorrelations for each chain. source"},{"id":258,"pagetitle":"Posterior statistics","title":"DataAPI.describe","ref":"/mcmcc/stable/stats/#DataAPI.describe","content":" DataAPI.describe  —  Function describe(io, chains[;\n         q = [0.025, 0.25, 0.5, 0.75, 0.975],\n         etype = :bm,\n         kwargs...]) Print the summary statistics and quantiles for the chain. source"},{"id":259,"pagetitle":"Posterior statistics","title":"Statistics.mean","ref":"/mcmcc/stable/stats/#Statistics.mean","content":" Statistics.mean  —  Function mean(chains[, params; kwargs...]) Calculate the mean of a chain. source"},{"id":260,"pagetitle":"Posterior statistics","title":"StatsBase.summarystats","ref":"/mcmcc/stable/stats/#StatsBase.summarystats","content":" StatsBase.summarystats  —  Function function summarystats(\n    chains;\n    sections = _default_sections(chains),\n    append_chains= true,\n    method::AbstractESSMethod = ESSMethod(),\n    maxlag = 250,\n    etype = :bm,\n    kwargs...\n) Compute the mean, standard deviation, naive standard error, Monte Carlo standard error, and effective sample size for each parameter in the chain. Setting  append_chains=false  will return a vector of dataframes containing the summary statistics for each chain. When estimating the effective sample size, autocorrelations are computed for at most  maxlag  lags. source"},{"id":261,"pagetitle":"Posterior statistics","title":"Statistics.quantile","ref":"/mcmcc/stable/stats/#Statistics.quantile","content":" Statistics.quantile  —  Function quantile(chains[; q = [0.025, 0.25, 0.5, 0.75, 0.975], append_chains = true, kwargs...]) Compute the quantiles for each parameter in the chain. Setting  append_chains=false  will return a vector of dataframes containing the quantiles for each chain. source"},{"id":262,"pagetitle":"Posterior statistics","title":"MCMCChains.hpd","ref":"/mcmcc/stable/stats/#MCMCChains.hpd","content":" MCMCChains.hpd  —  Function hpd(chn::Chains; alpha::Real=0.05, kwargs...) Return the highest posterior density interval representing  1-alpha  probability mass. Note that this will return a single interval and will not return multiple intervals for discontinuous regions. Examples julia> val = rand(500, 2, 3);\njulia> chn = Chains(val, [:a, :b]);\n\njulia> hpd(chn)\nHPD\n  parameters     lower     upper \n      Symbol   Float64   Float64 \n\n           a    0.0554    0.9944\n           b    0.0114    0.9460 source"},{"id":265,"pagetitle":"StatsPlots.jl","title":"StatsPlots.jl","ref":"/mcmcc/stable/statsplots/#StatsPlots.jl","content":" StatsPlots.jl MCMCChains implements many functions for plotting via  StatsPlots.jl ."},{"id":266,"pagetitle":"StatsPlots.jl","title":"Simple example","ref":"/mcmcc/stable/statsplots/#Simple-example","content":" Simple example The following simple example illustrates how to use Chain to visually summarize a MCMC simulation: using MCMCChains\nusing StatsPlots\n\n# Define the experiment\nn_iter = 100\nn_name = 3\nn_chain = 2\n\n# experiment results\nval = randn(n_iter, n_name, n_chain) .+ [1, 2, 3]'\nval = hcat(val, rand(1:2, n_iter, 1, n_chain))\n\n# construct a Chains object\nchn = Chains(val, [:A, :B, :C, :D])\n\n# visualize the MCMC simulation results\nplot(chn; size=(840, 600)) plot(chn, colordim = :parameter; size=(840, 400)) Note that the plot function takes the additional arguments described in the  Plots.jl  package."},{"id":267,"pagetitle":"StatsPlots.jl","title":"Mixed density","ref":"/mcmcc/stable/statsplots/#Mixed-density","content":" Mixed density plot(chn, seriestype = :mixeddensity) Or, for all seriestypes, use the alternative shorthand syntax: mixeddensity(chn)"},{"id":268,"pagetitle":"StatsPlots.jl","title":"Trace","ref":"/mcmcc/stable/statsplots/#Trace","content":" Trace plot(chn, seriestype = :traceplot) traceplot(chn)"},{"id":269,"pagetitle":"StatsPlots.jl","title":"Running average","ref":"/mcmcc/stable/statsplots/#Running-average","content":" Running average meanplot(chn)"},{"id":270,"pagetitle":"StatsPlots.jl","title":"Density","ref":"/mcmcc/stable/statsplots/#Density","content":" Density density(chn)"},{"id":271,"pagetitle":"StatsPlots.jl","title":"Histogram","ref":"/mcmcc/stable/statsplots/#Histogram","content":" Histogram histogram(chn)"},{"id":272,"pagetitle":"StatsPlots.jl","title":"Autocorrelation","ref":"/mcmcc/stable/statsplots/#Autocorrelation","content":" Autocorrelation autocorplot(chn)"},{"id":273,"pagetitle":"StatsPlots.jl","title":"Corner","ref":"/mcmcc/stable/statsplots/#Corner","content":" Corner corner(chn)"},{"id":276,"pagetitle":"Summarize","title":"Summarize","ref":"/mcmcc/stable/summarize/#Summarize","content":" Summarize The methods listed below are defined in  src/summarize.jl ."},{"id":277,"pagetitle":"Summarize","title":"MCMCChains.summarize","ref":"/mcmcc/stable/summarize/#MCMCChains.summarize-Tuple{Chains, Vararg{Any}}","content":" MCMCChains.summarize  —  Method summarize(chains, funs...[; sections, func_names = []]) Summarize  chains  in a  ChainsDataFrame . Examples summarize(chns)  : Complete chain summary summarize(chns[[:parm1, :parm2]])  : Chain summary of selected parameters summarize(chns; sections=[:parameters])   : Chain summary of :parameters section summarize(chns; sections=[:parameters, :internals])  : Chain summary for multiple sections source"}]
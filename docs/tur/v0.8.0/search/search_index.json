{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location": "/archive/", "text": "news archive2019 dec 14 2019 turing s blog", "title": "Articles"},{"location": "/feed.xml", "text": "turing jl turing a robust efficient and modular library for general purpose probabilistic programming refs tags v0 8 0 sun 05 jan 2020 06 22 46 0000 sun 05 jan 2020 06 22 46 0000 jekyll v3 8 5 turing s blog lt p gt all good open source projects should have a blog and turing is one such project later on members of the turing team may be populating this feed with posts on topics like lt p gt lt ul gt lt li gt interesting things you can do with turing or interesting things we have seen others do lt li gt lt li gt development updates and major release announcements lt li gt lt li gt research updates lt li gt lt li gt explorations of turing s internals lt li gt lt li gt updates to turing s sattelite projects lt a href quot https github com turinglang advancedhmc jl quot gt advancedhmc jl lt a gt or lt a href quot https github com turinglang bijectors jl quot gt bijectors jl lt a gt lt li gt lt ul gt lt p gt stay tuned lt p gt sat 14 dec 2019 00 00 00 0000 refs tags v0 8 0 posts 2019 12 14 initial post refs tags v0 8 0 posts 2019 12 14 initial post", "title": ""},{"location": "/", "text": "turing jl a robust efficient and modular library for general purpose probabilistic ai intuitive turing models are easy to read and write models work the way you write them general purpose turing supports models with discrete parameters and stochastic control flow specify complex models quickly and easily modular turing is modular written fully in julia and can be modified to suit your needs hello world in turing linear gaussian model turing s modelling syntax allows you to specify a model quickly and easily straightforward models can be expressed in the same way as complex hierarchical models with stochastic control flow quick start model gdemo x y begin assumptions inversegamma 2 3 normal 0 sqrt observations x normal sqrt y normal sqrt end news feed turing s blog december 14 2019 news advanced markov chain monte carlo samplers turing provides hamiltonian monte carlo sampling for differentiable posterior distributions particle mcmc sampling for complex posterior distributions involving discrete variables and stochastic control flow and gibbs sampling which combines particle mcmc hmc and many other mcmc algorithms samplers interoperable with deep learning libraries turing supports julia s flux package for automatic differentiation combine turing and flux to construct probabalistic variants of traditional machine learning models bayesian neural network tutorial community join the turing community to contribute learn and get your questions answered github report bugs request features discuss issues and more go to github turing jl discuss browse and join discussions on turing go to turing jl discuss slack discuss advanced topics request access here go to slack ecosystem explore a rich ecosystem of libraries tools and more to support development advancedhmc robust modular and efficient implementation of advanced hamiltonian monte carlo algorithms go to advancedhmc mcmcchains chain types and utility functions for mcmc simulations go to mcmcchains bijectors automatic transformations for constrained random variables go to bijectors", "title": "Turing.jl - Turing.jl"},{"location": "/news/", "text": "newssubscribe with rss to keep up with the latest news about turing turing s blog december 14 2019 all good open source projects should have a blog and turing is one such project later on members of the turing team may be populating this feed with posts on topics like read more want to see more see the news archive", "title": "News"},{"location": "/search/search_index.json", "text": "config lang en prebuild index false separator s docs for page in site pages unless page excluded in search if added endif assign added false location page url text page content strip html strip newlines slugify ascii replace title page title assign added true endunless endfor for post in site posts unless page excluded in search if added endif assign added false location post url text post content strip html strip newlines slugify ascii replace title post title assign added true endunless endfor for doc in site docs unless doc excluded in search if added endif assign added false location doc url text doc content strip html strip newlines slugify ascii replace title doc title assign added true endunless endfor", "title": ""},{"location": "/sitemap.xml", "text": "now date y m d daily for section in site data toc site baseurl section url now date y m d daily endfor", "title": ""},{"location": "/assets/css/style.css", "text": "import jekyll theme primer", "title": ""},{"location": "/robots.txt", "text": "sitemap sitemap xml absolute url", "title": ""},{"location": "/feed.xml", "text": "if page xsl endif jekyll site time date to xmlschema page url absolute url xml escape assign title site title default site name if page collection posts assign collection page collection capitalize assign title title append append collection endif if page category assign category page category capitalize assign title title append append category endif if title title smartify xml escape endif if site description site description xml escape endif if site author site author name default site author xml escape if site author email site author email xml escape endif if site author uri site author uri xml escape endif endif assign posts site page collection where exp post post draft true sort date reverse if page category assign posts posts where category page category endif for post in posts limit 10 post title smartify strip html normalize whitespace xml escape post date date to xmlschema post last modified at default post date date to xmlschema post id absolute url xml escape assign excerpt only post feed excerpt only default site feed excerpt only unless excerpt only post content strip xml escape endunless assign post author post author default post authors 0 default site author assign post author site data authors post author default post author assign post author email post author email default nil assign post author uri post author uri default nil assign post author name post author name default post author post author name default xml escape if post author email post author email xml escape endif if post author uri post author uri xml escape endif if post category endif for tag in post tags endfor if post excerpt and post excerpt empty post excerpt strip html normalize whitespace xml escape endif assign post image post image path default post image if post image unless post image contains assign post image post image absolute url endunless endif endfor", "title": ""},{"location": "/posts/2019-12-14-initial-post", "text": "all good open source projects should have a blog and turing is one such project later on members of the turing team may be populating this feed with posts on topics like interesting things you can do with turing or interesting things we have seen others do development updates and major release announcements research updates explorations of turing s internals updates to turing s sattelite projects advancedhmc jl or bijectors jl stay tuned", "title": "Turing's Blog"},{"location": "/docs/contributing/guide", "text": "contributingturing is an open source project if you feel that you have some relevant skills and are interested in contributing then please do get in touch you can contribute by opening issues on github or implementing things yourself and making a pull request we would also appreciate example models written using turing turing has a style guide it is not strictly necessary to review it before making a pull request but you may be asked to change portions of your code to conform with the style guide before it is merged how to contributegetting started fork this repository clone your fork on your local machine git clone https github com your username turing jl add a remote corresponding to this repository git remote add upstream https github com turinglang turing jl what can i do look at the issues page to find an outstanding issue for instance you could implement new features fix bugs or write example models git workflowfor more information on how the git workflow typically functions please see the github s introduction or julia s contribution guide", "title": "Contributing"},{"location": "/docs/contributing/style-guide", "text": "style guidethis style guide is adapted from invenia s style guide we would like to thank them for allowing us to access and use it please don t let not having read it stop you from contributing to turing no one will be annoyed if you open a pr whose style doesn t follow these conventions we will just help you correct it before it gets merged these conventions were originally written at invenia taking inspiration from a variety of sources including python s pep8 julia s notes for contributors and julia s style guide what follows is a mixture of a verbatim copy of invenia s original guide and some of our own modifications a word on consistencywhen adhering to this style it s important to realize that these are guidelines and not rules this is stated best in the pep8 a style guide is about consistency consistency with this style guide is important consistency within a project is more important consistency within one module or function is most important but most importantly know when to be inconsistent sometimes the style guide just doesn t apply when in doubt use your best judgment look at other examples and decide what looks best and don t hesitate to ask synopsisattempt to follow both the julia contribution guidelines the julia style guide and this guide when convention guidelines conflict this guide takes precedence known conflicts will be noted in this guide use 4 spaces per indentation level no tabs try to adhere to a 92 character line length limit use upper camel case convention for modules and types use lower case with underscores for method names note julia code likes to use lower case without underscores comments are good try to explain the intentions of the code use whitespace to make the code more readable no whitespace at the end of a line trailing whitespace avoid padding brackets with spaces ex int64 value preferred over int64 value editor configurationsublime text settingsif you are a user of sublime text we recommend that you have the following options in your julia syntax specific settings to modify these settings first open any julia file jl in sublime text then navigate to preferences gt settings more gt syntax specific user translate tabs to spaces true tab size 4 trim trailing white space on save true ensure newline at eof on save true rulers 92 vim settingsif you are a user of vim we recommend that you add the following options to your vimrc file set tabstop 4 sets tabstops to a width of four columns set softtabstop 4 determines the behaviour of tab and backspace keys with expandtab set shiftwidth 4 determines the results of gt gt lt lt and au filetype julia setlocal expandtab replaces tabs with spaces au filetype julia setlocal colorcolumn 93 highlights column 93 to help maintain the 92 character line limit by default vim seems to guess that jl files are written in lisp to ensure that vim recognizes julia files you can manually have it check for the jl extension but a better solution is to install julia vim which also includes proper syntax highlighting and a few cool other features atom settingsatom defaults preferred line length to 80 characters we want that at 92 for julia to change it go to atom gt preferences gt packages search for the language julia package and open the settings for it find preferred line length under julia grammar and change it to 92 code formattingfunction namingnames of functions should describe an action or property irrespective of the type of the argument the argument s type provides this information instead for example buyfood food should be buy food food names of functions should usually be limited to one or two lowercase words ideally write buyfood not buy food but if you are writing a function whose name is hard to read without underscores then please do use them method definitionsonly use short form function definitions when they fit on a single line yes foo x int64 abs x 3 no foobar array data abstractarray t item t where t lt int64 t abs x abs item 3 for x in array data no foobar array data abstractarray t item t where t lt int64 t abs x abs item 3 for x in array data yes function foobar array data abstractarray t item t where t lt int64 return t abs x abs item 3 for x in array data endwhen using long form functions always use the return keyword yes function fnc x result zero x result fna x return resultend no function fnc x result zero x result fna x end yes function foo x y return new x y end no function foo x y new x y endfunctions definitions with parameter lines which exceed 92 characters should separate each parameter by a newline and indent by one level yes function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend ok function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend no function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend no function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeendkeyword argumentswhen calling a function always separate your keyword arguments from your positional arguments with a semicolon this avoids mistakes in ambiguous cases such as splatting a dict yes xy foo x y 3 no xy foo x y 3 whitespaceavoid extraneous whitespace in the following situations immediately inside parentheses square brackets or braces yes spam ham 1 eggs no spam ham 1 eggs immediately before a comma or semicolon yes if x 4 show x y x y y x endno if x 4 show x y x y y x end when using ranges unless additional operators are used yes ham 1 9 ham 1 3 9 ham 1 3 end no ham 1 9 ham 1 3 9 yes ham lower upper ham lower step upper yes ham lower offset upper offset yes ham lower offset upper offset no ham lower offset upper offset more than one space around an assignment or other operator to align it with another yes x 1y 2long variable 3 no x 1y 2long variable 3 when using parametric types yes f a abstractarray t n where t lt real n g a abstractarray lt real n where n no f a abstractarray t n where t lt real n g a abstractarray lt real n where n always surround these binary operators with a single space on either side assignment updating operators etc numeric comparisons operators lt gt etc note that this guideline does not apply when performing assignment in method definitions yes i i 1no i i 1yes submitted 1no submitted 1yes x 2 lt yno x 2 lt y assignments using expanded array tuple or function notation should have the first open bracket on the same line assignment operator and the closing bracket should match the indentation level of the assignment alternatively you can perform assignments on a single line when they are short yes arr 1 2 3 arr 1 2 3 result function arg1 arg2 arr 1 2 3 no arr 1 2 3 arr 1 2 3 arr 1 2 3 nested array or tuples that are in expanded notation should have the opening and closing brackets at the same indentation level yes x 1 2 3 hello world a b c no y 1 2 3 hello world z 1 2 3 hello world always include the trailing comma when working with expanded arrays tuples or functions notation this allows future edits to easily move elements around or add additional elements the trailing comma should be excluded when the notation is only on a single line yes arr 1 2 3 result function arg1 arg2 arr 1 2 3 no arr 1 2 3 result function arg1 arg2 arr 1 2 3 triple quotes use the indentation of the lowest indented line excluding the opening triple quote this means the closing triple quote should be aligned to least indented line in the string triple backticks should also follow this style even though the indentation does not matter for them yes str hello world str hello world cmd program flag value parameter no str hello world commentscomments should be used to state the intended behaviour of code this is especially important when the code is doing something clever that may not be obvious upon first inspection avoid writing comments that state exactly what the code obviously does yes x x 1 compensate for border no x x 1 increment xcomments that contradict the code are much worse than no comments always make a priority of keeping the comments up to date with code changes comments should be complete sentences if a comment is a phrase or sentence its first word should be capitalized unless it is an identifier that begins with a lower case letter never alter the case of identifiers if a comment is short the period at the end can be omitted block comments generally consist of one or more paragraphs built out of complete sentences and each sentence should end in a period comments should be separated by at least two spaces from the expression and have a single space after the when referencing julia in documentation note that julia refers to the programming language while julia typically in backticks e g julia refers to the executable a commmentcode another commentmore codetododocumentationit is recommended that most modules types and functions should have docstrings that being said only exported functions are required to be documented avoid documenting methods like as the built in docstring for the function already covers the details well try to document a function and not individual methods where possible as typically all methods will have similar docstrings if you are adding a method to a function which was defined in base or another package only add a docstring if the behaviour of your function deviates from the existing docstring docstrings are written in markdown and should be concise docstring lines should be wrapped at 92 characters bar x y compute the bar index between x and y if y is missing compute the bar index betweenall pairs of columns of x function bar x y when types or methods have lots of parameters it may not be feasible to write a concise docstring in these cases it is recommended you use the templates below note if a section doesn t apply or is overly verbose for example throws if your function doesn t throw an exception it can be excluded it is recommended that you have a blank line between the headings and the content when the content is of sufficient length try to be consistent within a docstring whether you use this additional whitespace note that the additional space is only for reading raw markdown and does not effect the rendered version type template should be skipped if is redundant with the constructor s docstring myarray t n my super awesome array wrapper fields data abstractarray t n stores the array being wrapped metadata dict stores metadata about the array struct myarray t n lt abstractarray t n data abstractarray t n metadata dictendfunction template only required for exported functions mysearch array myarray t val t verbose true where t gt intsearches the array for the val for some reason we don t want to use julia sbuiltin search arguments array myarray t the array to search val t the value to search for keywords verbose bool true print out progress details returns int the index where val is located in the array throws notfounderror i guess we could throw an error if val isn t found function mysearch array abstractarray t val t where t endif your method contains lots of arguments or keywords you may want to exclude them from the method signature on the first line and instead use args and or kwargs manager args kwargs gt managera cluster manager which spawns workers arguments min workers integer the minimum number of workers to spawn or an exception is thrown max workers integer the requested number of worker to spawn keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance name abstractstring queue abstractstring function manager endfeel free to document multiple methods for a function within the same docstring be careful to only do this for functions you have defined manager max workers kwargs manager min workers max workers kwargs manager min workers max workers kwargs a cluster manager which spawns workers arguments min workers int the minimum number of workers to spawn or an exception is thrown max workers int the number of requested workers to spawn keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance name abstractstring queue abstractstring function manager endif the documentation for bullet point exceeds 92 characters the line should be wrapped and slightly indented avoid aligning the text to the keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance for additional details on documenting in julia see the official documentation test formattingtestsetsjulia provides test sets which allows developers to group tests into logical groupings test sets can be nested and ideally packages should only have a single root test set it is recommended that the runtests jl file contains the root test set which contains the remainder of the tests testset pkgextreme begin include arithmetic jl include utils jl endthe file structure of the test folder should mirror that of the src folder every file in src should have a complementary file in the test folder containing tests relevant to that file s contents comparisonsmost tests are written in the form test x y since the function doesn t take types into account tests like the following are valid test 1 0 1 avoid adding visual noise into test comparisons yes test value 0 no test value 0 0in cases where you are checking the numerical validity of a model s parameter estimates please use the check numerical function found in test test utils numerical tests jl this function will evaluate a model s parameter estimates using tolerance levels atol and rtol testing will only be performed if you are running the test suite locally or if travis is executing the numerical testing stage here is an example of usage check that m and s are plus or minus one from 1 5 and 2 2 respectively check numerical chain m s 1 5 2 2 atol 1 0 checks the estimates for a default gdemo model using values 1 5 and 2 0 check gdemo chain atol 0 1 checks the estimates for a default mog model check mogtest default chain atol 0 1", "title": "Style Guide"},{"location": "/docs/library/advancedhmc/", "text": "index advancedhmc abstractproposal advancedhmc abstracttrajectory advancedhmc abstracttrajectorysampler advancedhmc binarytree advancedhmc multinomialts advancedhmc multinomialts advancedhmc multinomialts advancedhmc nuts advancedhmc nuts advancedhmc nuts advancedhmc slicets advancedhmc slicets advancedhmc slicets advancedhmc termination advancedhmc termination advancedhmc termination advancedhmc transition advancedhmc a advancedhmc build tree advancedhmc combine advancedhmc find good eps advancedhmc isterminated advancedhmc isterminated advancedhmc maxabs advancedhmc mh accept ratio advancedhmc nom step size advancedhmc pm next advancedhmc simple pm next advancedhmc step size advancedhmc temper advancedhmc transitionfunctions advancedhmc find good eps method find a good initial leap frog step size via heuristic search statsbase sample method sample rng abstractrng h hamiltonian abstractproposal abstractvecormat t n samples int adaptor adaptation abstractadaptor adaptation noadaptation n adapts int min div n samples 10 1 000 drop warmup bool false verbose bool true progress bool false sample n samples samples using the proposal under hamiltonian h the randomness is controlled by rng if rng is not provided global rng will be used the initial point is given by the adaptor is set by adaptor for which the default is no adaptation it will perform n adapts steps of adaptation for which the default is the minimum of 1 000 and 10 of n samples drop warmup controls to drop the samples during adaptation phase or not verbose controls the verbosity progress controls whether to show the progress meter or not advancedhmc a method a single hamiltonian integration step note this function is intended to be used in find good eps only advancedhmc build tree method recursivly build a tree for a given depth j advancedhmc combine method combine treeleft binarytree treeright binarytree merge a left tree treeleft and a right tree treeright under given hamiltonian h then draw a new candidate sample and update related statistics for the resulting tree advancedhmc isterminated method detect u turn for two phase points zleft and zright under given hamiltonian h using the original no u turn cirterion ref https arxiv org abs 1111 4246 https arxiv org abs 1701 02434 advancedhmc isterminated method detect u turn for two phase points zleft and zright under given hamiltonian h using the generalised no u turn criterion ref https arxiv org abs 1701 02434 advancedhmc maxabs method maxabs a b return the value with the largest absolute value advancedhmc mh accept ratio method perform mh acceptance based on energy i e negative log probability advancedhmc nom step size method nom step size abstractintegrator get the nominal integration step size the current integration step size may differ from this for example if the step size is jittered nominal step size is usually used in adaptation advancedhmc pm next method progress meter update with all trajectory stats iteration number and metric shown advancedhmc simple pm next method simple progress meter update without any show values advancedhmc step size function step size abstractintegrator get the current integration step size advancedhmc temper method temper lf temperedleapfrog r step namedtuple i is half lt tuple integer bool n steps int tempering step step is a named tuple with i being the current leapfrog iteration and is half indicating whether or not it s the first half momentum tempering step advancedhmc transition method transition abstracttrajectory i h hamiltonian z phasepoint make a mcmc transition from phase point z using the trajectory under hamiltonian h note this is a rng implicit callback function for transition global rng h z types advancedhmc multinomialts type multinomial sampler carried during the building of the tree it contains the weight of the tree defined as the total probabilities of the leaves advancedhmc multinomialts method multinomial sampler for a trajectory consisting only a leaf node tree weight is the unnormalised energy of the leaf advancedhmc multinomialts method multinomial sampler for the starting single leaf tree log weights for leaf nodes are their unnormalised hamiltonian energies ref https github com stan dev stan blob develop src stan mcmc hmc nuts base nuts hpp l226 advancedhmc nuts type dynamic trajectory hmc using the no u turn termination criteria algorithm advancedhmc nuts method nuts args nuts multinomialts generalisednouturn args create an instance for the no u turn sampling algorithm with multinomial sampling and original no u turn criterion below is the doc for nuts s c nuts s c integrator i max depth int 10 max f 1000 0 where i lt abstractintegrator f lt abstractfloat s lt abstracttrajectorysampler c lt abstractterminationcriterion create an instance for the no u turn sampling algorithm advancedhmc nuts method nuts s c integrator i max depth int 10 max f 1000 0 where i lt abstractintegrator f lt abstractfloat s lt abstracttrajectorysampler c lt abstractterminationcriterion create an instance for the no u turn sampling algorithm advancedhmc slicets type slice sampler carried during the building of the tree it contains the slice variable and the number of acceptable condidates in the tree advancedhmc slicets method slice sampler for the starting single leaf tree slice variable is initialized advancedhmc slicets method create a slice sampler for a single leaf tree the slice variable is copied from the passed in sampler s and the number of acceptable candicates is computed by comparing the slice variable against the current energy advancedhmc abstractproposal type abstract markov chain monte carlo proposal advancedhmc abstracttrajectory type hamiltonian dynamics numerical simulation trajectories advancedhmc abstracttrajectorysampler type sampler carried during the building of the tree advancedhmc binarytree type a full binary tree trajectory with only necessary leaves and information stored advancedhmc termination type termination reasons dynamic due to stoping criteria numerical due to large energy deviation from starting possibly numerical errors advancedhmc termination method check termination of a hamiltonian trajectory advancedhmc termination method check termination of a hamiltonian trajectory advancedhmc transition type a transition that contains the phase point and other statistics of the transition", "title": "AdvancedHMC"},{"location": "/docs/library/", "text": "index turing binomiallogit turing flat turing flatpos turing orderedlogistic turing sampler turing vecbinomiallogit turing core model turing inference gibbs turing inference hmc turing inference hmcda turing inference is turing inference mh turing inference nuts turing inference pg turing inference smc libtask tarray libtask tzerosmodelling turing core model macro model body macro to specify a probabilistic model example model definition model model generator x default x y begin endexpanded model definition allows passing arguments as kwargsmodel generator x nothing y nothing model generator x y function model generator x nothing y nothing pvars dvars turing get vars tuple x y x x y y data turing get data dvars x x y y defaults turing get default values dvars x default x y nothing inner function sampler turing abstractsampler model inner function model function inner function model return inner function turing varinfo turing samplefromprior model end function inner function vi turing varinfo model return inner function vi turing samplefromprior model end define the main inner function function inner function vi turing varinfo sampler turing abstractsampler model local x if isdefined model data x if model data x isa type amp amp model data x lt abstractfloat model data x lt abstractarray x turing core get matching type sampler vi model data x else x model data x end else x model defaults x end local y if isdefined model data y if model data y isa type amp amp model data y lt abstractfloat model data y lt abstractarray y turing core get matching type sampler vi model data y else y model data y end else y model defaults y end vi logp zero real end model turing model pvars dvars inner function data defaults return modelendgenerating a model model generator x value model samplers turing sampler type sampler t generic interface for implementing inference algorithms an implementation of an algorithm should include the following a type specifying the algorithm and its parameters derived from inferencealgorithm a method of sample function that produces results of inference which is where actual inference happens turing translates models to chunks that call the modelling functions at specified points the dispatch is based on the value of a sampler variable to include a new inference algorithm implements the requirements mentioned above in a separate file then include that file at the end of this one turing inference gibbs type gibbs algs compositional mcmc interface gibbs sampling combines one or more sampling algorithms each of which samples from a different set of variables in a model example model gibbs example x begin v1 normal 0 1 v2 categorical 5 enduse pg for a v2 variable and use hmc for the v1 variable note that v2 is discrete so the pg sampler is more appropriatethan is hmc alg gibbs 1000 hmc 1 0 2 3 v1 pg 20 1 v2 tips hmc and nuts are fast samplers and can throw off particle basedmethods like particle gibbs you can increase the effectiveness of particle sampling by including more particles in the particle sampler turing inference hmc type hmc float64 n leapfrog int hamiltonian monte carlo sampler with static trajectory arguments float64 the leapfrog step size to use n leapfrog int the number of leapfrop steps to use usage hmc 0 05 10 tips if you are receiving gradient errors when using hmc try reducing thestep size parameter e g original step sizesample gdemo 1 5 2 hmc 1000 0 1 10 reduced step size sample gdemo 1 5 2 hmc 1000 0 01 10 turing inference hmcda type hmcda n adapts int float64 float64 float64 0 0 hamiltonian monte carlo sampler with dual averaging algorithm usage hmcda 200 0 65 0 3 arguments n adapts int numbers of samples to use for adaptation float64 target acceptance rate 65 is often recommended float64 target leapfrop length float64 0 0 inital step size 0 means automatically search by turing for more information please view the following paper arxiv link hoffman matthew d and andrew gelman the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo journal of machine learning research 15 no 1 2014 1593 1623 turing inference is type is importance sampling algorithm note that this method is particle based and arrays of variables must be stored in a tarray object usage is example define a simple normal model with unknown mean and variance model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s x 1 normal m sqrt s x 2 normal m sqrt s return s mendsample gdemo 1 5 2 is 1000 turing inference mh type mh n iters int metropolis hastings sampler usage mh 100 m x gt normal x 0 1 example define a simple normal model with unknown mean and variance model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s x 1 normal m sqrt s x 2 normal m sqrt s return s mendchn sample gdemo 1 5 2 mh 1000 turing inference nuts type nuts n adapts int float64 max depth int 5 max float64 1000 0 float64 0 0 no u turn sampler nuts sampler usage nuts 200 0 6j max arguments n adapts int the number of samples to use with adapatation float64 target acceptance rate max depth float64 maximum doubling tree depth max float64 maximum divergence during doubling tree float64 inital step size 0 means automatically search by turing turing inference pg type pg n particles int particle gibbs sampler note that this method is particle based and arrays of variables must be stored in a tarray object usage pg 100 100 turing inference smc type smc sequential monte carlo sampler note that this method is particle based and arrays of variables must be stored in a tarray object fields resampler a function used to sample particles from the particle container defaults to resample systematic resampler threshold the threshold at which resampling terminates defaults to 0 5 if the ess lt resampler threshold n particles the resampling step is completed usage smc distributions turing flat type flat lt continuousunivariatedistributiona distribution with support and density of one everywhere turing flatpos type flatpos l real a distribution with a lower bound of l and a density of one at every x above l turing binomiallogit type binomiallogit n lt real i lt integer a univariate binomial logit distribution turing vecbinomiallogit type binomiallogit n lt real i lt integer a multivariate binomial logit distribution turing orderedlogistic type orderedlogistic any cutpoints lt abstractvector an ordered logistic distribution data structures libtask tarray type tarray t dims implementation of data structures that automatically perform copy on write after task copying if current task is an existing key in s then return s current task otherwise returns current task s last task usage tarray dim example ta tarray 4 initfor i in 1 4 ta i i end assignarray ta convert to 4 element array int64 1 1 2 3 4 utilities libtask tzeros function tzeros dims construct a distributed array of zeros trailing arguments are the same as those accepted by tarray tzeros dim example tz tzeros 4 constructarray tz convert to 4 element array int64 1 0 0 0 0", "title": "API"},{"location": "/docs/library/bijectors/", "text": "index bijectors adbijector bijectors abstractbijector bijectors bijector bijectors composed bijectors distributionbijector bijectors inversed bijectors stacked bijectors bijector bijectors composel bijectors composer bijectors forward bijectors forward bijectors isclosedform bijectors logabsdetjac bijectors logabsdetjac bijectors logabsdetjacinv bijectors logabsdetjacinv bijectors logpdf with jac bijectors transformedfunctions bijectors bijector method bijector d distribution returns the constrained to unconstrained bijector for distribution d bijectors forward method forward b bijector x forward ib inversed lt bijector y computes both transform and logabsdetjac in one forward pass and returns a named tuple rv b x logabsdetjac logabsdetjac b x this defaults to the call above but often one can re use computation in the computation of the forward pass and the computation of the logabsdetjac forward allows the user to take advantange of such efficiencies if they exist bijectors forward method forward d distribution forward d distribution num samples int returns a namedtuple with fields x y logabsdetjac and logpdf in the case where d isa transformeddistribution this means x rand d dist y d transform x logabsdetjac is the logabsdetjac of the forward transform logpdf is the logpdf of y not xin the case where d isa distribution this means x rand d y x logabsdetjac 0 0 logpdf is logpdf of x bijectors isclosedform method isclosedform b bijector boolisclosedform b inversed lt bijector boolreturns true or false depending on whether or not evaluation of b has a closed form implementation most bijectors have closed form evaluations but there are cases where this is not the case for example the inverse evaluation of planarlayer requires an iterative procedure to evaluate and thus is not differentiable bijectors logabsdetjac method computes the absolute determinant of the jacobian of the inverse transformation bijectors logabsdetjac method logabsdetjac b bijector x logabsdetjac ib inversed lt bijector y computes the log abs det j b x where j is the jacobian of the transform similarily for the inverse transform default implementation for inversed lt bijector is implemented as logabsdetjac of original bijector bijectors logabsdetjacinv method logabsdetjacinv b bijector y just an alias for logabsdetjac inv b y bijectors logabsdetjacinv method logabsdetjacinv td univariatetransformed y real logabsdetjacinv td multivariatetransformed y abstractvector lt real computes the logabsdetjac of the inverse transformation since rand td returns the transformed random variable bijectors logpdf with jac method logpdf with jac td univariatetransformed y real logpdf with jac td mvtransformed y abstractvector lt real logpdf with jac td matrixtransformed y abstractmatrix lt real makes use of the forward method to potentially re use computation and returns a tuple logpdf logabsdetjac bijectors transformed method transformed d distribution transformed d distribution b bijector couples distribution d with the bijector b by returning a transformeddistribution if no bijector is provided i e transformed d is called then transformed d bijector d is returned bijectors composel method composel ts bijector composed lt tuple constructs composed such that ts are applied left to right bijectors composer method composer ts bijector composed lt tuple constructs composed such that ts are applied right to left types bijectors adbijector type abstract type for a bijector n making use of auto differentation ad to implement jacobian and by impliciation logabsdetjac bijectors bijector type abstract type of bijectors with fixed dimensionality bijectors composed type composed ts a b1 bijector n b2 bijector n composed lt tuple composel ts bijector n composed lt tuple composer ts bijector n composed lt tuple where a refers to either tuple vararg lt bijector n a tuple of bijectors of dimensionality n abstractarray lt bijector n an array of bijectors of dimensionality na bijector representing composition of bijectors composel and composer results in a composed for which application occurs from left to right and right to left respectively note that all the alternative ways of constructing a composed returns a tuple of bijectors this ensures type stability of implementations of all relating methdos e g inv if you want to use an array as the container instead you can docomposed b1 b2 in general this is not advised since you lose type stability but there might be cases where this is desired e g if you have a insanely large number of bijectors to compose examplessimple examplelet s consider a simple example of exp julia gt using bijectors expjulia gt b exp exp 0 julia gt b bcomposed tuple exp 0 exp 0 0 exp 0 exp 0 julia gt b b 1 0 exp exp 1 0 evaluationtruejulia gt inv b b exp exp 1 0 1 0 inversiontruejulia gt logabsdetjac b b 1 0 determinant of jacobian3 718281828459045notesorderit s important to note that does what is expected mathematically which means that the bijectors are applied to the input right to left e g first applying b2 and then b1 b1 b2 x b1 b2 x gt truebut in the composed struct itself we store the bijectors left to right so thatcb1 b1 b2 gt composed ts b2 b1 cb2 composel b2 b1 gt composed ts b2 b1 cb1 x cb2 x b1 b2 x gt truestructure will result in flatten the composition structure while composel and composer preserve the compositional structure this is most easily seen by an example julia gt b exp exp 0 julia gt cb1 b b cb2 b b julia gt cb1 cb2 ts lt different exp 0 exp 0 exp 0 exp 0 julia gt cb1 cb2 ts isa ntuple 4 exp 0 truejulia gt bijectors composer cb1 cb2 ts composed tuple exp 0 exp 0 0 exp 0 exp 0 composed tuple exp 0 exp 0 0 exp 0 exp 0 julia gt bijectors composer cb1 cb2 ts isa tuple composed composed true bijectors distributionbijector type distributionbijector d distribution distributionbijector lt adbackend d d distribution this is the default bijector for a distribution it uses link and invlink to compute the transformations and ad to compute the jacobian and logabsdetjac bijectors inversed type inv b bijector inversed b bijector a bijector representing the inverse transform of b bijectors stacked type stacked bs stacked bs ranges stack bs bijector 0 where 0 means 0 dim bijector a bijector which stacks bijectors together which can then be applied to a vector where bs i bijector is applied to x ranges i unitrange int arguments bs can be either a tuple or an abstractarray of 0 and or 1 dimensional bijectors if bs is a tuple implementations are type stable using generated functions if bs is an abstractarray implementations are not type stable and use iterative methods ranges needs to be an iterable consisting of unitrange int length bs length ranges needs to be true examplesb1 logit 0 0 1 0 b2 identity 0 b stack b1 b2 b 0 0 1 0 b1 0 0 1 0 gt true bijectors abstractbijector type abstract type for a bijector", "title": "Bijectors"},{"location": "/docs/tutorials/index", "text": "tutorialsthis section contains tutorials on how to implement common models in turing if you prefer to have an interactive jupyter notebook please fork or download the turingtutorials repository a list of all the tutorials available can be found to the left the introduction tutorial contains an introduction to coin flipping with turing and a brief overview of probabalistic programming tutorials are under continuous development but there are some older version available at the turingtutorials within the old notebooks section some of these were built using prior versions of turing and may not function correctly but they can assist in the syntax used for common models if there is a tutorial you would like to request please open an issue on the turingtutorials repository", "title": "Tutorials"},{"location": "/docs/using-turing/advanced", "text": "advanced usagehow to define a customized distributionturing jl supports the use of distributions from the distributions jl package by extension it also supports the use of customized distributions by defining them as subtypes of distribution type of the distributions jl package as well as corresponding functions below shows a workflow of how to define a customized distribution using a flat prior as a simple example 1 define the distribution typefirst define a type of the distribution as a subtype of a corresponding distribution type in the distributions jl package struct flat lt continuousunivariatedistribution end2 implement sampling and evaluation of the log pdfsecond define rand and logpdf which will be used to run the model distributions rand rng abstractrng d flat rand rng distributions logpdf d flat x real zero x 3 define helper functionsin most cases it may be required to define helper functions such as the minimum maximum rand and logpdf functions among others 3 1 domain transformationsome helper functions are necessary for domain transformation for univariate distributions the necessary ones to implement are minimum and maximum distributions minimum d flat infdistributions maximum d flat inffunctions for domain transformation which may be required by multivariate or matrix variate distributions are size link and invlink please see turing s transform jl for examples 3 2 vectorization supportthe vectorization syntax follows rv distribution which requires rand and logpdf to be called on multiple data points at once an appropriate implementation for flat is shown below distributions logpdf d flat x abstractvector lt real zero x model internalsthe model macro accepts a function definition and generates a turing model struct for use by the sampler models can be constructed by hand without the use of a macro taking the gdemo model as an example the two code sections below macro and macro free are equivalent using turing model gdemo x begin set priors s inversegamma 2 3 m normal 0 sqrt s observe each value of x x normal m sqrt s endsample gdemo 1 5 2 0 hmc 0 1 5 1000 using turing initialize a namedtuple containing our data variables data x 1 5 2 0 create the model function mf vi sampler ctx model begin set the accumulated logp to zero vi logp 0 x model args x assume s has an inversegamma distribution s lp turing inference tilde ctx sampler inversegamma 2 3 turing varname s vi add the lp to the accumulated logp vi logp lp assume m has a normal distribution m lp turing inference tilde ctx sampler normal 0 sqrt s turing varname m vi add the lp to the accumulated logp vi logp lp observe each value of x i according to a normal distribution vi logp turing inference dot tilde ctx sampler normal m sqrt s x vi end instantiate a model object model turing model mf data turing core modelgen nothing nothing sample the model chain sample model hmc 0 1 5 1000 task copyingturing copies julia tasks to deliver efficient inference algorithms but it also provides alternative slower implementation as a fallback task copying is enabled by default task copying requires we use the ctask facility which is provided by libtask to create tasks maximum a posteriori estimationturing does not currently have built in methods for calculating the maximum a posteriori map for a model this is a goal for turing s implementation see this issue but for the moment we present here a method for estimating the map using optim jl using turing define the simple gdemo model model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s return s mendfunction get nlogp model construct a trace struct vi turing varinfo model define a function to optimize function nlogp sm spl turing samplefromprior new vi turing varinfo vi spl sm model new vi spl new vi logp end return nlogpend define our data points x 1 5y 2 0model gdemo x y nlogp get nlogp model import optim jl using optim create a starting point call the optimizer sm 0 1 0 1 0 lb 0 0 inf ub inf inf result optimize nlogp lb ub sm 0 fminbox parallel samplingturing does not natively support parallel sampling currently users must perform additional structual support note that running chains in parallel may cause unintended issues below is an example of how to run samplers in parallel note that each process must be given a separate seed otherwise the samples generated by independent processes will be equivalent and unhelpful to inference using distributedaddprocs 4 everywhere using turing random set the progress to false to avoid too many notifications everywhere turnprogress false set a different seed for each process for i in procs fetch spawnat i random seed rand int64 end define the model using everywhere everywhere model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend sampling setup num chains 4sampler nuts 0 65 model gdemo 1 2 3 5 run all samples chns reduce chainscat pmap x gt sample model sampler 1000 1 num chains", "title": "Advanced Usage"},{"location": "/docs/using-turing/autodiff", "text": "automatic differentiationswitching ad modesturing supports two types of automatic differentiation ad in the back end during sampling the current default ad mode is forwarddiff but turing also supports tracker based differentation to switch between forwarddiff and tracker one can call function turing setadbackend backend sym where backend sym can be forward diff or reverse diff compositional sampling with differing ad modesturing supports intermixed automatic differentiation methods for different variable spaces the snippet below shows using forwarddiff to sample the mean m parameter and using the tracker based trackerad autodiff for the variance s parameter using turing define a simple normal model with unknown mean and variance model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end sample using gibbs and varying autodiff backends c sample gdemo 1 5 2 gibbs hmc turing forwarddiffad 1 0 1 5 m hmc turing trackerad 0 1 5 s 1000 generally trackerad is faster when sampling from variables of high dimensionality greater than 20 and forwarddiffad is more efficient for lower dimension variables this functionality allows those who are performance sensistive to fine tune their automatic differentiation for their specific models if the differentation method is not specified in this way turing will default to using whatever the global ad backend is currently this defaults to forwarddiff", "title": "Automatic Differentiation"},{"location": "/docs/using-turing/compiler", "text": "in this section i will describe the current design of turing s model compiler which enables turing to perform various types of bayesian inference without changing the model definition what we call compiler is essentially just a macro that transforms the user s code to something that julia s dispatch can operate on and that julia s compiler can successfully do type inference on for efficient machine code generation overviewthe following terminology will be used in this section d observed data variables conditioned upon in the posterior p parameter variables distributed according to the prior distributions these will also be referred to as random variables model a fully defined probabilistic model with input data and modelgen a model generator function that can be used to instantiate a model instance by inputing data d turing s model macro defines a modelgen that can be used to instantiate a model by passing in the observed data d model macro and modelgenthe following are the main jobs of the model macro parse and lines e g y normal c x 1 0 figure out if a variable belongs to the data d and or to the parameters p enable the handling of missing data variables in d when defining a model and treating them as parameter variables in p instead enable the tracking of random variables using the data structures varname and varinfo change lines with a variable in p on the lhs to a call to an assume dot assume block change lines with a variable in d on the lhs to a call to an observe dot observe block enable type stable automatic differentiation of the model using type parameterslet s take the following model as an example model gauss x missing y 1 0 type tv vector float64 where tv lt abstractvector begin if x missing x tv undef 3 end p tv undef 2 p 1 inversegamma 2 3 p 2 normal 0 1 0 x 1 2 normal p 2 sqrt p 1 x 3 normal y normal p 2 sqrt p 1 endthe above call of the model macro defines an instance of modelgen called gauss a model model can be defined using gauss rand 3 1 0 or gauss x rand 3 y 1 0 while constructing the model if an argument is not passed in it gets assigned to its default value if there is no default value given an error is thrown if an argument has a default value missing when not passed in it is treated as a random variable for variables which require an intialization because we need to loop or broadcast over its elements such as x above the following needs to be done if x missing x endif x is sampled as a whole from a multivariate distribution e g x mvnormal there is no need to initialize it in an if block modelgen is defined as struct modelgen targs f tdefaults lt function f f defaults tdefaultsendmodelgen targs args where targs modelgen targs typeof args args m modelgen args kwargs m f args kwargs targs is the tuple of the symbols of the model s arguments x y tv defaults is the namedtuple of default values x missing y 1 0 tv vector float64 the model macro is defined as macro model input expr build model info input expr gt replace tilde gt replace vi gt replace logpdf gt replace sampler gt build outputendbuild model infothe first stop that the model definition takes is build model info this function extracts some information from the model definition such as name the model name main body the model body excluding the header and end arg syms the argument symbols e g x y tv above args a modified version of the arguments changing type tv vector float64 and where tv lt abstractvector to tv type lt abstractvector vector float64 this is x missing y 1 0 tv type lt abstractvector vector float64 in the example above args nt an expression constructing a namedtuple of the input arguments e g x x y y tv tv in the example above defaults nt an expression constructing a namedtuple of the default values of the input arguments if any e g x missing y 1 tv vector float64 in the example above and returns it as a dictionary called model info replace tilde after some model information have been extracted replace tilde replaces the l r lines in the model with the output of core tilde l r model info where l and r are either expressions or symbols l can also be a constant literal the replace tilde function also replaces expressions of the form l r with the output of dot tilde l r model info in the above example p 1 inversegamma 2 3 is replaced with temp right inversegamma 2 3 turing core assert dist temp right msg preprocessed turing core preprocess val x y t turing getmissing model p 1 if preprocessed isa tuple vn inds preprocessed out turing inference tilde ctx sampler temp right vn inds vi p 1 out 1 vi logp out 2 else vi logp turing inference tilde ctx sampler temp right preprocessed vi endwhere ctx abstractcontext sampler abstractsampler and vi varinfo will be discussed later assert dist will check that the rhs of is a distribution otherwise an error is thrown the preprocess macro here checks if the symbol on the lhs of p in this case is in the arguments to the model x y t or not if it isn t then p 1 will be treated as a random variable if it is in the arguments but was among the arguments with a value of missing obtained using getmissing model then p 1 is also treated as a random variable if neither of the above is true but the value of p 1 is missing then p 1 will still be treated as a random variable otherwise p 1 is treated as an observation if preprocess treats p 1 as a random variable it will return a 2 tuple of 1 a variable identifier vn varname turing varname p 1 and 2 a tuple of tuples of the indices used in vn 1 in this example otherwise preprocess returns the value of p 1 turing varname and varname wil be explained later the above checks by preprocess were carefully written to make sure that the julia compiler can compile them away so no checks happen at runtime and only the correct branch is run straight away when the output of preprocess is a tuple i e p 1 is a random variable the turing inference tilde function will dispatch to a different method than when the output is of another type i e p 1 is an observation in the former case turing inference tilde returns 2 outputs the value of the random variable and the log probability while in the latter case only the log probability is returned the log probabilities then get accumulated and if p 1 is a random variable the first returned output by turing inference tilde gets assigned to it note that core tilde is different from inference tilde core tilde returns the expression block that will be run instead of the line a part of this expression block is a call to inference tilde as shown above core tilde is defined in the compiler jl file while inference tilde is defined in the inference jl file the dot tilde function does something similar for expressions of the form l r and l r in julia 1 1 and above let s take x 1 2 normal p 2 sqrt p 1 as an example this expressions replaced with temp right normal p 2 sqrt p 1 turing core assert dist temp right msg preprocessed turing core preprocess val x y t turing getmissing model x 1 2 if preprocessed isa tuple vn inds preprocessed temp left x 1 2 out turing inference dot tilde ctx sampler temp right temp left vn inds vi left out 1 vi logp out 2 else temp left preprocessed x 1 2 vi logp turing inference dot tilde ctx sampler temp right temp left vi endthe main difference in the expanded code between l r and l r is that the former doesn t assume l to be defined it can be a new julia variable in the scope while the latter assumes l already exists l is also always input to the dot tilde function but not the tilde function replace vi replace logpdf and replace sampler using varinfo inside the model body will give the user access to the vi varinfo object used inside the model the function replace vi therefore finds and replaces every use of varinfo with the handle to the varinfo instance used inside the model the logpdf macro will return vi logp which is the accumumlated log probability that the model is computing what this means can change depending on the context ctx used when running the model finally replace sampler will replace sampler with the sampler input to the model turing modelevery model model can be called as a function with arguments vi varinfo spl abstractsampler and ctx abstractcontext vi is a data structure that stores information about random variables in p spl includes the choice of the mcmc algorithm e g metropolis hastings importance sampling or hamiltonian monte carlo hmc ctx is used to modify the behaviour of the logp accumulator accumulating different variants of it for example if ctx isa likelihoodcontext only the log likelihood will be accumulated in vi logp by default ctx isa defaultcontext which accumulates the log joint probability of p and d the inference tilde and inference dot tilde functions will do something different for different subtypes of abstractsampler to facilitate the sampling process the model struct is defined as follows struct model f targs lt namedtuple tmodelgen tmissings lt val f f args targs modelgen tmodelgen missings tmissingsendmodel f args namedtuple modelgen model f args modelgen getmissing args model model vi model vi samplefromprior model model vi spl model vi spl defaultcontext model model args kwargs model f args model kwargs model f is an internal function that is called when model is called where model model when model is called model itself is passed as an argument to model f because we need to access model args among other things inside f model args is a namedtuple of all the arguments that were passed to the model generating function when constructing an instance of model modelgen is the instance of modelgen that was used to construct model missings is an instance of val e g val a b getmissings returns a val instance of all the symbols in args with a value missing this is the default definition of missings all variables in missings are treated as random variables rather than observations in some non traditional use cases missings is defined differently e g when computing the log joint probability of the random variables and only some observations simultaneously possibly conditioned on the remaining observations an example using the model above is logprob x rand 3 p rand 2 model gauss y nothing to evaluate this the model argument x on the lhs of is treated as a random variable leading to a call to the assume or dot assume function in place of the or expressions respectively the model is then run in the priorcontext which ignores the observe and dot observe functions and only runs the assume and dot assume ones this returns the correct log probability the reason why a model input argument such as x cannot be initialized to missing when on the lhs of is somewhat subtle in the model body before calling sometimes there would be a call to length x iterating over the elements of x in a loop calling on each element of x if x is initialized to missing this will error because length missing is not defined moreover it is not intuitive to require the user to handle the x missing case because the user never assigned x to be missing in the first place missing is merely an implementation detail in this case that the users need not concern themselves with therefore in this case it makes sense to de couple the missings field from the values of the arguments build outputnow that we have all the information we need in the model macro we can start building the model generator function the model generator function gauss will be defined as function outer function x missing y 1 0 tv type lt abstractvector vector float64 return outer function x y tv endfunction outer function x missing y 1 0 tv type lt abstractvector vector float64 function inner function vi turing varinfo sampler turing abstractsampler ctx abstractcontext model end return turing model inner function x x y y tv tv turing core modelgen x y tv outer function x missing y 1 0 tv vector float64 endgauss turing core modelgen x y tv outer function x missing y 1 0 tv vector float64 the above 2 methods enable constructing the model using positional or keyword arguments the second argument to the turing model constructor is the expression called args nt stored in model info the second argument to the modelgen constructor inside outer function and outside is the expression called defaults nt stored in model info the body of the inner function is explained below inner functionthe main method of inner function does some pre processing defining all the input variables from the model definition x y and tv in the example above then the rest of the model body is run as normal julia code with the l r and l r lines replaced with the calls to inference tilde and inference dot tilde respectively as shown earlier function inner function vi turing varinfo sampler turing abstractsampler ctx abstractcontext model temp x model args x xt typeof temp x if temp x isa turing core floatorarraytype x turing core get matching type sampler vi temp x elseif turing core hasmissing xt x turing core get matching type sampler vi xt temp x else x temp x end the code above is repeated for the other 2 variables y and tv reset vi logp vi logp 0 main model bodyendas one can see above x y and tv are defined in the method body using an if block followed by the rest of the code the first branch of this if block is run if the variable is a number or array type such as tv vector float64 one of the purposes of get matching type is to check if sampler requires automatic differentiation and to modify tv accordingly for example when using forwarddiff for automatic differentiation tv will be defined as some concrete subtype of vector lt forwarddiff dual this same function is also used to replace array with libtask tarray types when a particle sampler is used the second branch of the if block is to handle partially missing data converting the type of the input vector to another type befitting of the sampler used whether it is for automatic differentiation or for particle samplers finally the third branch is the one that will be run for x and y above simply assigning these names to model args x and model args y respectively the main model body is then the same model body passed in by the user after replacing l r l r varinfo and logpdf as explained eariler varnamein order to track random variables in the sampling process turing uses the struct varname sym which acts as a random variable identifier generated at runtime the varname of a random variable is generated from the expression on the lhs of a statement when the symbol on the lhs is in p every vn varname sym has a symbol sym which is the symbol of the julia variable in the model that the random variable belongs to for example x 1 normal will generate an instance of varname x assuming x is in p every vn varname also has a field indexing which stores the indices requires to access the random variable from the julia variable indicated by sym for example x 1 normal will generate a vn varname x with vn indexing 1 varname also supports hierarchical arrays and range indexing some more examples x 1 normal will generate a varname x with indexing 1 x 1 mvnormal zeros 2 will generate a varname x with indexing colon 1 x 1 2 normal will generate a varname x with indexing colon 1 2 varinfooverviewvarinfo is the data structure in turing that facilitates tracking random variables and certain metadata about them that are required for sampling for instance the distribution of every random variable is stored in varinfo because we need to know the support of every random variable when sampling using hmc for example random variables whose distributions have a constrained support are transformed using a bijector from bijectors jl so that the sampling happens in the unconstrained space different samplers require different metadata about the random variables the definition of varinfo in turing is struct varinfo tmeta tlogp lt abstractvarinfo metadata tmeta logp base refvalue tlogp num produce base refvalue int endbased on the type of metadata the varinfo is either aliased untypedvarinfo or typedvarinfo metadata can be either a subtype of the union type metadata or a namedtuple of multiple such subtypes let vi be an instance of varinfo if vi isa varinfo lt metadata then it is called an untypedvarinfo if vi isa varinfo lt namedtuple then vi metadata would be a namedtuple mapping each symbol in p to an instance of metadata vi would then be called a typedvarinfo the other fields of varinfo include logp which is used to accumulate the log probability or log probability density of the variables in p and d num produce keeps track of how many observations have been made in the model so far this is incremented when running a statement when the symbol on the lhs is in d metadatathe metadata struct stores some metadata about the random variables sampled this helps query certain information about a variable such as its distribution which samplers sample this variable its value and whether this value is transformed to real space or not let md be an instance of metadata md vns is the vector of all varname instances let vn be an arbitrary element of md vns md idcs is the dictionary that maps each varname instance to its index inmd vns md ranges md dists md orders and md flags md vns md idcs vn vn md dists md idcs vn is the distribution of vn md gids md idcs vn is the set of algorithms used to sample vn this is used inthe gibbs sampling process md orders md idcs vn is the number of observe statements before vn is sampled md ranges md idcs vn is the index range of vn in md vals md vals md ranges md idcs vn is the linearized vector of values of corresponding to vn md flags is a dictionary of true false flags md flags flag md idcs vn is thevalue of flag corresponding to vn note that in order to make md metadata type stable all the md vns must have the same symbol and distribution type however one can have a single julia variable e g x that is a matrix or a hierarchical array sampled in partitions e g x 1 mvnormal zeros 2 1 0 x 2 mvnormal ones 2 1 0 the symbol x can still be managed by a single md metadata without hurting the type stability since all the distributions on the rhs of are of the same type however in turing models one cannot have this restriction so we must use a type unstable metadata if we want to use one metadata instance for the whole model this is what untypedvarinfo does a type unstable metadata will still work but will have inferior performance to strike a balance between flexibility and performance when constructing the spl sampler instance the model is first run by sampling the parameters in p from their priors using an untypedvarinfo i e a type unstable metadata is used for all the variables then once all the symbols and distribution types have been identified a vi typedvarinfo is constructed where vi metadata is a namedtuple mapping each symbol in p to a specialized instance of metadata so as long as each symbol in p is sampled from only one type of distributions vi typedvarinfo will have fully concretely typed fields which brings out the peak performance of julia", "title": "Turing Compiler Design"},{"location": "/docs/using-turing/dynamichmc", "text": "using dynamichmcturing supports the use of dynamichmc as a sampler through the dynamicnuts function dynamicnuts is not appropriate for use in compositional inference if you intend to use gibbs sampling you must use turing s native nuts function to use the dynamicnuts function you must import the dynamichmc package as well as turing turing does not formally require dynamichmc but will include additional functionality if both packages are present here is a brief example of how to apply dynamicnuts import turing and dynamichmc using logdensityproblems dynamichmc turing model definition model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end pull 2 000 samples using dynamicnuts chn sample gdemo 1 5 2 0 dynamicnuts 2000", "title": "Using DynamicHMC"},{"location": "/docs/using-turing/get-started", "text": "getting startedinstallationto use turing you need to install julia first and then install turing install juliayou will need to install julia 1 0 or greater which you can get from the official julia website install turing jlturing is an officially registered julia package so the following will install a stable version of turing while inside julia s package manager press from the repl add turingif you want to use the latest version of turing with some experimental features you can try the following instead add turing mastertest turingif all tests pass you re ready to start using turing examplehere s a simple example showing the package in action using turingusing statsplots define a simple normal model with unknown mean and variance model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end run sampler collect resultschn sample gdemo 1 5 2 hmc 0 1 5 1000 summarise results currently requires the master branch from mcmcchains describe chn plot and save resultsp plot chn savefig gdemo plot png", "title": "Getting Started"},{"location": "/docs/using-turing/guide", "text": "guidebasicsintroductiona probabilistic program is julia code wrapped in a model macro it can use arbitrary julia code but to ensure correctness of inference it should not have external effects or modify global state stack allocated variables are safe but mutable heap allocated objects may lead to subtle bugs when using task copying to help avoid those we provide a turing safe datatype tarray that can be used to create mutable arrays in turing programs to specify distributions of random variables turing programs should use the notation x distr where x is a symbol and distr is a distribution if x is undefined in the model function inside the probabilistic program this puts a random variable named x distributed according to distr in the current scope distr can be a value of any type that implements rand distr which samples a value from the distribution distr if x is defined this is used for conditioning in a style similar to anglican another ppl in this case x is an observed value assumed to have been drawn from the distribution distr the likelihood is computed using logpdf distr y the observe statements should be arranged so that every possible run traverses all of them in exactly the same order this is equivalent to demanding that they are not placed inside stochastic control flow available inference methods include importance sampling is sequential monte carlo smc particle gibbs pg hamiltonian monte carlo hmc hamiltonian monte carlo with dual averaging hmcda and the no u turn sampler nuts simple gaussian demobelow is a simple gaussian demo illustrate the basic usage of turing jl import packages using turingusing statsplots define a simple normal model with unknown mean and variance model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s endnote as a sanity check the expectation of s is 49 24 2 04166666 and the expectation of m is 7 6 1 16666666 we can perform inference by using the sample function the first argument of which is our probabalistic program and the second of which is a sampler more information on each sampler is located in the api run sampler collect results c1 sample gdemo 1 5 2 smc 1000 c2 sample gdemo 1 5 2 pg 10 1000 c3 sample gdemo 1 5 2 hmc 0 1 5 1000 c4 sample gdemo 1 5 2 gibbs pg 10 m hmc 0 1 5 s 1000 c5 sample gdemo 1 5 2 hmcda 0 15 0 65 1000 c6 sample gdemo 1 5 2 nuts 0 65 1000 the mcmcchains module which is re exported by turing provides plotting tools for the chain objects returned by a sample function see the mcmcchains repository for more information on the suite of tools available for diagnosing mcmc chains summarise resultsdescribe c3 plot resultsplot c3 savefig gdemo plot png the arguments for each sampler are smc number of particles pg number of particles number of iterations hmc leapfrog step size leapfrog step numbers gibbs component sampler 1 component sampler 2 hmcda total leapfrog length target accept ratio nuts number of adaptation steps optional target accept ratio for detailed information on the samplers please review turing jl s api documentation modelling syntax explainedusing this syntax a probabilistic model is defined in turing the model function generated by turing can then be used to condition the model onto data subsequently the sample function can be used to generate samples from the posterior distribution in the following example the defined model is conditioned to the date arg1 1 arg2 2 by passing 1 2 to the model function model model name arg 1 arg 2 begin endthe conditioned model can then be passed onto the sample function to run posterior inference model func model name 1 2 chn sample model func hmc perform inference by sampling using hmc the returned chain contains samples of the variables in the model var 1 mean chn var 1 taking the mean of a variable named var 1 the key var 1 can be a symbol or a string for example to fetch x 1 one can use chn symbol x 1 or chn x 1 the benefit of using a symbol to index allows you to retrieve all the parameters associated with that symbol as an example if you have the parameters x 1 x 2 and x 3 calling chn x will return a new chain with only x 1 x 2 and x 3 turing does not have a declarative form more generally the order in which you place the lines of a model macro matters for example the following example works define a simple normal model with unknown mean and variance model model function y begin s poisson 1 y normal s 1 return yendsample model function 10 smc 100 but if we switch the s poisson 1 and y normal s 1 lines the model will no longer sample correctly define a simple normal model with unknown mean and variance model model function y begin y normal s 1 s poisson 1 return yendsample model function 10 smc 100 sampling multiple chainsif you have julia 1 3 or greater you may use psample to sample multiple chains in a multithreaded way generate 4 chains each with 1 000 samples chains psample model sampler 1000 4 for older versions of julia psample may not function correctly if you wish to run multiple chains you can do so with the mapreduce function replace num chains below with however many chains you wish to sample chains mapreduce c gt sample model fun sampler 1000 chainscat 1 num chains the chains variable now contains a chains object which can be indexed by chain to pull out the first chain from the chains object use chains 1 having multiple chains in the same object is valuable for evaluating convergence some diagnostic functions like gelmandiag require multiple chains sampling from an unconditional distribution the prior turing allows you to sample from a declared model s prior by calling the model without specifying inputs or a sampler in the below example we specify a gdemo model which returns two variables x and y the model includes x and y as arguments but calling the function without passing in x or y means that turing s compiler will assume they are missing values to draw from the relevant distribution the return statement is necessary to retrieve the sampled x and y values model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s return x yendassign the function with missing inputs to a variable and turing will produce a sample from the prior distribution samples from p x y g prior sample gdemo missing missing g prior sample output 0 685690547873451 1 1972706455914328 sampling from a conditional distribution the posterior treating observations as random variablesinputs to the model that have a value missing are treated as parameters aka random variables to be estimated sampled this can be useful if you want to simulate draws for that parameter or if you are sampling from a conditional distribution turing supports the following syntax model gdemo x type t float64 where t begin if x missing initialize x if missing x vector t undef 2 end s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend construct a model with x missingmodel gdemo missing c sample model hmc 0 01 5 500 note the need to initialize x when missing since we are iterating over its elements later in the model the generated values for x can be extracted from the chains object using c x turing also supports mixed missing and non missing values in x where the missing ones will be treated as random variables to be sampled while the others get treated as observations for example model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend x 1 is a parameter but x 2 is an observationmodel gdemo missing 2 4 c sample model hmc 0 01 5 500 default valuesarguments to turing models can have default values much like how default values work in normal julia functions for instance the following will assign missing to x and treat it as a random variable if the default value is not missing x will be assigned that value and will be treated as an observation instead using turing model generative x missing type t float64 where t lt real begin if x missing initialize x when missing x vector t undef 10 end s inversegamma 2 3 m normal 0 sqrt s for i in 1 length x x i normal m sqrt s end return s mendm generative chain sample m hmc 0 01 5 1000 access values inside chainyou can access the values inside a chain several ways turn them into a dataframe object use their raw axisarray form create a three dimensional array objectfor example let c be a chain 1 dataframe c converts c to a dataframe 2 c value retrieves the values inside c as an axisarray and 3 c value data retrieves the values inside c as a 3d array variable types and type parametersthe element type of a vector or matrix of random variables should match the eltype of the its prior distribution lt integer for discrete distributions and lt abstractfloat for continuous distributions moreover if the continuous random variable is to be sampled using a hamiltonian sampler the vector s element type needs to either be 1 real to enable auto differentiation through the model which uses special number types that are sub types of real or 2 some type parameter t defined in the model header using the type parameter syntax e g gdemo x type t float64 where t begin similarly when using a particle sampler the julia variable used should either be 1 a tarray or 2 an instance of some type parameter t defined in the model header using the type parameter syntax e g gdemo x type t vector float64 where t begin querying probabilities from model or chainconsider the following gdemo model model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s endthe following are examples of valid queries of the turing model or chain prob x 1 0 y 1 0 model gdemo s 1 0 m 1 0 calculates the likelihood of x 1 and y 1 given s 1 and m 1 prob s 1 0 m 1 0 model gdemo x nothing y nothing calculates the joint probability of s 1 and m 1 ignoring x and y x and y are ignored so they can be optionally dropped from the rhs of but it is recommended to define them prob s 1 0 m 1 0 x 1 0 model gdemo y nothing calculates the joint probability of s 1 m 1 and x 1 ignoring y prob s 1 0 m 1 0 x 1 0 y 1 0 model gdemo calculates the joint probability of all the variables after the mcmc sampling given a chain prob x 1 0 y 1 0 chain chain calculates the element wise likelihood of x 1 0 and y 1 0 for each sample in chain in all the above cases logprob can be used instead of prob to calculate the log probabilities instead beyond the basicscompositional sampling using gibbsturing jl provides a gibbs interface to combine different samplers for example one can combine an hmc sampler with a pg sampler to run inference for different parameters in a single model as below model simple choice xs begin p beta 2 2 z bernoulli p for i in 1 length xs if z 1 xs i normal 0 1 else xs i normal 2 1 end endendsimple choice f simple choice 1 5 2 0 0 3 chn sample simple choice f gibbs hmc 0 2 3 p pg 20 z 1000 the gibbs sampler can be used to specify unique automatic differentation backends for different variable spaces please see the automatic differentiation article for more for more details of compositional sampling in turing jl please check the corresponding paper working with mcmcchains jlturing jl wraps its samples using mcmcchains chain so that all the functions working for mcmcchains chain can be re used in turing jl two typical functions are mcmcchains describe and mcmcchains plot which can be used as follows for an obtained chain chn for more information on mcmcchains please see the github repository describe chn lists statistics of the samples plot chn plots statistics of the samples there are numerous functions in addition to describe and plot in the mcmcchains package such as those used in convergence diagnostics for more information on the package please see the github repository working with libtask jlthe libtask jl library provides write on copy data structures that are safe for use in turing s particle based samplers one data structure in particular is often required for use the tarray the following sampler types require the use of a tarray to store distributions ipmcmc is pg pmmh smcif you do not use a tarray to store arrays of distributions when using a particle based sampler you may experience errors here is an example of how the tarray using a tarray constructor function called tzeros can be applied in this way turing model definition model bayeshmm y begin declare a tarray with a length of n s tzeros int n m vector real undef k t vector vector real undef k for i 1 k t i dirichlet ones k k m i normal i 0 01 end draw from a distribution for each element in s s 1 categorical k for i 2 n s i categorical vec t s i 1 y i normal m s i 0 1 end return s m end changing default settingssome of turing jl s default settings can be changed for better usage ad chunk sizeforwarddiff turing s default ad backend uses forward mode chunk wise ad the chunk size can be manually set by setchunksize new chunk size alternatively use an auto tuning helper function auto tune chunk size mf function rep num 10 which will profile various chunk sizes here mf is the model function e g gdemo 1 5 2 and rep num is the number of repetitions during profiling ad backendsince 428 turing jl supports tracker as backend for reverse mode autodiff to switch between forwarddiff jl and tracker one can call function setadbackend backend sym where backend sym can be forward diff or reverse diff for more information on turing s automatic differentiation backend please see the automatic differentiation article progress meterturing jl uses progressmeter jl to show the progress of sampling which may lead to slow down of inference or even cause bugs in some ides due to i o this can be turned on or off by turnprogress true and turnprogress false of which the former is set as default", "title": "Guide"},{"location": "/docs/using-turing/index", "text": "turing documentationwelcome to the documentation for turing introductionturing is a general purpose probabilistic programming language for robust efficient bayesian inference and decision making current features include general purpose probabilistic programming with an intuitive modelling interface robust efficient hamiltonian monte carlo hmc sampling for differentiable posterior distributions particle mcmc sampling for complex posterior distributions involving discrete variables and stochastic control flow and compositional inference via gibbs sampling that combines particle mcmc hmc and random walk mh rwmh", "title": "Turing Documentation"},{"location": "/docs/using-turing/interface", "text": "the sampling interfaceturing implements a sampling interface hosted at abstractmcmc that is intended to provide a common framework for markov chain monte carlo samplers the interface presents several structures and functions that one needs to overload in order to implement an interface compatible sampler this guide will demonstrate how to implement the interface without turing and then demonstrate how to implement the same sampler with turing interface overviewany implementation of an inference method that uses the abstractmcmc interface should implement some combination of the following types and functions a subtype of abstractsampler defined as a mutable struct containing state information or sampler parameters a subtype of abstracttransition which represents a single draw from the sampler a function transition type which returns the abstracttransition type used by an implementation of an abstractsampler or a function transition init with returns a vector abstracttransition of length n a function sample init which performs any necessary set up a function step which returns an abstracttransition a function sample end which handles any sampler wrap up a function bundle samples which accepts an vector lt abstracttransition and returns some abstractchains object most implementations of the interface return an mcmcchains chains struct the interface methods with exclamation points are those that are intended to allow for some state mutation any mutating function is meant to allow mutation where needed you might use sample init to run some kind of sampler preparation before sampling begins this could mutate a sampler s state step might mutate a sampler flag after each sample sample end contains any wrap up you might need to do if you were sampling in a transformed space this might be where you convert everything back to a constrained space why do you have an interface the motivation for the interface is to allow julia s fantastic probabilistic programming language community to have a set of standards and common implementations so we can all thrive together markov chain monte carlo methods tend to have a very similar framework to one another and so a common interface should help more great inference methods built in single purpose packages to experience more use among the community implementing metropolis hastings without turingmetropolis hastings is often the first sampling method that people are exposed to it is a very straightforward algorithm and is accordingly the easiest to implement so it makes for a good example in this section you will learn how to use the types and functions listed above to implement the metropolis hastings sampler using the mcmc interface the full code for this implementation is housed in advancedmh jl importslet s begin by importing the relevant libraries we ll import abstracmcmc which contains the interface framework we ll fill out we also need distributions and random import the relevant libraries using abstractmcmcusing distributionsusing randoman interface extension like the one we re writing right now typically requires that you overload or implement several functions specifically you should import the functions you intend to overload this next code block accomplishes that from distributions we need sampleable variateform and valuesupport three abstract types that define a distribution models in the interface are assumed to be subtypes of sampleable variateform valuesupport in this section our model is going be be extremely simple so we will not end up using these except to make sure that the inference functions are dispatching correctly we need the mcmcchains chains function because at the end of sampling we need to be able to convert a vector of lt abstracttransitions to an mcmcchains object lastly we import the specific functions from abstractmcmc that we need to overload because metropolis hastings is so simple we only need a few functions types step abstractsampler abstracttransition transition typemore complex inference methods may need to import additional functions for now let s focus on the code import specific functions and types to use or overload import mcmcchains chainsimport abstractmcmc step abstractsampler abstracttransition transition type bundle samplessamplerlet s begin our sampler definition by defining a sampler called metropolishastings which is a subtype of abstractsampler correct typing is very important for proper interface implementation if you are missing a subtype your method may not be dispatched to when you call sample define a sampler type struct metropolishastings t d lt abstractsampler init t proposal dend default constructors metropolishastings init real metropolishastings init normal 0 1 metropolishastings init vector lt real metropolishastings init mvnormal length init 1 above we have defined a sampler that stores the initial parameterization of the prior and a distribution object from which proposals are drawn you can have a struct that has no fields and simply use it for dispatching onto the relevant functions or you can store a large amount of state information in your sampler the general intuition for what to store in your sampler struct is that anything you may need to perform inference between samples but you don t want to store in an abstracttransition should go into the sampler struct it s the only way you can carry non sample related state information between step calls modelnext we need to have a model of some kind a model is a struct that s a subtype of abstractmodel that contains whatever information is necessary to perform inference on your problem in our case we want to know the mean and variance parameters for a standard normal distribution so we can keep our model to the log density of a normal note that we only have to do this because we are not yet integrating the sampler with turing turing has a very sophisticated modelling engine that removes the need to define custom model structs define a model type stores the log density function struct densitymodel f lt function lt abstractmodel fendtransitionthe next step is to define some subtype of abstracttransition which we will return from each step call we ll keep it simple by just defining a wrapper struct that contains the parameter draws and the log density of that draw create a very basic transition type only stores the parameter draws and the log probability of the draw struct transition t lt union vector lt real lt real l lt real lt abstracttransition t lp lend store the new draw and its log density transition model densitymodel transition model transition can now store any type of parameter whether it s a vector of draws from multiple parameters or a single univariate draw we should also tell the interface what specific subtype of abstracttransition we re going to use so we can just define a new method on transition type tell the interface what transition type we would like to use transition type model densitymodel spl metropolishastings typeof transition spl init model spl init metropolis hastingsnow it s time to get into the actual inference we ve defined all of the core pieces we need but we need to implement the step function which actually perform inference as a refresher metropolis hastings implements a very basic algorithm pick some initial state theta 0 for t in 1 n do a generate a proposal parameterization t sim q theta t mid theta t 1 b calculate the acceptance probability alpha text min big 1 frac pi t pi theta t 1 frac q t 1 mid t q t mid t 1 big c if u le where u sim 0 1 then theta t theta t otherwise theta t theta t 1 of course it s much easier to do this in the log space so the acceptance probability is more commonly written as alpha min big log pi t log pi t 1 log q t 1 mid t log q t mid t 1 0 big in interface terms we should do the following make a new transition containing a proposed sample calculate the acceptance probability if we accept return the new transition otherwise return the old one stepsthe step function is the function that performs the bulk of your inference in our case we will implement two step functions one for the very first iteration and one for every subsequent iteration define the first step function which is called at the beginning of sampling return the initial parameter used to define the sampler function step rng abstractrng model densitymodel spl metropolishastings n integer kwargs return transition model spl init endthe first step function just packages up the initial parameterization inside the sampler and returns it we implicity accept the very first parameterization the other step function performs the usual steps from metropolis hastings included are several helper functions proposal and q which are designed to replicate the functions in the pseudocode above proposal generates a new proposal in the form of a transition which can be univariate if the value passed in is univariate or it can be multivariate if the transition given is multivariate proposals use a basic normal or mvnormal proposal distribution q returns the log density of one parameterization conditional on another according to the proposal distribution step generates a new proposal checks the acceptance probability and then returns either the previous transition or the proposed transition define a function that makes a basic proposal depending on a univariate parameterization or a multivariate parameterization propose spl metropolishastings model densitymodel real transition model rand spl proposal propose spl metropolishastings model densitymodel vector lt real transition model rand spl proposal propose spl metropolishastings model densitymodel t transition propose spl model t calculates the probability q cond using the proposal distribution spl proposal q spl metropolishastings real cond real logpdf spl proposal cond q spl metropolishastings vector lt real cond vector lt real logpdf spl proposal cond q spl metropolishastings t1 transition t2 transition q spl t1 t2 calculate the density of the model given some parameterization model densitymodel model model densitymodel t transition t lp define the other step function returns a transition containing either a new proposal if accepted or the previous proposal if not accepted function step rng abstractrng model densitymodel spl metropolishastings integer prev transition kwargs generate a new proposal propose spl model prev calculate the log acceptance probability model model prev q spl prev q spl prev decide whether to return the previous or the new one if log rand rng lt min 0 0 return else return prev endendchainsthe last piece in our puzzle is a bundle samples function which accepts a vector t lt transition and returns an mcmcchains chains struct fortunately our transition is incredibly simple and we only need to build a little bit of functionality to accept custom parameter names passed in by the user a basic chains constructor that works with the transition struct we defined function bundle samples rng abstractrng abstractmodel s metropolishastings n integer ts vector lt abstracttransition param names missing kwargs turn all the transitions into a vector of vectors vals copy reduce hcat vcat t t lp for t in ts check if we received any parameter names if ismissing param names param names parameter i for i in 1 length first vals 1 end add the log density field to the parameter names push param names lp bundle everything up and return a chains struct return chains vals param names internals lp endall done testing the implementationnow that we have all the pieces we should test the implementation by defining a model to calculate the mean and variance parameters of a normal distribution we can do this by constructing a target density function providing a sample of data and then running the sampler with sample generate a set of data from the posterior we want to estimate data rand normal 5 3 30 define the components of a basic model insupport 2 gt 0dist normal 1 2 density insupport sum logpdf dist data inf construct a densitymodel model densitymodel density set up our sampler with initial parameters spl metropolishastings 0 0 0 0 sample from the posterior chain sample model spl 100000 param names if all the interface functions have been extended properly you should get an output from display chain that looks something like this object of type chains with data of type 100000 3 1 array float64 3 iterations 1 100000thinning interval 1chains 1samples per chain 100000internals lpparameters 2 element array chaindataframe 1 summary statistics row parameters mean std naive se mcse ess r hat symbol float64 float64 float64 float64 any any 1 5 33157 0 854193 0 0027012 0 00893069 8344 75 1 00009 2 4 54992 0 632916 0 00200146 0 00534942 14260 8 1 00005 quantiles row parameters 2 5 25 0 50 0 75 0 97 5 symbol float64 float64 float64 float64 float64 1 3 6595 4 77754 5 33182 5 89509 6 99651 2 3 5097 4 09732 4 47805 4 93094 5 96821 it looks like we re extremely close to our true parameters of normal 5 3 though with a fairly high variance due to the low sample size conclusionwe ve seen how to implement the sampling interface for general projects turing s interface methods are ever evolving so please open an issue at abstractmcmc with feature requests or problems", "title": "Interface Guide"},{"location": "/docs/using-turing/performancetips", "text": "performance tipsthis section briefly summarises a few common techniques to ensure good performance when using turing we refer to julialang org for general techniques to ensure good performance of julia programs use multivariate distributionsit is generally preferable to use multivariate distributions if possible the following example model gmodel x begin m normal for i 1 length x x i normal m 0 2 endendcan be directly expressed more efficiently using a simple transformation model gmodel x begin m normal x mvnormal fill m length x 0 2 endchoose your ad backendturing currently provides support for two different automatic differentiation ad backends generally try to use forward diff for models with few parameters and reverse diff for models with large parameter vectors or linear algebra operations see automatic differentiation for details special care for reverse diffin case of reverse diff it is necessary to avoid loops for now this is mainly due to the reverse mode ad backend tracker which is inefficient for such cases therefore it is often recommended to write a custom distribution which implements a multivariate version of the prior distribution make your model type stablefor efficient gradient based inference e g using hmc nuts or advi it is important to ensure the model is type stable we refer to julialang org for a general discussion on type stability the following example model tmodel x y begin p n size x params vector real undef n for i 1 n params i truncated normal 0 inf end a x params y mvnormal a 1 0 endcan be transformed into the following type stable representation model tmodel x y type t vector float64 where t begin p n size x params t undef n for i 1 n params i truncated normal 0 inf end a x params y mvnormal a 1 0 endnote that you can use code warntype to find type instabilities in your model definition for example consider the following simple program model tmodel x begin p vector real undef 1 p 1 normal p p 1 x normal p 1 endwe can usemodel tmodel 1 0 varinfo turing varinfo model spl turing samplefromprior code warntype model f varinfo spl turing defaultcontext model to inspect the type instabilities in the model reuse computations in gibbs samplingoften when performing gibbs sampling one can save computational time by caching the output of expensive functions the cached values can then be reused in future gibbs sub iterations which do not change the inputs to this expensive function for example in the following model model demo x begin a gamma b normal c function1 a d function2 b x normal c d endalg gibbs mh a mh b sample demo zeros 10 alg 1000 when only updating a in a gibbs sub iteration keeping b the same the value of d doesn t change and when only updating b the value of c doesn t change however if function1 and function2 are expensive and are both run in every gibbs sub iteration a lot of time would be spent computing values that we already computed before such a problem can be overcome using memoization jl memoizing a function lets us store and reuse the output of the function for every input it is called with this has a slight time overhead but for expensive functions the savings will be far greater to use memoization jl simply define memoized versions of function1 and function2 as such using memoization memoize memoized function1 args function1 args memoize memoized function2 args function2 args then define the turing model using the new functions as such model demo x begin a gamma b normal c memoized function1 a d memoized function2 b x normal c d end", "title": "Performance Tips"},{"location": "/docs/using-turing/quick-start", "text": "probablistic programming in thirty secondsif you are already well versed in probabalistic programming and just want to take a quick look at how turing s syntax works or otherwise just want a model to start with we have provided a bayesian coin flipping model to play with this example can be run on however you have julia installed see getting started but you will need to install the packages turing distributions mcmcchains and statsplots if you have not done so already this is an excerpt from a more formal example introducing probabalistic programming which can be found in jupyter notebook form here or as part of the documentation website here import libraries using turing statsplots random set the true probability of heads in a coin p true 0 5 iterate from having seen 0 observations to 100 observations ns 0 100 draw data from a bernoulli distribution i e draw heads or tails random seed 12 data rand bernoulli p true last ns declare our turing model model coinflip y begin our prior belief about the probability of heads in a coin p beta 1 1 the number of observations n length y for n in 1 n heads or tails of a coin are drawn from a bernoulli distribution y n bernoulli p endend settings of the hamiltonian monte carlo hmc sampler iterations 1000 0 05 10 start sampling chain sample coinflip data hmc iterations plot a summary of the sampling process for the parameter p i e the probability of heads in a coin histogram chain p", "title": "Probablistic Programming in Thirty Seconds"},{"location": "/docs/using-turing/sampler-viz", "text": "sampler visualizationintroductionthe codefor each sampler we will use the same code to plot sampler paths the block below loads the relevant libraries and defines a function for plotting the sampler s trajectory across the posterior the turing model definition used here is not especially practical but it is designed in such a way as to produce visually interesting posterior surfaces to show how different samplers move along the distribution using plotsusing statsplotsusing turingusing bijectorsusing randomrandom seed 0 define a strange model model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s bumps sin m cos m m m 5 bumps for i in eachindex x x i normal m sqrt s end return s mend define our data points x 1 5 2 0 13 0 2 1 0 0 set up the model call sample from the prior model gdemo x vi turing varinfo model vi turing samplefromprior vi flags trans true false evaluate surface at coordinates function evaluate m1 m2 vi vals m1 m2 model vi turing samplefromprior vi logpendfunction plot sampler chain extract values from chain val get chain s m lp ss link ref inversegamma 2 3 val s ms val m lps val lp how many surface points to sample granularity 500 range start stop points spread 0 5 start minimum ss spread std ss stop maximum ss spread std ss start minimum ms spread std ms stop maximum ms spread std ms rng collect range start stop stop length granularity rng collect range start stop stop length granularity make surface plot p surface rng rng evaluate camera 30 65 ticks nothing colorbar false color inferno line range 1 length ms plot3d ss line range ms line range lps line range lc viridis line z collect line range legend false colorbar false alpha 0 5 return pend samplersgibbsgibbs sampling tends to exhibit a jittery trajectory the example below combines hmc and pg sampling to traverse the posterior c sample model gibbs hmc 0 01 5 s pg 20 m 1000 plot sampler c hmchamiltonian monte carlo hmc sampling is a typical sampler to use as it tends to be fairly good at converging in a efficient manner it can often be tricky to set the correct parameters for this sampler however and the nuts sampler is often easier to run if you don t want to spend too much time fiddling with step size and and the number of steps to take c sample model hmc 0 01 10 1000 plot sampler c hmcdathe hmcda sampler is an implementation of the hamiltonian monte carlo with dual averaging algorithm found in the paper the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo by hoffman and gelman 2011 the paper can be found on arxiv for the interested reader c sample model hmcda 200 0 65 0 3 1000 plot sampler c mhmetropolis hastings mh sampling is one of the earliest markov chain monte carlo methods mh sampling does not move a lot unlike many of the other samplers implemented in turing typically a much longer chain is required to converge to an appropriate parameter estimate the plot below only uses 1 000 iterations of metropolis hastings c sample model mh 1000 plot sampler c as you can see the mh sampler doesn t move parameter estimates very often nutsthe no u turn sampler nuts is an implementation of the algorithm found in the paper the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo by hoffman and gelman 2011 the paper can be found on arxiv for the interested reader nuts tends to be very good at traversing the minima of complex posteriors quickly c sample model nuts 0 65 1000 plot sampler c the only parameter that needs to be set other than the number of iterations to run is the target acceptance rate in the hoffman and gelman paper they note that a target acceptance rate of 0 65 is typical here is a plot showing a very high acceptance rate note that it appears to stick to a locla minima and is not particularly good at exploring the posterior c sample model nuts 0 95 1000 plot sampler c an exceptionally low acceptance rate will show very few moves on the posterior c sample model nuts 0 2 1000 plot sampler c pgthe particle gibbs pg sampler is an implementation of an algorithm from the paper particle markov chain monte carlo methods by andrieu doucet and holenstein 2010 the interested reader can learn more here the two parameters are the number of particles and the number of iterations the plot below shows the use of 20 particles c sample model pg 20 1000 plot sampler c next we plot using 50 particles c sample model pg 50 1000 plot sampler c pmmh currently not supported the particle marginal metropolis hastings pmmh sampler is an implementation of an algorithm from the paper particle markov chain monte carlo methods by andrieu doucet and holenstein 2010 the interested reader can learn more here pmmh supports the use of different samplers across different parameter spaces similar to the gibbs sampler the plot below uses smc and mh c sample model pmmh 1000 smc 20 m mh 10 s plot sampler c pimh currently not supported in addition to pmmh turing also support the particle independent metropolis hastings pimh pimh accepts a number of iterations and an smc call c sample model pimh 1000 smc 20 plot sampler c sghmc currently not supported stochastic gradient hamiltonian monte carlo sghmc tends to produce sampling paths not unlike that of stochastic gradient descent in other machine learning model types it is an implementation of an algorithm in the paper stochastic gradient hamiltonian monte carlo by chen fox and guestrin 2014 the interested reader can learn more here this sampler is very similar to the sgld sampler below the two parameters used in sghmc are the learing rate and the momentum decay here is sampler with a higher momentum decay of 0 1 c sample model sghmc 1000 0 001 0 1 plot sampler c and the same sampler with a much lower momentum decay c sample model sghmc 1000 0 001 0 01 plot sampler c sgld currently not supported the stochastic gradient langevin dynamics sgld is based on the paper bayesian learning via stochastic gradient langevin dynamics by welling and teh 2011 a link to the article can be found here sgld is an approximation to langevin adjusted mh sgld uses stochastic gradients that are based on mini batches of data and it skips the mh correction step to improve scalability computing metropolis hastings accept probabilities requires evaluation likelihoods for the full dataset making it significantly less scalable the resulting gibbs sampler is no longer unbiased since sgld is an approximate sampler c sample model sgld 1000 0 01 plot sampler c", "title": "Sampler Visualization"}]}

<!DOCTYPE html><HTML><head><script charset="utf-8" src="../../../../assets/__default/flexsearch.bundle.js" type="text/javascript"></script><script charset="utf-8" src="../../../../assets/__default/flexsearch_integration.js" type="text/javascript"></script><meta charset="utf-8"/><meta content="IE=edge" http-equiv="X-UA-Compatible"/><meta content="en" http-equiv="content-language"/><meta content="width=device-width, initial-scale=1" name="viewport"/><title>Linear Regression</title><meta content="Linear Regression" name="description"/><meta content="The Turing Team" name="author"/><meta content="red" name="theme-color"/><link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/><link href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono" rel="stylesheet"/><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet"/><link href="/v0.7.1/tutorials/5-linearregression/" rel="canonical"/><link href="/v0.7.1/feed.xml" rel="alternate" title="Turing.jl" type="application/rss+xml"/><meta content="Copy to clipboard" name="lang:clipboard.copy"/><meta content="Copied to clipboard" name="lang:clipboard.copied"/><meta content="en" name="lang:search.language"/><meta content="True" name="lang:search.pipeline.stopwords"/><meta content="True" name="lang:search.pipeline.trimmer"/><meta content="No matching documents" name="lang:search.result.none"/><meta content="1 matching document" name="lang:search.result.one"/><meta content="# matching documents" name="lang:search.result.other"/><meta content="[\s\-]+" name="lang:search.tokenizer"/><script src="/versions.js"></script><script src="/v0.7.1/assets/js/modernizr.74668098.js"></script><link href="/v0.7.1/assets/img/favicon.ico" rel="shortcut icon"/><link href="/v0.7.1/assets/css/main.css" rel="stylesheet"/><link href="/v0.7.1/assets/css/palette.css" rel="stylesheet"/><link href="/v0.7.1/assets/css/header.css" rel="stylesheet"/><link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet"/><link href="../../../../assets/__default/multidoc.css" rel="stylesheet" type="text/css"/><link href="../../../../assets/__default/flexsearch.css" rel="stylesheet" type="text/css"/></head><body data-md-color-accent="red" data-md-color-primary="red" dir="ltr"><nav id="multi-page-nav"><div class="hidden-on-mobile" id="nav-items"><a class="nav-link active nav-item" href="../../../">Turing</a><a class="nav-link nav-item" href="../../../../ppl/">DynamicPPL</a><a class="nav-link nav-item" href="../../../../hmc/">AdvancedHMC</a><a class="nav-link nav-item" href="../../../../ns/">NestedSamplers</a><a class="nav-link nav-item" href="../../../../mcmcc/">MCMCChains</a><div class="search nav-item"><input id="search-input" placeholder="Search..."/><ul class="suggestions hidden" id="search-result-container"></ul><div class="search-keybinding">/</div></div></div><a id="multidoc-toggler"></a></nav><svg class="md-svg"><defs><svg><path d="M160 304q0 10-3.125 20.5t-10.75 19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM320 304q0 10-3.125 20.5t-10.75 19-18.125 8.5-18.125-8.5-10.75-19-3.125-20.5 3.125-20.5 10.75-19 18.125-8.5 18.125 8.5 10.75 19 3.125 20.5zM360 304q0-30-17.25-51t-46.75-21q-10.25 0-48.75 5.25-17.75 2.75-39.25 2.75t-39.25-2.75q-38-5.25-48.75-5.25-29.5 0-46.75 21t-17.25 51q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75 8-38.375zM416 260q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5-41.75 1.125q-19.5 0-35.5-0.75t-36.875-3.125-38.125-7.5-34.25-12.875-30.25-20.25-21.5-28.75q-15.5-30.75-15.5-82.75 0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875q36.75-8.75 77.25-8.75 37 0 70 8 26.25-20.5 46.75-30.25t47.25-9.75q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z" fill="currentColor"></path></svg></defs></svg><input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/><input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/><label class="md-overlay" data-md-component="overlay" for="__drawer"></label><a class="md-skip" href="#linear-regression" tabindex="1"> Skip to content </a><header class="md-header" data-md-component="header" data-md-state="none"><nav class="md-header-nav md-grid"><div class="md-flex"><div class="md-flex__cell md-flex__cell--shrink"></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label></div><div class="md-flex__cell md-flex__cell--shrink"><a class="md-header-nav__button md-logo" href="/v0.7.1/" title="Turing.jl"><div class="md-flex__ellipsis md-header-nav__title" data-md-component="title"><span class="md-header-nav__topic">Turing.jl</span></div></a></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><div class="dropdown version-switch"><a aria-expanded="false" aria-haspopup="true" class="dropdown-toggle" data-toggle="dropdown" href="#" id="navbarDropdown" role="button"></a><div class="dropdown-menu"></div></div></div></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><a class="md-source" href="/v0.7.1/docs/using-turing/get-started" title="Go to Get Started">
                      Get Started
                    </a></div></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><a class="md-source" href="/v0.7.1/docs/using-turing/" title="Go to Documentation">
                        Documentation
                      </a></div></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><a class="md-source" href="/v0.7.1/tutorials/" title="Go to Tutorial">
                          Tutorials
                        </a></div></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><a class="md-source" data-md-source="github" href="https://github.com/TuringLang/Turing.jl" title="Go to repository"><div class="md-source__icon" style="padding-top:5px"><i class="fa fa-github fa-3x"></i></div><div class="md-source__repository">
                      TuringLang/Turing.jl
                    </div></a></div></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><a class="twitter-follow-button" data-show-count="false" data-show-screen-name="false" data-size="large" href="https://twitter.com/TuringLang?ref_src=twsrc%5Etfw">Follow @TuringLang</a><script async="" charset="utf-8" src="https://platform.twitter.com/widgets.js"></script></div></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--search md-header-nav__button" for="__search"></label><div class="md-search" data-md-component="search" role="dialog"><label class="md-search__overlay" for="__search"></label><div class="md-search__inner" role="search"><form class="md-search__form" name="search"><input autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="query" data-md-state="active" name="query" placeholder="Search" spellcheck="false" type="text"/><label class="md-icon md-search__icon" for="__search"></label><button class="md-icon md-search__icon" data-md-component="reset" tabindex="-1" type="reset"> Óóç </button></form><div class="md-search__output"><div class="md-search__scrollwrap" data-md-scrollfix=""><div class="md-search-result" data-md-component="result"><div class="md-search-result__meta"> Type to start searching </div><ol class="md-search-result__list"></ol></div></div></div></div></div></div></div></nav></header><div class="md-container"><main class="md-main"><div class="md-main__inner md-grid full-width" data-md-component="container"><div class="md-sidebar md-sidebar--primary" data-md-component="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--primary" data-md-level="0"><label class="md-nav__title md-nav__title--site"><a class="" href="/v0.7.1/" title="Turing.jl"><span class="md-nav__button md-logo">
                Turing.jl
              </span></a></label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item md-nav__item--active"><input class="md-toggle md-nav__toggle" data-md-toggle="toc" id="__toc" type="checkbox"/><nav class="md-nav md-nav--secondary"><a class="" href="/v0.7.1/" title="Turing.jl"><label class="md-nav__title md-nav__title--site"><span class="md-nav__button md-logo">
                      Turing.jl
                    </span></label></a><div class="md-nav__source"><a class="md-source" data-md-source="github" href="https://github.com/TuringLang/Turing.jl" title="Go to repository"><span class="md-source__icon"><i class="fa fa-github fa-3x"></i></span><span class="md-source__repository">
                      TuringLang/Turing.jl
                    </span></a></div><div class="md-nav__dropdown"><select id="version-selector"></select></div><label class="md-nav__title" for="__drawer"></label><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item md-nav__item--nested navbar_bottom-border"><a class="md-nav__link pancakes-parent-mobile" id="pancakes-using-turing" title="USING TURING">USING TURING</a><nav class="md-nav md-nav--secondary"><a class="md-nav__title mobile-navbar-back" for="__toc">Table of contents</a><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/docs/using-turing/get-started" id="pancakes-getting-started" title="Getting Started">Getting Started</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/docs/using-turing/quick-start" id="pancakes-quick-start" title="Quick Start">Quick Start</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/docs/using-turing/guide" id="pancakes-guide" title="Guide">Guide</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/docs/using-turing/advanced" id="pancakes-advanced-usage" title="Advanced Usage">Advanced Usage</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/docs/using-turing/autodiff" id="pancakes-automatic-differentiation" title="Automatic Differentiation">Automatic Differentiation</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/docs/using-turing/dynamichmc" id="pancakes-using-dynamichmc" title="Using DynamicHMC">Using DynamicHMC</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/docs/using-turing/sampler-viz" id="pancakes-sampler-visualization" title="Sampler Visualization">Sampler Visualization</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested navbar_bottom-border"><a class="md-nav__link pancakes-parent-mobile" id="pancakes-tutorials" title="TUTORIALS">TUTORIALS</a><nav class="md-nav md-nav--secondary"><a class="md-nav__title mobile-navbar-back" for="__toc">Table of contents</a><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/tutorials" id="pancakes-home" title="Home">Home</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/tutorials/0-introduction" id="pancakes-introduction-to-turing" title="Introduction to Turing">Introduction to Turing</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/tutorials/1-gaussianmixturemodel" id="pancakes-gaussian-mixture-models" title="Gaussian Mixture Models">Gaussian Mixture Models</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/tutorials/2-logisticregression" id="pancakes-bayesian-logistic-regression" title="Bayesian Logistic Regression">Bayesian Logistic Regression</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/tutorials/3-bayesnn" id="pancakes-bayesian-neural-networks" title="Bayesian Neural Networks">Bayesian Neural Networks</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/tutorials/4-bayeshmm" id="pancakes-hidden-markov-models" title="Hidden Markov Models">Hidden Markov Models</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/tutorials/5-linearregression" id="pancakes-linear-regression" title="Linear Regression">Linear Regression</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/tutorials/6-infinitemixturemodel" id="pancakes-infinite-mixture-models" title="Infinite Mixture Models">Infinite Mixture Models</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/tutorials/7-poissonregression" id="pancakes-bayesian-poisson-regression" title="Bayesian Poisson Regression">Bayesian Poisson Regression</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested navbar_bottom-border"><a class="md-nav__link pancakes-parent-mobile" id="pancakes-api" title="API">API</a><nav class="md-nav md-nav--secondary"><a class="md-nav__title mobile-navbar-back" for="__toc">Table of contents</a><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/docs/library" id="pancakes-public" title="Public">Public</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested navbar_bottom-border"><a class="md-nav__link pancakes-parent-mobile" id="pancakes-contributing" title="CONTRIBUTING">CONTRIBUTING</a><nav class="md-nav md-nav--secondary"><a class="md-nav__title mobile-navbar-back" for="__toc">Table of contents</a><ul class="md-nav__list" data-md-scrollfix=""><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/docs/contributing/guide" id="pancakes-how-to-contribute" title="How to Contribute">How to Contribute</a></li><li class="md-nav__item navbar_bottom-border"><a class="md-nav__link" href="/v0.7.1/docs/contributing/style-guide" id="pancakes-style-guide" title="Style Guide">Style Guide</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item mobile-nav" style="display:none"><a class="md-nav__link" title="USING TURING">USING TURING</a></li><li class="md-nav__item mobile-nav" style="display:none"><a class="md-nav__link" title="TUTORIALS">TUTORIALS</a></li><li class="md-nav__item mobile-nav" style="display:none"><a class="md-nav__link" title="API">API</a></li><li class="md-nav__item mobile-nav" style="display:none"><a class="md-nav__link" title="CONTRIBUTING">CONTRIBUTING</a></li><li class="md-nav__item md-nav__item--nested not-mobile-nav invisible"><a class="md-nav__link pancakes-parent " id="pancakes-using-turing" title="USING TURING">USING TURING</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/docs/using-turing/get-started" style="display:none;" title="Getting Started">Getting Started</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/docs/using-turing/quick-start" style="display:none;" title="Quick Start">Quick Start</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/docs/using-turing/guide" style="display:none;" title="Guide">Guide</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/docs/using-turing/advanced" style="display:none;" title="Advanced Usage">Advanced Usage</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/docs/using-turing/autodiff" style="display:none;" title="Automatic Differentiation">Automatic Differentiation</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/docs/using-turing/dynamichmc" style="display:none;" title="Using DynamicHMC">Using DynamicHMC</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/docs/using-turing/sampler-viz" style="display:none;" title="Sampler Visualization">Sampler Visualization</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested not-mobile-nav invisible"><a class="md-nav__link pancakes-parent open-parent" id="pancakes-tutorials" title="TUTORIALS">TUTORIALS</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/tutorials" title="Home">Home</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/tutorials/0-introduction" title="Introduction to Turing">Introduction to Turing</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/tutorials/1-gaussianmixturemodel" title="Gaussian Mixture Models">Gaussian Mixture Models</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/tutorials/2-logisticregression" title="Bayesian Logistic Regression">Bayesian Logistic Regression</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/tutorials/3-bayesnn" title="Bayesian Neural Networks">Bayesian Neural Networks</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/tutorials/4-bayeshmm" title="Hidden Markov Models">Hidden Markov Models</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/tutorials/5-linearregression" style="color: red;" title="Linear Regression">Linear Regression</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/tutorials/6-infinitemixturemodel" title="Infinite Mixture Models">Infinite Mixture Models</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/tutorials/7-poissonregression" title="Bayesian Poisson Regression">Bayesian Poisson Regression</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested not-mobile-nav invisible"><a class="md-nav__link pancakes-parent " id="pancakes-api" title="API">API</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/docs/library" style="display:none;" title="Public">Public</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested not-mobile-nav invisible"><a class="md-nav__link pancakes-parent " id="pancakes-contributing" title="CONTRIBUTING">CONTRIBUTING</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/docs/contributing/guide" style="display:none;" title="How to Contribute">How to Contribute</a></li><li class="md-nav__item"><a class="md-nav__link pancakes-child" href="/v0.7.1/docs/contributing/style-guide" style="display:none;" title="Style Guide">Style Guide</a></li></ul></nav></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary invisible" data-md-component="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">Table of contents</label><ul class="md-nav__list" data-md-scrollfix="" id="nav-toc"></ul><a class="social-link" href="https://twitter.com/intent/tweet?original_referer=https://github.com/TuringLang/Turing.jl/edit/master/docs/src_tutorials/5_LinearRegression.md" title="Tweet this page"><i class="fa fa-twitter fa-1x"></i>Tweet this page</a><a class="social-link" href="https://discourse.julialang.org/c/domain/probprog" title="Ask questions"><i class="fa fa-stack-overflow fa-1x"></i>Ask questions</a><a class="social-link" href="https://github.com/TuringLang/Turing.jl//issues/new?label=question&amp;title=Question:&amp;body=Question%20on:%20https://github.com/TuringLang/Turing.jl/edit/master/docs/_tutorials/5_LinearRegression.md" title="Report issues"><i class="fa fa-comments fa-1x"></i>Report issues</a><a class="social-link" href="https://github.com/TuringLang/Turing.jl/edit/master/docs/src_tutorials/5_LinearRegression.md" title="Edit this page on github"><i class="fa fa-github fa-1x"></i> Edit me</a></nav></div></div></div><div id="md-container-pancakes"><div class="md-content full-width"><article class="md-content__inner md-typeset  full-width"><h1 id="linear-regression">Linear Regression</h1><p>Turing is powerful when applied to complex hierarchical models, but it can also be put to task at common statistical procedures, like <a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a>. This tutorial covers how to implement a linear regression model in Turing.</p><h2 id="set-up">Set Up</h2><p>We begin by importing all the necessary libraries.</p><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Import Turing and Distributions.</span><span class="k">using</span><span class="n">Turing</span><span class="x">,</span><span class="n">Distributions</span><span class="c"># Import RDatasets.</span><span class="k">using</span><span class="n">RDatasets</span><span class="c"># Import MCMCChains, Plots, and StatPlots for visualizations and diagnostics.</span><span class="k">using</span><span class="n">MCMCChains</span><span class="x">,</span><span class="n">Plots</span><span class="x">,</span><span class="n">StatsPlots</span><span class="c"># Set a seed for reproducibility.</span><span class="k">using</span><span class="n">Random</span><span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="x">(</span><span class="mi">0</span><span class="x">);</span><span class="c"># Hide the progress prompt while sampling.</span><span class="n">Turing</span><span class="o">.</span><span class="n">turnprogress</span><span class="x">(</span><span class="nb">false</span><span class="x">);</span></code></pre></div></div><p>We will use the <code class="language-plaintext highlighter-rouge">mtcars</code> dataset from the <a href="https://github.com/johnmyleswhite/RDatasets.jl">RDatasets</a> package. <code class="language-plaintext highlighter-rouge">mtcars</code> contains a variety of statistics on different car models, including their miles per gallon, number of cylinders, and horsepower, among others.</p><p>We want to know if we can construct a Bayesian linear regression model to predict the miles per gallon of a car, given the other statistics it has. Lets take a look at the data we have.</p><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Import the "Default" dataset.</span><span class="n">data</span><span class="o">=</span><span class="n">RDatasets</span><span class="o">.</span><span class="n">dataset</span><span class="x">(</span><span class="s">"datasets"</span><span class="x">,</span><span class="s">"mtcars"</span><span class="x">);</span><span class="c"># Show the first six rows of the dataset.</span><span class="n">first</span><span class="x">(</span><span class="n">data</span><span class="x">,</span><span class="mi">6</span><span class="x">)</span></code></pre></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>6√ó12 DataFrame. Omitted printing of 6 columns
‚îÇ Row ‚îÇ Model             ‚îÇ MPG      ‚îÇ Cyl    ‚îÇ Disp     ‚îÇ HP     ‚îÇ DRat   
  ‚îÇ
‚îÇ     ‚îÇ String‚ç∞           ‚îÇ Float64‚ç∞ ‚îÇ Int64‚ç∞ ‚îÇ Float64‚ç∞ ‚îÇ Int64‚ç∞ ‚îÇ Float64
‚ç∞ ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÄ‚îÄ‚î§
‚îÇ 1   ‚îÇ Mazda RX4         ‚îÇ 21.0     ‚îÇ 6      ‚îÇ 160.0    ‚îÇ 110    ‚îÇ 3.9    
  ‚îÇ
‚îÇ 2   ‚îÇ Mazda RX4 Wag     ‚îÇ 21.0     ‚îÇ 6      ‚îÇ 160.0    ‚îÇ 110    ‚îÇ 3.9    
  ‚îÇ
‚îÇ 3   ‚îÇ Datsun 710        ‚îÇ 22.8     ‚îÇ 4      ‚îÇ 108.0    ‚îÇ 93     ‚îÇ 3.85   
  ‚îÇ
‚îÇ 4   ‚îÇ Hornet 4 Drive    ‚îÇ 21.4     ‚îÇ 6      ‚îÇ 258.0    ‚îÇ 110    ‚îÇ 3.08   
  ‚îÇ
‚îÇ 5   ‚îÇ Hornet Sportabout ‚îÇ 18.7     ‚îÇ 8      ‚îÇ 360.0    ‚îÇ 175    ‚îÇ 3.15   
  ‚îÇ
‚îÇ 6   ‚îÇ Valiant           ‚îÇ 18.1     ‚îÇ 6      ‚îÇ 225.0    ‚îÇ 105    ‚îÇ 2.76   
  ‚îÇ
</code></pre></div></div><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">size</span><span class="x">(</span><span class="n">data</span><span class="x">)</span></code></pre></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(32, 12)
</code></pre></div></div><p>The next step is to get our data ready for testing. We‚Äôll split the <code class="language-plaintext highlighter-rouge">mtcars</code> dataset into two subsets, one for training our model and one for evaluating our model. Then, we separate the labels we want to learn (<code class="language-plaintext highlighter-rouge">MPG</code>, in this case) and standardize the datasets by subtracting each column‚Äôs means and dividing by the standard deviation of that column.</p><p>The resulting data is not very familiar looking, but this standardization process helps the sampler converge far easier. We also create a function called <code class="language-plaintext highlighter-rouge">unstandardize</code>, which returns the standardized values to their original form. We will use this function later on when we make predictions.</p><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Function to split samples.</span><span class="k">function</span><span class="nf"> split_data</span><span class="x">(</span><span class="n">df</span><span class="x">,</span><span class="n">at</span><span class="o">=</span><span class="mf">0.70</span><span class="x">)</span><span class="x">(</span><span class="n">r</span><span class="x">,</span><span class="n">_</span><span class="x">)</span><span class="o">=</span><span class="n">size</span><span class="x">(</span><span class="n">df</span><span class="x">)</span><span class="n">index</span><span class="o">=</span><span class="kt">Int</span><span class="x">(</span><span class="n">round</span><span class="x">(</span><span class="n">r</span><span class="o">*</span><span class="n">at</span><span class="x">))</span><span class="n">train</span><span class="o">=</span><span class="n">df</span><span class="x">[</span><span class="mi">1</span><span class="o">:</span><span class="n">index</span><span class="x">,</span><span class="o">:</span><span class="x">]</span><span class="n">test</span><span class="o">=</span><span class="n">df</span><span class="x">[(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="x">)</span><span class="o">:</span><span class="k">end</span><span class="x">,</span><span class="o">:</span><span class="x">]</span><span class="k">return</span><span class="n">train</span><span class="x">,</span><span class="n">test</span><span class="k">end</span><span class="c"># Split our dataset 70%/30% into training/test sets.</span><span class="n">train</span><span class="x">,</span><span class="n">test</span><span class="o">=</span><span class="n">split_data</span><span class="x">(</span><span class="n">data</span><span class="x">,</span><span class="mf">0.7</span><span class="x">)</span><span class="c"># Save dataframe versions of our dataset.</span><span class="n">train_cut</span><span class="o">=</span><span class="n">DataFrame</span><span class="x">(</span><span class="n">train</span><span class="x">)</span><span class="n">test_cut</span><span class="o">=</span><span class="n">DataFrame</span><span class="x">(</span><span class="n">test</span><span class="x">)</span><span class="c"># Create our labels. These are the values we are trying to predict.</span><span class="n">train_label</span><span class="o">=</span><span class="n">train</span><span class="x">[</span><span class="o">:</span><span class="x">,</span><span class="o">:</span><span class="n">MPG</span><span class="x">]</span><span class="n">test_label</span><span class="o">=</span><span class="n">test</span><span class="x">[</span><span class="o">:</span><span class="x">,</span><span class="o">:</span><span class="n">MPG</span><span class="x">]</span><span class="c"># Get the list of columns to keep.</span><span class="n">remove_names</span><span class="o">=</span><span class="n">filter</span><span class="x">(</span><span class="n">x</span><span class="o">-&gt;!</span><span class="k">in</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="x">[</span><span class="o">:</span><span class="n">MPG</span><span class="x">,</span><span class="o">:</span><span class="n">Model</span><span class="x">]),</span><span class="n">names</span><span class="x">(</span><span class="n">data</span><span class="x">))</span><span class="c"># Filter the test and train sets.</span><span class="n">train</span><span class="o">=</span><span class="kt">Matrix</span><span class="x">(</span><span class="n">train</span><span class="x">[</span><span class="o">:</span><span class="x">,</span><span class="n">remove_names</span><span class="x">]);</span><span class="n">test</span><span class="o">=</span><span class="kt">Matrix</span><span class="x">(</span><span class="n">test</span><span class="x">[</span><span class="o">:</span><span class="x">,</span><span class="n">remove_names</span><span class="x">]);</span><span class="c"># A handy helper function to rescale our dataset.</span><span class="k">function</span><span class="nf"> standardize</span><span class="x">(</span><span class="n">x</span><span class="x">)</span><span class="k">return</span><span class="x">(</span><span class="n">x</span><span class="o">.-</span><span class="n">mean</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">dims</span><span class="o">=</span><span class="mi">1</span><span class="x">))</span><span class="o">./</span><span class="n">std</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">dims</span><span class="o">=</span><span class="mi">1</span><span class="x">),</span><span class="n">x</span><span class="k">end</span><span class="c"># Another helper function to unstandardize our datasets.</span><span class="k">function</span><span class="nf"> unstandardize</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">orig</span><span class="x">)</span><span class="k">return</span><span class="n">x</span><span class="o">.*</span><span class="n">std</span><span class="x">(</span><span class="n">orig</span><span class="x">,</span><span class="n">dims</span><span class="o">=</span><span class="mi">1</span><span class="x">)</span><span class="o">.+</span><span class="n">mean</span><span class="x">(</span><span class="n">orig</span><span class="x">,</span><span class="n">dims</span><span class="o">=</span><span class="mi">1</span><span class="x">)</span><span class="k">end</span><span class="c"># Standardize our dataset.</span><span class="x">(</span><span class="n">train</span><span class="x">,</span><span class="n">train_orig</span><span class="x">)</span><span class="o">=</span><span class="n">standardize</span><span class="x">(</span><span class="n">train</span><span class="x">)</span><span class="x">(</span><span class="n">test</span><span class="x">,</span><span class="n">test_orig</span><span class="x">)</span><span class="o">=</span><span class="n">standardize</span><span class="x">(</span><span class="n">test</span><span class="x">)</span><span class="x">(</span><span class="n">train_label</span><span class="x">,</span><span class="n">train_l_orig</span><span class="x">)</span><span class="o">=</span><span class="n">standardize</span><span class="x">(</span><span class="n">train_label</span><span class="x">)</span><span class="x">(</span><span class="n">test_label</span><span class="x">,</span><span class="n">test_l_orig</span><span class="x">)</span><span class="o">=</span><span class="n">standardize</span><span class="x">(</span><span class="n">test_label</span><span class="x">);</span></code></pre></div></div><h2 id="model-specification">Model Specification</h2><p>In a traditional frequentist model using <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">OLS</a>, our model might look like:</p><p>$$
MPG_i = \alpha + \boldsymbol{\beta}^T\boldsymbol{X_i}
$$</p><p>where <script type="math/tex">\boldsymbol{\beta}</script> is a vector of coefficients and <script type="math/tex">\boldsymbol{X}</script> is a vector of inputs for observation <script type="math/tex">i</script>. The Bayesian model we are more concerned with is the following:</p><p>$$
MPG_i \sim \mathcal{N}(\alpha + \boldsymbol{\beta}^T\boldsymbol{X_i}, \sigma^2)
$$</p><p>where <script type="math/tex">\alpha</script> is an intercept term common to all observations, <script type="math/tex">\boldsymbol{\beta}</script> is a coefficient vector, <script type="math/tex">\boldsymbol{X_i}</script> is the observed data for car <script type="math/tex">i</script>, and <script type="math/tex">\sigma^2</script> is a common variance term.</p><p>For <script type="math/tex">\sigma^2</script>, we assign a prior of <code class="language-plaintext highlighter-rouge">TruncatedNormal(0,100,0,Inf)</code>. This is consistent with <a href="http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf">Andrew Gelman‚Äôs recommendations</a> on noninformative priors for variance. The intercept term (<script type="math/tex">\alpha</script>) is assumed to be normally distributed with a mean of zero and a variance of three. This represents our assumptions that miles per gallon can be explained mostly by our assorted variables, but a high variance term indicates our uncertainty about that. Each coefficient is assumed to be normally distributed with a mean of zero and a variance of 10. We do not know that our coefficients are different from zero, and we don‚Äôt know which ones are likely to be the most important, so the variance term is quite high. Lastly, each observation <script type="math/tex">y_i</script> is distributed according to the calculated <code class="language-plaintext highlighter-rouge">mu</code> term given by <script type="math/tex">\alpha + \boldsymbol{\beta}^T\boldsymbol{X_i}</script>.</p><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Bayesian linear regression.</span><span class="nd">@model</span><span class="n">linear_regression</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">y</span><span class="x">,</span><span class="n">n_obs</span><span class="x">,</span><span class="n">n_vars</span><span class="x">)</span><span class="o">=</span><span class="k">begin</span><span class="c"># Set variance prior.</span><span class="n">œÉ‚ÇÇ</span><span class="o">~</span><span class="n">TruncatedNormal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span><span class="mi">100</span><span class="x">,</span><span class="mi">0</span><span class="x">,</span><span class="nb">Inf</span><span class="x">)</span><span class="c"># Set intercept prior.</span><span class="n">intercept</span><span class="o">~</span><span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span><span class="mi">3</span><span class="x">)</span><span class="c"># Set the priors on our coefficients.</span><span class="n">coefficients</span><span class="o">=</span><span class="kt">Array</span><span class="x">{</span><span class="kt">Real</span><span class="x">}(</span><span class="nb">undef</span><span class="x">,</span><span class="n">n_vars</span><span class="x">)</span><span class="n">coefficients</span><span class="o">~</span><span class="x">[</span><span class="n">Normal</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span><span class="mi">10</span><span class="x">)]</span><span class="c"># Calculate all the mu terms.</span><span class="n">mu</span><span class="o">=</span><span class="n">intercept</span><span class="o">.+</span><span class="n">x</span><span class="o">*</span><span class="n">coefficients</span><span class="k">for</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="o">:</span><span class="n">n_obs</span><span class="n">y</span><span class="x">[</span><span class="n">i</span><span class="x">]</span><span class="o">~</span><span class="n">Normal</span><span class="x">(</span><span class="n">mu</span><span class="x">[</span><span class="n">i</span><span class="x">],</span><span class="n">œÉ‚ÇÇ</span><span class="x">)</span><span class="k">end</span><span class="k">end</span><span class="x">;</span></code></pre></div></div><p>With our model specified, we can call the sampler. We will use the No U-Turn Sampler (<a href="http://turing.ml/docs/library/#-turingnuts--type">NUTS</a>) here.</p><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_obs</span><span class="x">,</span><span class="n">n_vars</span><span class="o">=</span><span class="n">size</span><span class="x">(</span><span class="n">train</span><span class="x">)</span><span class="n">model</span><span class="o">=</span><span class="n">linear_regression</span><span class="x">(</span><span class="n">train</span><span class="x">,</span><span class="n">train_label</span><span class="x">,</span><span class="n">n_obs</span><span class="x">,</span><span class="n">n_vars</span><span class="x">)</span><span class="n">chain</span><span class="o">=</span><span class="n">sample</span><span class="x">(</span><span class="n">model</span><span class="x">,</span><span class="n">NUTS</span><span class="x">(</span><span class="mi">1500</span><span class="x">,</span><span class="mi">200</span><span class="x">,</span><span class="mf">0.65</span><span class="x">));</span></code></pre></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[NUTS] Finished with
  Running time        = 28.76012935999999;
  #lf / sample        = 0.0;
  #evals / sample     = 0.0006666666666666666;
  pre-cond. metric    = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,....
</code></pre></div></div><p>As a visual check to confirm that our coefficients have converged, we show the densities and trace plots for our parameters using the <code class="language-plaintext highlighter-rouge">plot</code> functionality.</p><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="x">(</span><span class="n">chain</span><span class="x">)</span></code></pre></div></div><p><img alt="" src="../figures/5_LinearRegression_7_1.png"/></p><p>It looks like each of our parameters has converged. We can check our numerical esimates using <code class="language-plaintext highlighter-rouge">describe(chain)</code>, as below.</p><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">describe</span><span class="x">(</span><span class="n">chain</span><span class="x">)</span></code></pre></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2-element Array{ChainDataFrame,1}

Summary Statistics
. Omitted printing of 1 columns
‚îÇ Row ‚îÇ parameters       ‚îÇ mean       ‚îÇ std       ‚îÇ naive_se   ‚îÇ mcse      
 ‚îÇ
‚îÇ     ‚îÇ Symbol           ‚îÇ Float64    ‚îÇ Float64   ‚îÇ Float64    ‚îÇ Float64   
 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÄ‚î§
‚îÇ 1   ‚îÇ coefficients[1]  ‚îÇ 0.374513   ‚îÇ 0.44485   ‚îÇ 0.011486   ‚îÇ 0.0227265 
 ‚îÇ
‚îÇ 2   ‚îÇ coefficients[2]  ‚îÇ -0.171117  ‚îÇ 0.476053  ‚îÇ 0.0122916  ‚îÇ 0.0225355 
 ‚îÇ
‚îÇ 3   ‚îÇ coefficients[3]  ‚îÇ -0.0681829 ‚îÇ 0.356122  ‚îÇ 0.00919503 ‚îÇ 0.0163717 
 ‚îÇ
‚îÇ 4   ‚îÇ coefficients[4]  ‚îÇ 0.66256    ‚îÇ 0.33855   ‚îÇ 0.00874132 ‚îÇ 0.0141081 
 ‚îÇ
‚îÇ 5   ‚îÇ coefficients[5]  ‚îÇ 0.0969497  ‚îÇ 0.483806  ‚îÇ 0.0124918  ‚îÇ 0.0278113 
 ‚îÇ
‚îÇ 6   ‚îÇ coefficients[6]  ‚îÇ 0.0400533  ‚îÇ 0.272691  ‚îÇ 0.00704085 ‚îÇ 0.016834  
 ‚îÇ
‚îÇ 7   ‚îÇ coefficients[7]  ‚îÇ -0.0995777 ‚îÇ 0.295442  ‚îÇ 0.00762827 ‚îÇ 0.0135998 
 ‚îÇ
‚îÇ 8   ‚îÇ coefficients[8]  ‚îÇ 0.10959    ‚îÇ 0.313314  ‚îÇ 0.00808972 ‚îÇ 0.0171665 
 ‚îÇ
‚îÇ 9   ‚îÇ coefficients[9]  ‚îÇ 0.200219   ‚îÇ 0.329276  ‚îÇ 0.00850186 ‚îÇ 0.0116165 
 ‚îÇ
‚îÇ 10  ‚îÇ coefficients[10] ‚îÇ -0.682739  ‚îÇ 0.361389  ‚îÇ 0.00933104 ‚îÇ 0.0179951 
 ‚îÇ
‚îÇ 11  ‚îÇ intercept        ‚îÇ 0.0108571  ‚îÇ 0.170723  ‚îÇ 0.00440804 ‚îÇ 0.0106107 
 ‚îÇ
‚îÇ 12  ‚îÇ lf_eps           ‚îÇ 0.0581085  ‚îÇ 0.0402024 ‚îÇ 0.00103802 ‚îÇ 0.00132349
 ‚îÇ
‚îÇ 13  ‚îÇ œÉ‚ÇÇ               ‚îÇ 0.484513   ‚îÇ 0.492571  ‚îÇ 0.0127181  ‚îÇ 0.036488  
 ‚îÇ

Quantiles
. Omitted printing of 1 columns
‚îÇ Row ‚îÇ parameters       ‚îÇ 2.5%      ‚îÇ 25.0%       ‚îÇ 50.0%      ‚îÇ 75.0%    
 ‚îÇ
‚îÇ     ‚îÇ Symbol           ‚îÇ Float64   ‚îÇ Float64     ‚îÇ Float64    ‚îÇ Float64  
 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÄ‚î§
‚îÇ 1   ‚îÇ coefficients[1]  ‚îÇ -0.497325 ‚îÇ 0.106879    ‚îÇ 0.367559   ‚îÇ 0.650437 
 ‚îÇ
‚îÇ 2   ‚îÇ coefficients[2]  ‚îÇ -1.08863  ‚îÇ -0.444431   ‚îÇ -0.174282  ‚îÇ 0.101657 
 ‚îÇ
‚îÇ 3   ‚îÇ coefficients[3]  ‚îÇ -0.808397 ‚îÇ -0.294186   ‚îÇ -0.0607567 ‚îÇ 0.176866 
 ‚îÇ
‚îÇ 4   ‚îÇ coefficients[4]  ‚îÇ 0.028891  ‚îÇ 0.453163    ‚îÇ 0.669321   ‚îÇ 0.847996 
 ‚îÇ
‚îÇ 5   ‚îÇ coefficients[5]  ‚îÇ -0.848829 ‚îÇ -0.197623   ‚îÇ 0.0904946  ‚îÇ 0.384393 
 ‚îÇ
‚îÇ 6   ‚îÇ coefficients[6]  ‚îÇ -0.495648 ‚îÇ -0.128853   ‚îÇ 0.0474724  ‚îÇ 0.200374 
 ‚îÇ
‚îÇ 7   ‚îÇ coefficients[7]  ‚îÇ -0.662909 ‚îÇ -0.268329   ‚îÇ -0.109192  ‚îÇ 0.0712903
 ‚îÇ
‚îÇ 8   ‚îÇ coefficients[8]  ‚îÇ -0.421245 ‚îÇ -0.053784   ‚îÇ 0.105746   ‚îÇ 0.24969  
 ‚îÇ
‚îÇ 9   ‚îÇ coefficients[9]  ‚îÇ -0.438313 ‚îÇ -0.00346737 ‚îÇ 0.20142    ‚îÇ 0.408158 
 ‚îÇ
‚îÇ 10  ‚îÇ coefficients[10] ‚îÇ -1.38271  ‚îÇ -0.88346    ‚îÇ -0.679579  ‚îÇ -0.460576
 ‚îÇ
‚îÇ 11  ‚îÇ intercept        ‚îÇ -0.192764 ‚îÇ -0.0576108  ‚îÇ 0.00142006 ‚îÇ 0.0631787
 ‚îÇ
‚îÇ 12  ‚îÇ lf_eps           ‚îÇ 0.0233708 ‚îÇ 0.0564162   ‚îÇ 0.0564162  ‚îÇ 0.0564162
 ‚îÇ
‚îÇ 13  ‚îÇ œÉ‚ÇÇ               ‚îÇ 0.293726  ‚îÇ 0.369497    ‚îÇ 0.435216   ‚îÇ 0.508814 
 ‚îÇ
</code></pre></div></div><h2 id="comparing-to-ols">Comparing to OLS</h2><p>A satisfactory test of our model is to evaluate how well it predicts. Importantly, we want to compare our model to existing tools like OLS. The code below uses the <a href="">GLM.jl</a> package to generate a traditional OLS multivariate regression on the same data as our probabalistic model.</p><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Import the GLM package.</span><span class="k">using</span><span class="n">GLM</span><span class="c"># Perform multivariate OLS.</span><span class="n">ols</span><span class="o">=</span><span class="n">lm</span><span class="x">(</span><span class="nd">@formula</span><span class="x">(</span><span class="n">MPG</span><span class="o">~</span><span class="n">Cyl</span><span class="o">+</span><span class="n">Disp</span><span class="o">+</span><span class="n">HP</span><span class="o">+</span><span class="n">DRat</span><span class="o">+</span><span class="n">WT</span><span class="o">+</span><span class="n">QSec</span><span class="o">+</span><span class="n">VS</span><span class="o">+</span><span class="n">AM</span><span class="o">+</span><span class="n">Gear</span><span class="o">+</span><span class="n">Carb</span><span class="x">),</span><span class="n">train_cut</span><span class="x">)</span><span class="c"># Store our predictions in the original dataframe.</span><span class="n">train_cut</span><span class="o">.</span><span class="n">OLSPrediction</span><span class="o">=</span><span class="n">GLM</span><span class="o">.</span><span class="n">predict</span><span class="x">(</span><span class="n">ols</span><span class="x">);</span><span class="n">test_cut</span><span class="o">.</span><span class="n">OLSPrediction</span><span class="o">=</span><span class="n">GLM</span><span class="o">.</span><span class="n">predict</span><span class="x">(</span><span class="n">ols</span><span class="x">,</span><span class="n">test_cut</span><span class="x">);</span></code></pre></div></div><p>The function below accepts a chain and an input matrix and calculates predictions. We use the mean observation of each parameter in the model starting with sample 200, which is where the warm-up period for the NUTS sampler ended.</p><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Make a prediction given an input vector.</span><span class="k">function</span><span class="nf"> prediction</span><span class="x">(</span><span class="n">chain</span><span class="x">,</span><span class="n">x</span><span class="x">)</span><span class="n">p</span><span class="o">=</span><span class="n">get_params</span><span class="x">(</span><span class="n">chain</span><span class="x">[</span><span class="mi">200</span><span class="o">:</span><span class="k">end</span><span class="x">,</span><span class="o">:</span><span class="x">,</span><span class="o">:</span><span class="x">])</span><span class="n">Œ±</span><span class="o">=</span><span class="n">mean</span><span class="x">(</span><span class="n">p</span><span class="o">.</span><span class="n">intercept</span><span class="x">)</span><span class="n">Œ≤</span><span class="o">=</span><span class="n">collect</span><span class="x">(</span><span class="n">mean</span><span class="o">.</span><span class="x">(</span><span class="n">p</span><span class="o">.</span><span class="n">coefficients</span><span class="x">))</span><span class="k">return</span><span class="n">Œ±</span><span class="o">.+</span><span class="n">x</span><span class="o">*</span><span class="n">Œ≤</span><span class="k">end</span></code></pre></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prediction (generic function with 2 methods)
</code></pre></div></div><p>When we make predictions, we unstandardize them so they‚Äôre more understandable. We also add them to the original dataframes so they can be placed in context.</p><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Calculate the predictions for the training and testing sets.</span><span class="n">train_cut</span><span class="o">.</span><span class="n">BayesPredictions</span><span class="o">=</span><span class="n">unstandardize</span><span class="x">(</span><span class="n">prediction</span><span class="x">(</span><span class="n">chain</span><span class="x">,</span><span class="n">train</span><span class="x">),</span><span class="n">train_l_orig</span><span class="x">);</span><span class="n">test_cut</span><span class="o">.</span><span class="n">BayesPredictions</span><span class="o">=</span><span class="n">unstandardize</span><span class="x">(</span><span class="n">prediction</span><span class="x">(</span><span class="n">chain</span><span class="x">,</span><span class="n">test</span><span class="x">),</span><span class="n">test_l_orig</span><span class="x">);</span><span class="c"># Show the first side rows of the modified dataframe.</span><span class="n">first</span><span class="x">(</span><span class="n">test_cut</span><span class="x">,</span><span class="mi">6</span><span class="x">)</span></code></pre></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>6√ó14 DataFrame. Omitted printing of 8 columns
‚îÇ Row ‚îÇ Model            ‚îÇ MPG      ‚îÇ Cyl    ‚îÇ Disp     ‚îÇ HP     ‚îÇ DRat    
 ‚îÇ
‚îÇ     ‚îÇ String‚ç∞          ‚îÇ Float64‚ç∞ ‚îÇ Int64‚ç∞ ‚îÇ Float64‚ç∞ ‚îÇ Int64‚ç∞ ‚îÇ Float64‚ç∞
 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÄ‚î§
‚îÇ 1   ‚îÇ AMC Javelin      ‚îÇ 15.2     ‚îÇ 8      ‚îÇ 304.0    ‚îÇ 150    ‚îÇ 3.15    
 ‚îÇ
‚îÇ 2   ‚îÇ Camaro Z28       ‚îÇ 13.3     ‚îÇ 8      ‚îÇ 350.0    ‚îÇ 245    ‚îÇ 3.73    
 ‚îÇ
‚îÇ 3   ‚îÇ Pontiac Firebird ‚îÇ 19.2     ‚îÇ 8      ‚îÇ 400.0    ‚îÇ 175    ‚îÇ 3.08    
 ‚îÇ
‚îÇ 4   ‚îÇ Fiat X1-9        ‚îÇ 27.3     ‚îÇ 4      ‚îÇ 79.0     ‚îÇ 66     ‚îÇ 4.08    
 ‚îÇ
‚îÇ 5   ‚îÇ Porsche 914-2    ‚îÇ 26.0     ‚îÇ 4      ‚îÇ 120.3    ‚îÇ 91     ‚îÇ 4.43    
 ‚îÇ
‚îÇ 6   ‚îÇ Lotus Europa     ‚îÇ 30.4     ‚îÇ 4      ‚îÇ 95.1     ‚îÇ 113    ‚îÇ 3.77    
 ‚îÇ
</code></pre></div></div><p>Now let‚Äôs evaluate the loss for each method, and each prediction set. We will use sum of squared error function to evaluate loss, given by</p><p>$$
\text{SSE} = \sum{(y_i - \hat{y_i})^2}
$$</p><p>where <script type="math/tex">y_i</script> is the actual value (true MPG) and <script type="math/tex">\hat{y_i}</script> is the predicted value using either OLS or Bayesian linear regression. A lower SSE indicates a closer fit to the data.</p><div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bayes_loss1</span><span class="o">=</span><span class="n">sum</span><span class="x">((</span><span class="n">train_cut</span><span class="o">.</span><span class="n">BayesPredictions</span><span class="o">-</span><span class="n">train_cut</span><span class="o">.</span><span class="n">MPG</span><span class="x">)</span><span class="o">.^</span><span class="mi">2</span><span class="x">)</span><span class="n">ols_loss1</span><span class="o">=</span><span class="n">sum</span><span class="x">((</span><span class="n">train_cut</span><span class="o">.</span><span class="n">OLSPrediction</span><span class="o">-</span><span class="n">train_cut</span><span class="o">.</span><span class="n">MPG</span><span class="x">)</span><span class="o">.^</span><span class="mi">2</span><span class="x">)</span><span class="n">bayes_loss2</span><span class="o">=</span><span class="n">sum</span><span class="x">((</span><span class="n">test_cut</span><span class="o">.</span><span class="n">BayesPredictions</span><span class="o">-</span><span class="n">test_cut</span><span class="o">.</span><span class="n">MPG</span><span class="x">)</span><span class="o">.^</span><span class="mi">2</span><span class="x">)</span><span class="n">ols_loss2</span><span class="o">=</span><span class="n">sum</span><span class="x">((</span><span class="n">test_cut</span><span class="o">.</span><span class="n">OLSPrediction</span><span class="o">-</span><span class="n">test_cut</span><span class="o">.</span><span class="n">MPG</span><span class="x">)</span><span class="o">.^</span><span class="mi">2</span><span class="x">)</span><span class="n">println</span><span class="x">(</span><span class="s">"Training set:
    Bayes loss: </span><span class="si">$$</span><span class="s">bayes_loss1
    OLS loss: </span><span class="si">$$</span><span class="s">ols_loss1
Test set: 
    Bayes loss: </span><span class="si">$$</span><span class="s">bayes_loss2
    OLS loss: </span><span class="si">$$</span><span class="s">ols_loss2"</span><span class="x">)</span></code></pre></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training set:
    Bayes loss: 68.00979321046889
    OLS loss: 67.56037474764624
Test set: 
    Bayes loss: 242.57948201282844
    OLS loss: 270.94813070761944
</code></pre></div></div><p>As we can see above, OLS and our Bayesian model fit our training set about the same. This is to be expected, given that it is our training set. But when we look at our test set, we see that the Bayesian linear regression model is better able to predict out of sample.</p><script crossorigin="anonymous" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" src="https://code.jquery.com/jquery-3.3.1.min.js"></script><script>
$(document).ready(function() {

    var toc = $('#nav-toc');

    // Select each header
    sections = $('#md-container-pancakes h1');
        $.each(sections, function(idx, v) {
            section = $(v);
            var div_id = $(section).attr('id');
            var div_text = section.text().split('¬∂')[0];
            var parent = $("#" + div_id)
            var content = '<li id="link_' + div_id + '" class="md-nav__item"><a class="md-nav__link toc-side-bar" href="#' + div_id + '" title="' + div_text +'">' + div_text +'</a></li>';
            $(toc).append(content);

            // Add section code to subnavigation
            var children = $('<nav class="md-nav"><ul class="md-nav__list"></nav></ul>')
            var contenders = $("#" + div_id).nextUntil( "h1" );
            $.each(contenders, function(idx, contender){
               if($(contender).is('h2')) {
                   var contender_id = $(contender).attr('id');
                   var contender_text = $(contender).text().split('¬∂')[0];
                   var content = '<li class="md-nav__item"><a class="md-nav__link toc-side-bar" href="#' + contender_id + '" title="' + contender_text +'">' + contender_text +'</a></li>';
                   children.append(content);
                }
             })
             $("#link_" + div_id).append(children);
        });
    });
</script><style>
.more {
    float:right;
    font-size: 1.0rem !important;
}
.more:hover {
    color: cornflowerblue !important;
}

.dropdown {
    position: relative;
    display: inline-block;
}

.dropdown-content {
    display: none;
    position: absolute;
    background-color: #f9f9f9;
    min-width: 160px;
    font-weight: 200;
    box-shadow: 0px 8px 6px 0px rgba(0,0,0,0.2);
    padding: 0px 10px;
    z-index: 1;
}

.dropdown:hover .dropdown-content {
    display: block;
}
</style></article></div></div></div></main></div><footer class="c-footer md-footer-nav"><div class="md-footer-copyright__highlight">
    Copyright ¬© 2019 - The Turing Team
  </div></footer><script src="/v0.7.1/assets/js/application.js"></script><script>console.log('3')</script><script>app.initialize({version:"0.17.4", url:{base:'/v0.7.1'}})</script><script src="/v0.7.1/assets/js/version-switch.js"></script><script>
var headers = ["h1", "h2", "h3", "h4"]
var colors = ["red", "orange", "green", "blue"]

$.each(headers, function(i, header){
    var color = colors[i];
    $(header).each(function () {
        var href=$(this).attr("id");
        $(this).append('<a class="headerlink" style="color:' + color + '" href="#' + href + '" title="Permanent link">¬∂</a>')
    });
})

// Ensure that sidebar on left has arrows
$(".pancakes-parent").on('click', function(){
    console.log($(this).next());
    $(this).next().find('.pancakes-child').toggle();
    if ($(this).hasClass('open-parent')){
        $(this).removeClass('open-parent');
    } else {
        $(this).addClass('open-parent');
    }
})

$(".pancakes-parent-mobile").on('click', function(){
    var nav = $(this).next();
     nav.addClass('mobile-sub-navbar-display');
})

$(".mobile-navbar-back").on('click', function(){
    var nav = $(this).parent();
    nav.removeClass('mobile-sub-navbar-display');
})

</script><script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script><script>
$('h1').first().append('<div></div>')</script><style>
#scrolltop {
  display: none; /* Hidden by default */
  position: fixed; /* Fixed/sticky position */
  bottom: 20px; /* Place the button at the bottom of the page */
  right: 30px; /* Place the button 30px from the right */
  z-index: 99; /* Make sure it does not overlap */
  border: none; /* Remove borders */
  outline: none; /* Remove outline */
  background-color: #d2e6f5; /* Set a background color */
  color: white; /* Text color */
  cursor: pointer; /* Add a mouse pointer on hover */
  padding: 10px 15px; /* Some padding */
  border-radius: 100px; /* Rounded corners */
  font-size: 18px; /* Increase font size */
  font-weight: 600;
}

#scrolltop:hover {
  background-color: #555; /* Add a dark-grey background on hover */
}
</style><button id="scrolltop" onclick="topFunction()" title="Go to top">üîù</button><script>
// When the user scrolls down 20px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
    document.getElementById("scrolltop").style.display = "block";
  } else {
    document.getElementById("scrolltop").style.display = "none";
  }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
  document.body.scrollTop = 0; // For Safari
  document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
}
</script></body></HTML>